<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Anantha Raju C]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Anantha Raju C</title><link>http://localhost:2368/</link></image><generator>Ghost 5.8</generator><lastBuildDate>Sat, 22 Jun 2024 13:50:10 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Building a Real-Time Data Pipeline Using Python, MySQL, Kafka, and ClickHouse]]></title><description><![CDATA[This post explores a data pipeline architecture for real-time data streaming, processing, and visualization using popular open-source tools like Python, MySQL, Kafka, and ClickHouse.]]></description><link>http://localhost:2368/building-a-real-time-data-pipeline-using-python-mysql-kafka-and-clickhouse/</link><guid isPermaLink="false">666522ff85dc8624db48eecf</guid><category><![CDATA[Apache Kafka]]></category><category><![CDATA[Change Data Capture]]></category><category><![CDATA[ClickHouse]]></category><category><![CDATA[Data]]></category><category><![CDATA[dbt]]></category><category><![CDATA[MySQL]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 09 Jun 2024 03:48:53 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1593583845845-7d67ede93328?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQ4fHxwaXBlbGluZXxlbnwwfHx8fDE3MTc5MDQxNDd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1593583845845-7d67ede93328?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQ4fHxwaXBlbGluZXxlbnwwfHx8fDE3MTc5MDQxNDd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Building a Real-Time Data Pipeline Using Python, MySQL, Kafka, and ClickHouse"><p>In the modern data-driven world, real-time data processing and analytics are critical for making timely and informed decisions. This post explores a data pipeline architecture for real-time data streaming, processing, and visualization using popular open-source tools like Python, MySQL, Kafka, and ClickHouse.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><img src="http://localhost:2368/content/images/2024/06/streaming_etl.png" alt="Building a Real-Time Data Pipeline Using Python, MySQL, Kafka, and ClickHouse" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="code-and-documentation">Code and Documentation</h3>
<p>The complete code for this data pipeline, along with detailed documentation of all steps, is available on <a href="https://github.com/AnanthaRajuC/Streaming-ETL-Pipeline-for-Realtime-Analytics">GitHub</a>.</p>
<h4 id="key-features-of-the-github-project">Key Features of the GitHub Project</h4>
<ul>
<li><strong>Step-by-Step Documentation</strong>: The GitHub repository includes detailed instructions for setting up and running the entire pipeline. This makes it easy for users to replicate the setup and understand each component&apos;s role.</li>
<li><strong>Sample Data Generator</strong>: Python scripts to simulate real-time data generation are provided, helping users to test and experiment with the pipeline.</li>
<li><strong>Configuration Files</strong>: Pre-configured settings for MySQL, Debezium, Kafka, and ClickHouse are included to streamline the setup process.</li>
<li><strong>Stream Processing with ksql</strong>: Examples of how to use ksql for real-time data enrichment and transformation are provided, showcasing the power of stream processing.</li>
<li><strong>Integration with BI Tools</strong>: Guidance on how to connect Metabase and Apache Superset to the data pipeline for visualization and reporting is available.</li>
</ul>
<p>By following the instructions and leveraging the provided code, users can quickly set up a robust and scalable real-time data pipeline tailored to their specific needs.</p>
<h3 id="data-generation">Data Generation</h3>
<p>The journey begins with the <strong>Data Generator</strong>. In our example, Python is used to simulate and generate event data. This could be any real-time data source, such as IoT devices, transaction logs, or user interactions on a website.</p>
<h3 id="oltp-online-transaction-processing-data-source">OLTP (Online Transaction Processing) Data Source</h3>
<p>The generated data is initially stored in a <strong>MySQL</strong> database. MySQL serves as the OLTP data source, managing high transaction volumes and maintaining data integrity. It logs changes in real-time using a binlog (binary log), which captures all updates made to the database.</p>
<h3 id="change-data-capture-cdc-with-debezium">Change Data Capture (CDC) with Debezium</h3>
<p>To stream changes from the MySQL database in real-time, <strong>Debezium</strong> is employed. Debezium is a distributed platform that captures row-level changes in databases. It acts as a MySQL Source Connector, monitoring the MySQL binlog for data changes and converting these changes into events.</p>
<h3 id="stream-processing-platform-with-kafka">Stream Processing Platform with Kafka</h3>
<p>These change events are then sent to <strong>Kafka</strong>, a distributed streaming platform capable of handling large volumes of data with low latency. Kafka organizes these events into topics. Each topic is a log of messages, which can be processed in real-time.</p>
<p>To enrich and process these messages further, a stream processing tool like <strong>ksql</strong> (Kafka Stream Processing) can be used. ksql enables real-time data transformations and filtering directly on Kafka topics.</p>
<h3 id="analytical-database-with-clickhouse">Analytical Database with ClickHouse</h3>
<p>For analytical processing, the data is transferred from Kafka to <strong>ClickHouse</strong>, an OLAP (Online Analytical Processing) data warehouse known for its high performance and efficiency in handling analytical queries. ClickHouse consumes Kafka topics through its Kafka Table Engine. The data is then transformed into Materialized Views and stored in MergeTree Tables, optimized for fast query performance.</p>
<h3 id="business-intelligence-and-data-visualization">Business Intelligence and Data Visualization</h3>
<p>Finally, the processed and aggregated data is ready for visualization and analysis. Tools like <strong>Metabase</strong> and <strong>Apache Superset</strong> can connect to ClickHouse, providing interactive dashboards and reports. Additionally, tools like <strong>dbt</strong> (data build tool) can be used for data transformation and modeling, enabling more advanced analytics and insights.</p>
<p>Other applications of this data pipeline include feeding into <strong>ML models</strong> for predictive analytics, performing <strong>operational analytics</strong>, and various other data-driven applications.</p>
<h3 id="conclusion">Conclusion</h3>
<p>This data pipeline demonstrates a architecture for real-time data processing. By leveraging tools like Python, MySQL, Debezium, Kafka, ClickHouse, and various BI tools, organizations can ensure that they have timely and accurate data for decision-making and analytics. The flexibility of this setup make it suitable for a wide range of use cases, from operational monitoring to analytics etc.,</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster]]></title><description><![CDATA[This post briefly captures the usage of dbt-core and it's integration with Dagster.]]></description><link>http://localhost:2368/dbt-core/</link><guid isPermaLink="false">658933857bbe782450364b75</guid><category><![CDATA[Data]]></category><category><![CDATA[ClickHouse]]></category><category><![CDATA[dbt]]></category><category><![CDATA[Dagster]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 25 Dec 2023 12:22:54 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1591164811435-2b8a547039de?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDEyfHx0cmFuc2Zvcm1lcnN8ZW58MHx8fHwxNzAzNDkwNDYxfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1591164811435-2b8a547039de?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDEyfHx0cmFuc2Zvcm1lcnN8ZW58MHx8fHwxNzAzNDkwNDYxfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster"><p>In this blog post, we embark on a journey to explore the seamless integration of dbt-core with Dagster, complemented by the powerful analytics capabilities of ClickHouse. Let&apos;s dive in and witness the synergy of these tools in action.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>At the heart of our data transformation endeavors lies <a href="https://github.com/dbt-labs/dbt-core"><strong>dbt Core</strong></a>, a versatile tool that empowers data teams to wield analytics engineering best practices with ease. Coupled with <a href="https://github.com/dagster-io/dagster"><strong>dagster</strong></a>, an orchestration platform tailored for data asset development and observation, and <a href="https://clickhouse.com/"><strong>ClickHouse</strong></a>, an open-source column-oriented database management system.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following components installed to get started.</p>
<ul>
<li><a href="https://github.com/dagster-io/dagster">Dagster</a></li>
<li><a href="https://github.com/dbt-labs/dbt-core">dbt-core</a></li>
<li><a href="https://clickhouse.com/">ClickHouse</a></li>
<li><a href="https://www.python.org/">Python</a></li>
<li><a href="https://github.com/ClickHouse/dbt-clickhouse">dbt ClickHouse plugin</a></li>
<li><a href="https://docs.dagster.io/integrations/dbt/using-dbt-with-dagster">dagster-dbt</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="installations">Installations</h3>
<p>Install the dbt ClickHouse plugin.</p>
<pre><code class="language-shell">pip install dbt-clickhouse
</code></pre>
<p>Install the <strong>dagster-dbt</strong> library.</p>
<pre><code class="language-shell">pip install dagster-dbt dagster-webserver
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="getting-started">Getting Started</h3>
<p>These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.</p>
<p>Initialize the dbt project and configure it to interface with ClickHouse.</p>
<pre><code class="language-shell">dbt init dbt_data_practitioner 
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt1-1.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>Navigate to Your dbt Project Directory</p>
<pre><code class="language-shell">cd dbt_data_practitioner
</code></pre>
<pre><code class="language-shell">touch profiles.yml
</code></pre>
<p>Add ClickHouse configuration details to <code>profiles.yml</code>. Make sure all dependencies are correctly set up in your <code>profiles.yml</code> file for your database connections.</p>
<pre><code class="language-yml">dbt_data_practitioner:
  target: dev
  outputs:
    dev:
      type: clickhouse
      schema: sakila_db
      host: localhost
      port: 8123
      user: default
      password: root
      secure: False
</code></pre>
<p>Running <code>dbt debug</code> provides valuable insights into your dbt project&apos;s configuration and environment, ensuring everything is set up correctly before diving into data transformations. When you run dbt debug, it performs a series of checks and validations, including: Configuration Validation, Connection Testing, Adapter Information, Environment Information.</p>
<p>By running <code>dbt debug</code> as part of your setup process, you can catch any configuration errors or connectivity issues early on, ensuring a smoother experience when running dbt commands and executing data transformations.</p>
<pre><code class="language-shell">dbt debug
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt2.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>After executing this command, carefully review the output to ensure that everything is configured correctly and that dbt can successfully connect to your ClickHouse instance.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="dbt-docs">dbt docs</h3>
<p>Generate documentation for the dbt project:</p>
<pre><code class="language-shell">dbt docs generate
</code></pre>
<p>This command generates the documentation artifacts for your project. It creates a <code>manifest.json</code> and <code>catalog.json</code> file in the target directory.</p>
<p><img src="http://localhost:2368/content/images/2023/12/dbt3.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>To view the documentation, you need to serve it locally using the following command:</p>
<pre><code class="language-shell">dbt docs serve
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt4.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>By default, this will start a local server at <a href="http://localhost:8080">http://localhost:8080</a> where you can view the documentation.</p>
<p><img src="http://localhost:2368/content/images/2023/12/2525.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="models">models</h3>
<p>Create models and define transformations:</p>
<pre><code class="language-shell">cd models  
mkdir sakila_db  
cd sakila_db  
</code></pre>
<pre><code class="language-shell">touch actor_film_actor_join.sql
touch point_of_interest_1.sql 
</code></pre>
<script src="https://gist.github.com/AnanthaRajuC/4346dd1c6df08418b43778056bfb5e10.js"></script>
<p>Delete the <strong>examples</strong> folder present inside the models folder.</p>
<pre><code class="language-shell">cd ..
cd .. 
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="build-the-project">Build the project:</h3>
<p>The following command runs a series of tasks to build your models, including running models, tests, and snapshots. This command essentially combines the functionality of <code>dbt run</code>, <code>dbt test</code>, and <code>dbt snapshot</code>.</p>
<p>The dbt build command will perform the following tasks:</p>
<ol>
<li><strong>Run Models</strong>: Executes SQL scripts to create views or tables based on your model definitions.</li>
<li><strong>Run Tests</strong>: Executes tests defined in your project to validate data quality and integrity.</li>
<li><strong>Run Snapshots</strong>: Executes snapshot scripts to capture and store historical data.</li>
</ol>
<pre><code class="language-shell">dbt build
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt5.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>The tables and views defined are now generated in ClickHouse DB.</p>
<pre><code class="language-shell">dbt docs generate
</code></pre>
<pre><code class="language-shell">dbt docs serve
</code></pre>
<p>Lineage Graph and other details.</p>
<p><img src="http://localhost:2368/content/images/2023/12/25252.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="dagster-integration">Dagster Integration</h3>
<p>Now, let&apos;s integrate Dagster into our data transformation pipeline:</p>
<pre><code class="language-shell">cd dbt_data_practitioner
</code></pre>
<pre><code class="language-shell">dagster-dbt project scaffold --project-name dagster_data_practitioner
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dag1.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<pre><code class="language-shell">cd dagster_data_practitioner
</code></pre>
<pre><code class="language-shell">DAGSTER_DBT_PARSE_PROJECT_ON_LOAD=1 dagster dev
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dag2-1.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>To access the Dagster UI from your browser, navigate to: <a href="http://127.0.0.1:3000">http://127.0.0.1:3000</a></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="dagster-ui">Dagster UI</h4>
<p><img src="http://localhost:2368/content/images/2023/12/dag3.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>Click on the black &quot;<strong>Materialize all</strong>&quot; button to materialize the transformations.</p>
<p><img src="http://localhost:2368/content/images/2023/12/dag4.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p><img src="http://localhost:2368/content/images/2023/12/dag5.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="sample-project">Sample Project</h4>
<p><a href="https://github.com/AnanthaRajuC/DataPractitioner">DataPractitioner</a> is the sample project i&apos;ve used to illustrate the usage of the aforementioned tools.</p>
<p><em>Noticed an issue with this Sample Project? Open an <a href="https://github.com/AnanthaRajuC/DataPractitioner/issues">issue</a> or a <a href="https://github.com/AnanthaRajuC/DataPractitioner/pulls">PR</a> on GitHub!</em></p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine]]></title><description><![CDATA[<p>In this guide, we&apos;ll walk through the seamless process of replicating a MySQL database into ClickHouse, leveraging the experimental MaterializedMySQL Database Engine. By replicating MySQL data into ClickHouse, we harness the real-time analytical capabilities of ClickHouse while preserving data integrity and consistency.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>MySQL stands as a stalwart</p>]]></description><link>http://localhost:2368/replicate-mysql-database-in-clickhouse-using/</link><guid isPermaLink="false">65891ce27bbe782450364a54</guid><category><![CDATA[Data]]></category><category><![CDATA[MySQL]]></category><category><![CDATA[ClickHouse]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 25 Dec 2023 07:34:10 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1472017053394-b29fded587cd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI4fHx0d2lufGVufDB8fHx8MTcwMzQ4OTU2N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1472017053394-b29fded587cd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI4fHx0d2lufGVufDB8fHx8MTcwMzQ4OTU2N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine"><p>In this guide, we&apos;ll walk through the seamless process of replicating a MySQL database into ClickHouse, leveraging the experimental MaterializedMySQL Database Engine. By replicating MySQL data into ClickHouse, we harness the real-time analytical capabilities of ClickHouse while preserving data integrity and consistency.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>MySQL stands as a stalwart in the realm of relational database management systems, renowned for its open-source nature and robust features. On the other hand, ClickHouse shines as a lightning-fast, column-oriented database management system, empowering users to generate analytical data reports in real-time through SQL queries.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following components installed and configured:</p>
<ul>
<li><a href="https://www.mysql.com/">MySQL</a> database.</li>
<li><a href="https://clickhouse.com/">ClickHouse</a> database.</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="mysqlcnf-mysql-server-configuration-for-replication">mysql.cnf MySQL server configuration for Replication</h3>
<p>The following are mandatory MySQL server configuration which must be set.</p>
<ol>
<li>Stop MySQL service.</li>
</ol>
<pre><code class="language-shell">systemctl status mysql
systemctl stop mysql.service
</code></pre>
<ol start="2">
<li>Update the configuration.</li>
</ol>
<p>Edit the MySQL configuration file mysqld.cnf:</p>
<pre><code class="language-shell">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf
</code></pre>
<p>Add/Update the following parameters:</p>
<pre><code class="language-txt">[mysqld]
server-id                     = 223344
log_bin                       = mysql-bin
expire_logs_days              = 1
binlog_format                 = row
binlog_row_image              = FULL
default_authentication_plugin = mysql_native_password
gtid_mode                     = on
enforce_gtid_consistency      = on
</code></pre>
<p>ClickHouse reads binlog and performs DDL and DML queries.</p>
<ol start="3">
<li>Start MySQL service.</li>
</ol>
<pre><code class="language-shell">systemctl start mysql.service
</code></pre>
<p><em>Reference:</em> <a href="https://clickhouse.com/docs/en/engines/database-engines/materialized-mysql">https://clickhouse.com/docs/en/engines/database-engines/materialized-mysql</a></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="mysql-user-creation-for-replication">MySQL user creation for replication</h3>
<p>Create a MySQL user specifically for replication:</p>
<pre><code class="language-shell">create user clickhouse_replication@&apos;localhost&apos; identified with mysql_native_password by &apos;ChRep$316&apos;;
</code></pre>
<pre><code class="language-shell">grant replication slave, replication client, reload, select on *.* to clickhouse_replication@&apos;localhost&apos;;
</code></pre>
<pre><code class="language-shell">flush privileges;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="sample-mysql-data-setup">Sample MySQL Data Setup</h3>
<p>Setup a sample table and insert data into MySQL:</p>
<ol>
<li>Create a table.</li>
</ol>
<pre><code class="language-shell">CREATE TABLE `user` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `created_date` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `last_modified_date` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `username` varchar(255) DEFAULT NULL,
  `email` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `UK_sb8bbouer5wak8vyiiy4pf2bx` (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
</code></pre>
<ol start="2">
<li>Insert sample data.</li>
</ol>
<pre><code class="language-shell">INSERT INTO `user`(`username`,`email`)VALUES(&apos;John Doe&apos;,&apos;johndoe@example.com&apos;);

INSERT INTO `user`(`username`,`email`)VALUES(&apos;Jane Doe&apos;,&apos;janedoe@example.com&apos;);
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="clickhouse-server-configuration">CLickHouse server configuration</h3>
<pre><code class="language-shell">sudo systemctl status clickhouse-server
sudo systemctl stop clickhouse-server
</code></pre>
<p>Update ClickHouse server configuration to allow experimental MaterializedMySQL engine:</p>
<pre><code class="language-shell">sudo nano /etc/clickhouse-server/users.xml
</code></pre>
<p>Add/Update the following value within <code>&lt;profiles&gt;</code>:</p>
<pre><code class="language-xml">    &lt;profiles&gt;
        &lt;default&gt;                           &lt;allow_experimental_database_materialized_mysql&gt;1&lt;/allow_experimental_database_materialized_mysql&gt;
        &lt;/default&gt;
    &lt;/profiles&gt;
</code></pre>
<pre><code class="language-shell">sudo systemctl start clickhouse-server
</code></pre>
<p>Login to ClickHouse server using terminal or any GUI tool and verify that the changes have been saved.</p>
<pre><code class="language-shell">clickhouse-client --password   
</code></pre>
<pre><code class="language-sql">SELECT
    name,
    value,
    changed,
    description
FROM system.settings
WHERE name = &apos;allow_experimental_database_materialized_mysql&apos;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="activating-the-replication-in-clickhouse">Activating the replication in ClickHouse</h3>
<p>Create a ClickHouse database with MaterializedMySQL engine for replication:</p>
<pre><code class="language-shell">CREATE DATABASE mysqlCH
ENGINE = MaterializeMySQL(&apos;127.0.0.1&apos;, &apos;ch&apos;, &apos;clickhouse_replication&apos;, &apos;ChRep$316&apos;)
SETTINGS allows_query_when_mysql_lost = 1, max_wait_time_when_mysql_unavailable = 10000, materialized_mysql_tables_list = &apos;user&apos;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--1-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>Verify Replication</strong></p>
<p>Check if the database and the table with data got replicated into ClickHouse:</p>
<pre><code class="language-sql">select * from mysqlCH.user;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--2-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>Additional Operations</strong></p>
<p>Perform additional operations in MySQL and verify replication status in ClickHouse:</p>
<p><strong>INSERT</strong> additional rows in MySQL table and check the replication status.</p>
<pre><code class="language-sql">INSERT INTO `user`(`username`,`email`)VALUES(&apos;Alice&apos;,&apos;alice@example.com&apos;);
INSERT INTO `user`(`username`,`email`)VALUES(&apos;Bob&apos;,&apos;alice@example.com&apos;);
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--3-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>UPDATE</strong> Alice&apos;s email in MySQL user table.</p>
<pre><code class="language-sql">UPDATE `ch`.`user` SET `email` = &apos;alice@domain.com&apos; WHERE `username` = &apos;Alice&apos;;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--4-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>DELETE</strong> Bob in MySQL user table.</p>
<pre><code class="language-sql">DELETE FROM `ch`.`user` WHERE (`id` = &apos;4&apos;);
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--5-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p>With these steps, you&apos;ve successfully set up MySQL replication into ClickHouse using the MaterializedMySQL engine. Harness the power of ClickHouse&apos;s real-time analytical capabilities while ensuring data consistency and integrity. Explore the vast possibilities of real-time analytics with ClickHouse and unlock insights from your replicated MySQL data effortlessly.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[ClickHouse]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>This post briefly documents the process of using ClickHouse.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="introduction">Introduction</h4>
<p>ClickHouse is a fast open-source column-oriented database management system that allows generating analytical data reports in real-time using SQL queries.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="minimum-software-requirements">Minimum Software Requirements</h4>
<ul>
<li><a href="https://clickhouse.com/">ClickHouse</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="download-the-software">Download the software</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--12-.png" class="kg-image" alt loading="lazy" width="858" height="577" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--12-.png 600w, http://localhost:2368/content/images/2023/11/carbon--12-.png 858w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="running-the-application">Running the application</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--14--1.png" class="kg-image" alt loading="lazy" width="1256" height="465" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--14--1.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--14--1.png 1000w, http://localhost:2368/content/images/2023/11/carbon--14--1.png 1256w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="accessing-the-client">Accessing the client</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--15-.png" class="kg-image" alt loading="lazy" width="1145" height="1507" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--15-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--15-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--15-.png 1145w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--16-.png" class="kg-image" alt loading="lazy" width="1145" height="1343" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--16-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--16-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--16-.png 1145w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="resources">Resources</h4>
<p><a href="https://clickhouse.com/docs/en/getting-started/example-datasets/menus">New York Public Library &quot;What&</a></p>]]></description><link>http://localhost:2368/clickhouse/</link><guid isPermaLink="false">655223945650dc3b96a57209</guid><category><![CDATA[Data]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 13 Nov 2023 13:59:16 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1542274368-443d694d79aa?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHZlcnRpY2FsJTIwcGlwZXN8ZW58MHx8fHwxNjk5ODgxODgzfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1542274368-443d694d79aa?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHZlcnRpY2FsJTIwcGlwZXN8ZW58MHx8fHwxNjk5ODgxODgzfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="ClickHouse"><p>This post briefly documents the process of using ClickHouse.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="introduction">Introduction</h4>
<p>ClickHouse is a fast open-source column-oriented database management system that allows generating analytical data reports in real-time using SQL queries.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="minimum-software-requirements">Minimum Software Requirements</h4>
<ul>
<li><a href="https://clickhouse.com/">ClickHouse</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="download-the-software">Download the software</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--12-.png" class="kg-image" alt="ClickHouse" loading="lazy" width="858" height="577" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--12-.png 600w, http://localhost:2368/content/images/2023/11/carbon--12-.png 858w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="running-the-application">Running the application</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--14--1.png" class="kg-image" alt="ClickHouse" loading="lazy" width="1256" height="465" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--14--1.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--14--1.png 1000w, http://localhost:2368/content/images/2023/11/carbon--14--1.png 1256w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="accessing-the-client">Accessing the client</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--15-.png" class="kg-image" alt="ClickHouse" loading="lazy" width="1145" height="1507" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--15-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--15-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--15-.png 1145w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--16-.png" class="kg-image" alt="ClickHouse" loading="lazy" width="1145" height="1343" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--16-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--16-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--16-.png 1145w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="resources">Resources</h4>
<p><a href="https://clickhouse.com/docs/en/getting-started/example-datasets/menus">New York Public Library &quot;What&apos;s on the Menu?&quot; Dataset</a> an example of denormalizing data.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Airbyte]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>Airbyte is a powerful, open-source data integration engine designed to help you seamlessly consolidate your data into data warehouses, lakes, and databases. Whether you&apos;re managing data from various sources or need to ensure your data is easily accessible for analysis, Airbyte simplifies the process.</p>
<p>In this blog</p>]]></description><link>http://localhost:2368/airbyte/</link><guid isPermaLink="false">65521b155650dc3b96a571c7</guid><category><![CDATA[Data]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 13 Nov 2023 13:23:17 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1619249722898-492c571615fe?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDl8fGNvbm5lY3RvcnN8ZW58MHx8fHwxNjk5ODgwMDc0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<img src="https://images.unsplash.com/photo-1619249722898-492c571615fe?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDl8fGNvbm5lY3RvcnN8ZW58MHx8fHwxNjk5ODgwMDc0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Airbyte"><p>Airbyte is a powerful, open-source data integration engine designed to help you seamlessly consolidate your data into data warehouses, lakes, and databases. Whether you&apos;re managing data from various sources or need to ensure your data is easily accessible for analysis, Airbyte simplifies the process.</p>
<p>In this blog post, I will demonstrate how to use Airbyte with MySQL installed locally on Ubuntu as the source and a local JSON file as the output.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>To get started with Airbyte, ensure you have the following software installed:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a></li>
<li><a href="https://www.mysql.com/">MySQL</a> (installed locally on Ubuntu)</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="running-the-application">Running the application</h3>
<p>First, ensure Docker is running on your machine. If Docker isn&apos;t installed, download and install it from the official Docker website.</p>
<p>Open a terminal and run the following command&apos;s to download and start Airbyte:</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--10-.png" class="kg-image" alt="Airbyte" loading="lazy" width="800" height="291" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--10-.png 600w, http://localhost:2368/content/images/2023/11/carbon--10-.png 800w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--6-.png" class="kg-image" alt="Airbyte" loading="lazy" width="1024" height="1340" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--6-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--6-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--6-.png 1024w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="accessing-the-ui">Accessing the UI</h3>
<p>Once Airbyte is up and running, you can access the user interface by navigating to the following URL in your web browser:</p>
<p><a href="http://localhost:8000/">http://localhost:8000/</a></p>
<p>Use the following credentials to log in:</p>
<p><strong>username</strong> <em>airbyte</em><br>
<strong>password</strong> <em>password</em></p>
<p><em>note</em>: For security reasons, it&apos;s recommended to change these credentials. You can do this by modifying the .env file located in your Airbyte directory.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/123.png" class="kg-image" alt="Airbyte" loading="lazy" width="938" height="299" srcset="http://localhost:2368/content/images/size/w600/2023/11/123.png 600w, http://localhost:2368/content/images/2023/11/123.png 938w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/ab1.png" class="kg-image" alt="Airbyte" loading="lazy" width="1366" height="680" srcset="http://localhost:2368/content/images/size/w600/2023/11/ab1.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/ab1.png 1000w, http://localhost:2368/content/images/2023/11/ab1.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="source-configuration">Source Configuration</h3>
<p>After logging in, you&apos;ll need to configure your data sources. Airbyte supports a wide variety of data sources, allowing you to integrate data from multiple platforms effortlessly.</p>
<h4 id="1-adding-a-source">1. Adding a Source</h4>
<ul>
<li>Click on the &quot;Sources&quot; tab in the UI.</li>
<li>Select &quot;MySQL&quot; as the type of source you want to add.</li>
<li>Fill in the required details, such as connection name, host, port, database name, username, and password.</li>
<li>Test the connection to ensure it&apos;s working correctly.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/ab3.png" class="kg-image" alt="Airbyte" loading="lazy" width="1195" height="1557" srcset="http://localhost:2368/content/images/size/w600/2023/11/ab3.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/ab3.png 1000w, http://localhost:2368/content/images/2023/11/ab3.png 1195w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="2-setting-up-a-destination">2. Setting Up a Destination</h4>
<p>For this example, we&apos;ll use a local JSON file as the destination.</p>
<ul>
<li>Click on the &quot;Destinations&quot; tab.</li>
<li>Select &quot;Local JSON&quot; as your desired destination.</li>
<li>Enter the necessary details, such as the file path where the JSON file will be stored.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2024/06/localjson.png" class="kg-image" alt="Airbyte" loading="lazy" width="1195" height="722" srcset="http://localhost:2368/content/images/size/w600/2024/06/localjson.png 600w, http://localhost:2368/content/images/size/w1000/2024/06/localjson.png 1000w, http://localhost:2368/content/images/2024/06/localjson.png 1195w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="3-creating-a-connection">3. Creating a Connection</h4>
<ul>
<li>Once your source and destination are configured, navigate to the &quot;Connections&quot; tab.</li>
<li>Click &quot;New Connection&quot; and select your source and destination.</li>
<li>Configure the sync frequency and any other options as needed.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2024/06/conn.png" class="kg-image" alt="Airbyte" loading="lazy" width="1366" height="659" srcset="http://localhost:2368/content/images/size/w600/2024/06/conn.png 600w, http://localhost:2368/content/images/size/w1000/2024/06/conn.png 1000w, http://localhost:2368/content/images/2024/06/conn.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="4-running-your-first-sync">4. Running Your First Sync</h4>
<ul>
<li>After setting up your connection, you can run your first sync manually by clicking &quot;Sync Now&quot; on your connection page.</li>
<li>Monitor the progress and check for any errors that may need to be addressed.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2024/06/syn.png" class="kg-image" alt="Airbyte" loading="lazy" width="1366" height="659" srcset="http://localhost:2368/content/images/size/w600/2024/06/syn.png 600w, http://localhost:2368/content/images/size/w1000/2024/06/syn.png 1000w, http://localhost:2368/content/images/2024/06/syn.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="conclusion">Conclusion</h3>
<p>Airbyte is an excellent tool for integrating and consolidating your data from various sources. By following this quick start guide, you can set up and run your first data sync, ensuring your data is centralized and ready for analysis. For more advanced configurations and troubleshooting, refer to the official Airbyte documentation.</p>
<p>By leveraging Airbyte&apos;s capabilities, you can streamline your data workflows and focus on deriving insights from your data, rather than managing the complexities of data integration.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Exploratory data analysis with sweetviz]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>&quot;Exploratory Data Analysis (EDA) is an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis.&quot; <a href="https://www.epa.gov/caddis-vol4/exploratory-data-analysis#:~:text=Exploratory%20Data%20Analysis%20(EDA)%20is,step%20in%20any%20data%20analysis.">1</a></p>
<p>&quot;<a href="https://github.com/fbdesignpro/sweetviz">Sweetviz</a> is an open-source Python library that generates</p>]]></description><link>http://localhost:2368/exploratory-data-analysis-with/</link><guid isPermaLink="false">65450e67b9d645b14615fe98</guid><category><![CDATA[Data]]></category><category><![CDATA[Python]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Fri, 03 Nov 2023 15:38:01 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1682687220509-61b8a906ca19?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wxfDF8YWxsfDF8fHx8fHwyfHwxNjk5MDI1Njg5fA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1682687220509-61b8a906ca19?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wxfDF8YWxsfDF8fHx8fHwyfHwxNjk5MDI1Njg5fA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Exploratory data analysis with sweetviz"><p>&quot;Exploratory Data Analysis (EDA) is an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis.&quot; <a href="https://www.epa.gov/caddis-vol4/exploratory-data-analysis#:~:text=Exploratory%20Data%20Analysis%20(EDA)%20is,step%20in%20any%20data%20analysis.">1</a></p>
<p>&quot;<a href="https://github.com/fbdesignpro/sweetviz">Sweetviz</a> is an open-source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with just two lines of code. Output is a fully self-contained HTML application.&quot;</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><strong>Installation</strong></p>
<p>Before installing Sweetviz, you need to ensure that Python is installed on your system. You can check this by opening your command line or terminal and running the following command:</p>
<pre><code class="language-sh">python --version
</code></pre>
<p>If Python is installed, this command will display the version number. For example, you might see something like Python <strong>3.8.10</strong>. If Python is not installed, you can download and install it from the official Python website.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/python_version_check.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1360" height="484" srcset="http://localhost:2368/content/images/size/w600/2023/11/python_version_check.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/python_version_check.png 1000w, http://localhost:2368/content/images/2023/11/python_version_check.png 1360w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p>Once Python is installed, you can proceed to install Sweetviz using pip. Run the following command in your command line or terminal:</p>
<pre><code class="language-sh">pip install sweetviz
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/pip_install_sweetviz.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1024" height="1321" srcset="http://localhost:2368/content/images/size/w600/2023/11/pip_install_sweetviz.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/pip_install_sweetviz.png 1000w, http://localhost:2368/content/images/2023/11/pip_install_sweetviz.png 1024w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong>Basic Program</strong></p>
<p>Once Sweetviz is installed, you can create a simple program to generate a data report. We&apos;ll use the Titanic dataset for this example.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class="language-python">import pandas as pd
import sweetviz as sv

# Load the dataset
data = pd.read_csv(&apos;titanic.csv&apos;)

# Generate the Sweetviz report
report = sv.analyze(data)

# Display the report in the browser
report.show_html()
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><strong>Explanation of the Code</strong></p>
<ol>
<li>
<p><strong>Import Libraries:</strong> Import the necessary libraries, pandas for data manipulation and sweetviz for generating the report.</p>
</li>
<li>
<p><strong>Load Dataset:</strong> Read the Titanic dataset into a Pandas DataFrame.</p>
</li>
<li>
<p><strong>Generate Report:</strong> Use Sweetviz&apos;s analyze function to create a report based on the DataFrame.</p>
</li>
<li>
<p><strong>Show Report:</strong> Generate an HTML report that is automatically opened in your web browser.</p>
</li>
</ol>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/sweetviz_python_file_execution.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1024" height="298" srcset="http://localhost:2368/content/images/size/w600/2023/11/sweetviz_python_file_execution.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/sweetviz_python_file_execution.png 1000w, http://localhost:2368/content/images/2023/11/sweetviz_python_file_execution.png 1024w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong>Output</strong></p>
<p>After running the script, Sweetviz will generate a self-contained HTML report and open it in your default web browser. The report provides visualizations and analysis of your dataset.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/sweetvizoutput.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1366" height="1509" srcset="http://localhost:2368/content/images/size/w600/2023/11/sweetvizoutput.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/sweetvizoutput.png 1000w, http://localhost:2368/content/images/2023/11/sweetvizoutput.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong>Resources</strong></p>
<p>To further enhance your understanding and usage of Sweetviz and Python scripts, here are some additional resources:</p>
<ul>
<li>
<p>Download the dataset used in this example: <a href="https://github.com/datasciencedojo/datasets/blob/master/titanic.csv">https://github.com/datasciencedojo/datasets/blob/master/titanic.csv</a></p>
</li>
<li>
<p>A comprehensive guide on running Python scripts, useful for beginners: <a href="https://www.knowledgehut.com/blog/programming/run-python-scripts#how-to-run-python-script-by-the-interpreter">https://www.knowledgehut.com/blog/programming/run-python-scripts#how-to-run-python-script-by-the-interpreter</a></p>
</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><strong>Conclusion</strong></p>
<p>Sweetviz is a powerful tool that simplifies the process of exploratory data analysis by generating interactive reports. By following the steps outlined in this article, you can quickly start analyzing your datasets and gain valuable insights. Feel free to explore more features and customize your reports to suit your needs.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Automating Your Ubuntu Setup: A Post-Installation Shell Script]]></title><description><![CDATA[<p>Streamline your Ubuntu setup process with a post-installation shell script designed to automate the installation of essential software and configurations. This script simplifies the setup of a new Ubuntu system, saving you time and effort by handling common post-install tasks automatically.</p><!--kg-card-begin: markdown--><h3 id="understanding-shell-scripts">Understanding Shell Scripts</h3>
<p>A shell script is a computer</p>]]></description><link>http://localhost:2368/ubuntu-post-install-script/</link><guid isPermaLink="false">638af8c4bbbc8447f5ca93ed</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sat, 03 Dec 2022 07:30:42 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1605791767308-46f38113f418?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGFmdGVyfGVufDB8fHx8MTY3MDA1MjA0Mw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1605791767308-46f38113f418?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGFmdGVyfGVufDB8fHx8MTY3MDA1MjA0Mw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Automating Your Ubuntu Setup: A Post-Installation Shell Script"><p>Streamline your Ubuntu setup process with a post-installation shell script designed to automate the installation of essential software and configurations. This script simplifies the setup of a new Ubuntu system, saving you time and effort by handling common post-install tasks automatically.</p><!--kg-card-begin: markdown--><h3 id="understanding-shell-scripts">Understanding Shell Scripts</h3>
<p>A shell script is a computer program designed to be run by a Unix shell, serving as a command-line interpreter. By leveraging shell scripting, you can automate repetitive tasks and streamline your workflow on Ubuntu.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>To utilize this post-installation shell script, you need a system running <a href="https://ubuntu.com/">Ubuntu</a>, a popular Linux distribution known for its user-friendly interface and robustness.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="sample-project">Sample Project</h3>
<p>Explore the <a href="https://github.com/AnanthaRajuC/ubuntu-post-install">Ubuntu Post Install Scripts repository</a> on GitHub to access the sample script and customize it according to your preferences.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="basic-usage">Basic Usage</h3>
<ol>
<li>
<p><strong>Open a Terminal Window:</strong> Launch a terminal window on Ubuntu by pressing either of the following key combinations on your keyboard:</p>
<ul>
<li><kbd>Ctrl</kbd> + <kbd>Alt</kbd> + <kbd>T</kbd></li>
<li><kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd></li>
</ul>
</li>
<li>
<p><strong>Set Script Permissions:</strong> Update the permissions of the script to make it executable using the <code>chmod</code> command.</p>
<ul>
<li><code>sudo chmod +x ubuntu-post-install.sh</code></li>
</ul>
</li>
<li>
<p>Run the script with the <code>bash</code> command.</p>
<ul>
<li><code>sudo bash ubuntu-post-install.sh</code></li>
</ul>
</li>
</ol>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="customization">Customization</h3>
<p>While the provided script includes a set of preferential packages and configurations, you can customize it to suit your specific requirements. Feel free to update the script with your preferred software packages and system configurations by modifying the script available on the <a href="https://github.com/AnanthaRajuC/ubuntu-post-install/blob/main/ubuntu-post-install.sh">GitHub repository</a>.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><img src="http://localhost:2368/content/images/2023/11/ubuntu_post_unstall_script.png" alt="Automating Your Ubuntu Setup: A Post-Installation Shell Script" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="conclusion">Conclusion</h3>
<p>By leveraging the Ubuntu Post Install Script, you can expedite the setup process of your Ubuntu system, ensuring that it is configured according to your preferences and equipped with essential software packages. Embrace automation to enhance your productivity and make the most out of your Ubuntu experience.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE's, API Tools, Dev Tools]]></title><description><![CDATA[<div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="gnome-shell-extensions">Gnome Shell Extensions</h2></div><!--kg-card-begin: markdown--><p><a href="https://extensions.gnome.org/">GNOME Shell extensions</a> provide a powerful way to customize your GNOME desktop environment, tailoring it to your preferences and enhancing its functionality. Here, i&#x2019;ve compiled a list of some of useful GNOME Shell extensions that can improve your user experience.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="extensions">Extensions</h4>
<p><strong><a href="https://gitlab.com/arcmenu/ArcMenu" target="_blank">ArcMenu</a></strong></p>
<p><strong>Description:</strong> ArcMenu is</p>]]></description><link>http://localhost:2368/list/</link><guid isPermaLink="false">6382ec72f99fa23f8440d39f</guid><category><![CDATA[Tools]]></category><category><![CDATA[Linux]]></category><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 27 Nov 2022 05:25:15 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1584974414562-cc750a809485?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxFeHRlbnNpb258ZW58MHx8fHwxNjY5NTI0NjAx&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="gnome-shell-extensions">Gnome Shell Extensions</h2></div><!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1584974414562-cc750a809485?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxFeHRlbnNpb258ZW58MHx8fHwxNjY5NTI0NjAx&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools"><p><a href="https://extensions.gnome.org/">GNOME Shell extensions</a> provide a powerful way to customize your GNOME desktop environment, tailoring it to your preferences and enhancing its functionality. Here, i&#x2019;ve compiled a list of some of useful GNOME Shell extensions that can improve your user experience.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="extensions">Extensions</h4>
<p><strong><a href="https://gitlab.com/arcmenu/ArcMenu" target="_blank">ArcMenu</a></strong></p>
<p><strong>Description:</strong> ArcMenu is a customizable application menu for GNOME Shell, offering a modern and intuitive way to access your applications and system settings. It can be configured to resemble traditional application menus found in other desktop environments, making it ideal for users transitioning to GNOME.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Highly customizable menu layout</li>
<li>Search functionality for quick access to applications</li>
<li>Support for themes and icon packs</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/ArcMenu_Banner.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="1602" height="629" srcset="http://localhost:2368/content/images/size/w600/2022/11/ArcMenu_Banner.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/ArcMenu_Banner.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/ArcMenu_Banner.png 1600w, http://localhost:2368/content/images/2022/11/ArcMenu_Banner.png 1602w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/PRATAP-KUMAR/control-blur-effect-on-lock-screen" target="_blank">Control Blur Effect On Lock Screen</a></strong><br>
<strong>Description:</strong> This extension allows you to control the blur effect on the lock screen, providing a customizable aesthetic touch to your GNOME desktop. You can adjust the blur intensity to your liking, ensuring a perfect balance between style and readability.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Adjustable blur intensity</li>
<li>Simple and straightforward configuration</li>
<li>Enhances visual appeal of the lock screen</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot_2935_QPQQttf.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="1366" height="768" srcset="http://localhost:2368/content/images/size/w600/2022/11/screenshot_2935_QPQQttf.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/screenshot_2935_QPQQttf.png 1000w, http://localhost:2368/content/images/2022/11/screenshot_2935_QPQQttf.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/home-sweet-gnome/dash-to-panel" target="_blank">Dash to Panel</a></strong><br>
<strong>Description:</strong> Dash to Panel transforms your GNOME Shell dash into a unified taskbar, combining application launchers and the system tray into a single panel. This extension is perfect for users who prefer a more traditional desktop layout similar to Windows or KDE Plasma.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Combines dash and system tray into one panel</li>
<li>Highly customizable appearance and behavior</li>
<li>Supports multi-monitor setups</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/dtp-main-p2.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="1294" height="397" srcset="http://localhost:2368/content/images/size/w600/2022/11/dtp-main-p2.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/dtp-main-p2.png 1000w, http://localhost:2368/content/images/2022/11/dtp-main-p2.png 1294w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/neffo/earth-view-wallpaper-gnome-extension" target="_blank">Google Earth Wallpaper</a></strong><br>
<strong>Description:</strong> Bring the beauty of the world to your desktop with the Google Earth Wallpaper extension. This extension sets your wallpaper to a random, high-quality photo from the curated Google Earth collection, ensuring your background is always stunning and unique.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Automatically updates wallpaper with random Google Earth photos</li>
<li>High-resolution images</li>
<li>Option to manually refresh the wallpaper</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot_1295.jpg" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="1920" height="1080" srcset="http://localhost:2368/content/images/size/w600/2022/11/screenshot_1295.jpg 600w, http://localhost:2368/content/images/size/w1000/2022/11/screenshot_1295.jpg 1000w, http://localhost:2368/content/images/size/w1600/2022/11/screenshot_1295.jpg 1600w, http://localhost:2368/content/images/2022/11/screenshot_1295.jpg 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://gitlab.com/skrewball/openweather" target="_blank">OpenWeather</a></strong><br>
<strong>Description:</strong> Stay updated with real-time weather information directly on your GNOME Shell with the OpenWeather extension. It displays weather data for any location worldwide, offering detailed forecasts and current conditions.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Displays current weather and forecasts</li>
<li>Supports multiple locations</li>
<li>Detailed weather information including temperature, humidity, and wind speed</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/openweather-screenshot.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="1674" height="1202" srcset="http://localhost:2368/content/images/size/w600/2022/11/openweather-screenshot.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/openweather-screenshot.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/openweather-screenshot.png 1600w, http://localhost:2368/content/images/2022/11/openweather-screenshot.png 1674w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/kazysmaster/gnome-shell-extension-lockkeys" target="_blank">Lock Keys</a></strong><br>
<strong>Description:</strong> The Lock Keys extension displays the status of Numlock and Capslock on the GNOME panel, providing a convenient way to check if these keys are active. This is especially useful for users who frequently switch between text and numeric input.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Visual indicator for Numlock and Capslock status</li>
<li>Simple and unobtrusive design</li>
<li>Essential for users with keyboards lacking LED indicators</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="240" height="170"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://gitlab.gnome.org/World/ShellExtensions/desktop-icons" target="_blank">Desktop Icons</a></strong><br>
<strong>Description:</strong> The Desktop Icons extension brings back the ability to add and manage icons on your desktop, a feature that is missing in the default GNOME Shell. This extension is perfect for users who prefer to have quick access to files, folders, and applications directly from their desktop.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Add and manage desktop icons</li>
<li>Support for dragging and dropping files</li>
<li>Customizable icon placement</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot_1465_2g841mr.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins, Web Browser Plugins, Online IDE&apos;s, API Tools, Dev Tools" loading="lazy" width="224" height="227"></figure><!--kg-card-begin: markdown--><p>With these GNOME Shell extensions, you can significantly enhance your GNOME desktop environment, making it more functional and personalized to suit your needs. Whether you&apos;re looking for aesthetic improvements, productivity boosts, or additional features, these extensions have got you covered.</p>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="intellij-idea-plugins">IntelliJ IDEA Plugins</h2></div><!--kg-card-begin: markdown--><p><a href="https://www.jetbrains.com/idea/">IntelliJ IDEA</a>: stands as a premier integrated development environment (IDE) crafted by JetBrains, designed to facilitate the creation of software using Java, Kotlin, Groovy, and other JVM-based languages. It comes in two editions: a community edition licensed under Apache 2 and a proprietary commercial edition. One of its most compelling features is its extensibility through plugins, which enrich its capabilities and tailor it to individual developer needs.</p>
<h3 id="understanding-plugins">Understanding Plugins</h3>
<p>Plugins are software extensions that augment the functionality of a program. In the realm of IntelliJ IDEA, they serve as indispensable tools, providing developers with additional features, productivity enhancements, and integration with external services.</p>
<h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<ul>
<li>To take advantage of these plugins, ensure you have <a href="https://www.jetbrains.com/idea/download/#section=linux">IntelliJ IDEA</a> installed on your system.</li>
</ul>
<h4 id="plugins">Plugins</h4>
<ul>
<li>
<p><strong><a href="https://github.com/andrey4623/intellij-rainbow-csv" target="_blank">Rainbow CSV</a></strong><br>
<strong>Description:</strong> Rainbow CSV is a versatile plugin that enhances the readability of CSV files by highlighting them in different colors. This visual distinction makes it easier to interpret and manipulate large datasets, thereby improving productivity during data analysis and manipulation tasks.</p>
</li>
<li>
<p><strong><a href="https://plugins.jetbrains.com/plugin/7973-sonarlint" target="_blank">SonarLint</a></strong><br>
<strong>Description:</strong> SonarLint is a free IDE extension that acts as a guardian for your codebase, continuously analyzing it to detect and rectify bugs, vulnerabilities, and code smells in real-time. Similar to a spell checker, SonarLint identifies issues as you write code, offering quick fixes and actionable insights to ensure clean and robust code.</p>
</li>
<li>
<p><strong><a href="https://github.com/robohorse/RoboPOJOGenerator" target="_blank">RoboPOJOGenerator</a></strong><br>
<strong>Description:</strong> This indispensable plugin streamlines the process of transforming JSON data structures into Plain Old Java Objects (POJOs). Ideal for IntelliJ IDEA and Android Studio users, RoboPOJOGenerator automates the tedious task of manual POJO generation, saving developers valuable time and effort.</p>
</li>
<li>
<p><strong><a href="https://github.com/gejun123456/intellij-generateAllSetMethod" target="_blank">intellij-generateAllSetMethod</a></strong><br>
<strong>Description:</strong> Simplify the task of generating setter method calls for class properties with this intuitive IntelliJ IDEA plugin. By automating the creation of setter method invocations, it accelerates the development process and reduces the likelihood of manual errors.</p>
</li>
<li>
<p><strong><a href="https://plugins.jetbrains.com/plugin/9792-key-promoter-x" target="_blank">Key Promoter X</a></strong><br>
<strong>Description:</strong> Mastering keyboard shortcuts is key to efficient coding, and Key Promoter X is here to help. This plugin facilitates the learning process by displaying keyboard shortcuts whenever you perform an action using the mouse within the IDE. By encouraging the use of keyboard shortcuts, it promotes a faster, mouse-free development workflow.</p>
</li>
<li>
<p><strong><a href="https://plugins.jetbrains.com/plugin/7179-maven-helper" target="_blank">Maven Helper</a></strong><br>
<strong>Description:</strong> Maven Helper is an indispensable tool for managing dependencies within your IntelliJ IDEA projects. It provides valuable insights into dependency conflicts, allowing you to analyze and exclude conflicting dependencies effortlessly. Additionally, it offers advanced features for optimizing Maven builds, ensuring smooth and efficient project development.</p>
</li>
</ul>
<p>IntelliJ IDEA plugins play a crucial role in enhancing productivity, streamlining development workflows, and empowering developers to write clean, efficient code. By incorporating IntelliJ IDEA plugins into your IntelliJ IDEA setup, you can unlock new capabilities, automate repetitive tasks.</p>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="web-browser-plugins">Web Browser Plugins</h2></div><!--kg-card-begin: markdown--><p>Enhance your browsing experience and streamline your workflow with these essential web browser plugins. From improving SEO practices to organizing your favorite links, these tools are designed to make your web browsing more efficient and enjoyable.</p>
<h4 id="general-plugins">General Plugins</h4>
<ul>
<li>
<p><strong><a href="https://www.checkbot.io">Checkbot</a></strong><br>
<em>Browser extension that tests if your website follows 50+ SEO, speed, and security best practices. Crawls your site checking multiple pages at once.</em></p>
</li>
<li>
<p><strong><a href="http://fvdspeeddial.com/">FVD Speed Dial</a></strong><br>
<em>Tool for saving and organizing the links to your favorite sites.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/hover-zoom/nonjdcjchghhkdoolnlbekcfllmednbl?hl=en">Hover Zoom</a></strong><br>
<em>Browse image galleries with ease: move the mouse cursor over thumbnails to view images in their full size without loading a new page.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/floating-for-youtube/jjphmlaoffndcnecccgemfdaaoighkel">Floating for YouTube&#x2122;</a></strong><br>
<em>Always on top Floating Mini Player for YouTube&#x2122;.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/floating-for-youtube-exte/egncdnniomonjgpjbapalkckojhkfddk">Floating for YouTube&#x2122; Extension</a></strong><br>
<em>Open YouTube videos in Floating Mini Player for YouTube&#x2122;.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/whatfont/jabopobgcpjmedljpbcaablpmlmfcogm">WhatFont</a></strong><br>
<em>The easiest way to identify fonts on web pages.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/alexa-traffic-rank/cknebhggccemgcnbidipinkifmmegdel">Alexa Traffic Rank</a></strong><br>
<em>The Official Alexa Traffic Rank Extension, providing Alexa Traffic Rank and site information when clicked.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/clear-cache/cppjkneekbjaeellbfkmgnhonkkjfpdn">Clear Cache</a></strong><br>
<em>Clear your cache and browsing data with a single click of a button.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/speedtest-by-ookla/pgjjikdiikihdfpoppgaidccahalehjh">Speedtest by Ookla</a></strong><br>
<em>Take a Speedtest directly from your Google Chrome toolbar to quickly test your internet performance.</em></p>
</li>
<li>
<p><strong><a href="http://tampermonkey.net/">Tampermonkey</a></strong><br>
<em>Userscript manager with over 10 million users. Available for Chrome, Microsoft Edge, Safari, Opera Next, and Firefox.</em></p>
</li>
</ul>
<h4 id="developer-plugins">Developer Plugins</h4>
<ul>
<li>
<p><strong><a href="https://github.com/harshjv/github-repo-size">GitHub Repository Size</a></strong><br>
<em>Automatically adds repository size to GitHub&apos;s repository summary.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/livereload/jnihajbhpnppcggbcgedagnkighmdlei?hl=en">LiveReload</a></strong><br>
<em>LiveReload monitors changes in the file system. As soon as you save a file, it is preprocessed as needed, and the browser is refreshed.</em></p>
</li>
<li>
<p><strong><a href="https://chrome.google.com/webstore/detail/isometric-contributions/mjoedlfflcchnleknnceiplgaeoegien">Isometric Contributions</a></strong><br>
<em>Toggle between the normal GitHub contribution chart and an isometric pixel art version.</em></p>
</li>
<li>
<p><strong><a href="http://getfireshot.com/">FireShot</a></strong><br>
<em>Capture full web page screenshots in Firefox, Chrome, Opera, or IE, and then edit and save them.</em></p>
</li>
</ul>
<h4 id="web-browsers">Web Browsers</h4>
<ul>
<li>
<p><strong><a href="https://blisk.io/">Blisk</a></strong><br>
<em>A free browser for web developers, offering tools for development, debugging, and testing: emulation, sync, analytics, and screenshots.</em></p>
</li>
<li>
<p><strong><a href="https://brave.com/">Brave</a></strong><br>
<em>A free browser that automatically blocks ads and trackers, making it faster and safer.</em></p>
</li>
</ul>
<p>These plugins can significantly enhance your web browsing experience and boost your productivity. Whether you&apos;re a developer, a designer, or just someone who loves exploring the web, these tools are sure to be invaluable additions to your toolkit.</p>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="online-ides">Online IDE&apos;s</h2><h3 class="kg-header-card-subheader" id="online-coding-environments">Online Coding Environments</h3></div><!--kg-card-begin: markdown--><ul>
<li>
<p><strong><a href="https://tio.run/#">Try It Online</a></strong><br>
Try It Online (TIO) is an online compiler that supports a wide range of practical and recreational programming languages. It provides a convenient platform for quickly testing and running code snippets without the need for local installations.</p>
</li>
<li>
<p><strong><a href="https://ideone.com/">Ideone</a></strong><br>
Ideone is a versatile online compiler and debugging tool that supports over 60 programming languages. It allows developers to compile and execute code directly in the browser, making it an excellent choice for rapid prototyping and sharing code snippets.</p>
</li>
<li>
<p><strong><a href="https://codenvy.io">Codenvy</a></strong><br>
Codenvy offers self-service Eclipse Che workspaces in the cloud. It provides a collaborative environment for teams to develop, build, and deploy applications using the power of container-based development.</p>
</li>
<li>
<p><strong><a href="http://www.tutorialspoint.com/codingground.htm">Coding Ground - TutorialsPoint</a></strong><br>
TutorialsPoint&apos;s Coding Ground is a comprehensive online platform that supports coding in multiple popular programming languages. It offers a seamless editing, compiling, executing, and sharing experience, all within a browser-based interface.</p>
</li>
</ul>
<h4 id="code-fiddles">Code Fiddles</h4>
<ul>
<li>
<p><strong><a href="http://phpfiddle.org/">PhpFiddle</a></strong><br>
PhpFiddle is a versatile online PHP IDE that provides a range of tools and resources for PHP, MySQL, SQLite, HTML, CSS, and JavaScript development. It enables users to test, debug, and share PHP code snippets and web applications with ease.</p>
</li>
<li>
<p><strong><a href="https://dotnetfiddle.net/">.NET Fiddle</a></strong><br>
.NET Fiddle is a convenient online sandbox for experimenting with .NET code snippets. It offers a lightweight development environment for quickly trying out C#, F#, and Visual Basic code without the need for local installations.</p>
</li>
<li>
<p><strong><a href="https://jsfiddle.net/">JSFiddle</a></strong><br>
JSFiddle is a popular online playground for testing and sharing JavaScript, CSS, HTML, and CoffeeScript code snippets. It provides a simple yet powerful code editor and live preview functionality, making it ideal for frontend development and prototyping.</p>
</li>
<li>
<p><strong><a href="http://sqlfiddle.com/">SQL Fiddle</a></strong><br>
SQL Fiddle is a handy online tool for testing and sharing SQL database queries and schema designs. It supports various database systems, including MySQL, PostgreSQL, Oracle, and SQLite, allowing users to experiment with SQL code in a collaborative environment.</p>
</li>
<li>
<p><strong><a href="http://pythonfiddle.com/">Python Fiddle</a></strong><br>
Python Fiddle offers a user-friendly web-based Python IDE for experimenting with Python code snippets. It allows users to run, edit, and share Python scripts in real-time, making it an excellent choice for learning Python and testing small programs.</p>
</li>
<li>
<p><strong><a href="http://www.r-fiddle.org/#/">R-Fiddle</a></strong><br>
R-Fiddle provides a convenient online platform for coding, testing, and sharing R code snippets and data analysis scripts. It offers an interactive R environment with support for plotting, statistical analysis, and package management, making it ideal for statistical computing tasks.</p>
</li>
<li>
<p><strong><a href="https://go.dev/play/">Go Playground</a></strong><br>
Go Playground is an official online tool provided by the Go programming language community. It offers a clean and minimalist environment for writing and executing Go code, making it easy to experiment with Go features and syntax without setting up a local development environment.</p>
</li>
</ul>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="api-tools">API Tools</h2><h3 class="kg-header-card-subheader" id="enhancing-your-api-workflow-essential-tools-for-developers">Enhancing Your API Workflow: Essential Tools for Developers</h3></div><!--kg-card-begin: markdown--><p>APIs (Application Programming Interfaces) and web services play a crucial role in modern software development, enabling seamless communication and integration between different applications and systems. To streamline your API development process and enhance productivity, consider incorporating the following essential tools into your toolkit:</p>
<ul>
<li>
<p><strong><a href="http://www.downforeveryoneorjustme.com/" target="_blank">down for everyone or just me</a></strong><br>
<strong>Description:</strong> This simple tool allows you to quickly check if a website is down for everyone or just for you. It provides valuable insights into website availability, helping you troubleshoot connectivity issues more efficiently.</p>
</li>
<li>
<p><strong><a href="https://uptime.is/" target="_blank">Uptime</a></strong><br>
<strong>Description:</strong> Ensure your service-level agreements (SLAs) are met with this uptime calculator tool. It helps you calculate and track uptime metrics, enabling you to maintain high availability and reliability for your applications and services.</p>
</li>
<li>
<p><strong><a href="https://apigee.com/providers" target="_blank">Apigee API Console</a></strong><br>
<strong>Description:</strong> Discover, learn, test, and debug any API with interactive developer tools and documentation provided by Apigee API Console. It offers a comprehensive set of features for API exploration and testing, empowering developers to build and integrate APIs seamlessly.</p>
</li>
<li>
<p><strong><a href="https://postb.in/" target="_blank">PostBin</a></strong><br>
<strong>Description:</strong> Programatically test your API clients or webhooks with PostBin. It provides a convenient platform for sending and receiving HTTP requests, allowing you to validate and debug your API integrations effectively.</p>
</li>
<li>
<p><strong><a href="http://httpbin.org/" target="_blank">httpbin</a></strong><br>
<strong>Description:</strong> httpbin is a versatile HTTP request and response service designed for client testing. It offers a wide range of endpoints for testing various HTTP methods and parameters, making it an invaluable tool for API development and debugging.</p>
</li>
<li>
<p><strong><a href="http://requestbin.com/" target="_blank">RequestBin</a></strong><br>
<strong>Description:</strong> RequestBin provides a URL that collects requests made to it, allowing you to inspect and debug them in a human-friendly way. Use RequestBin to monitor and analyze HTTP requests sent by your clients or webhooks, facilitating effective troubleshooting and debugging.</p>
</li>
<li>
<p><strong><a href="http://mockbin.com/" target="_blank">Mockbin</a></strong><br>
<strong>Description:</strong> Mockbin enables you to generate custom endpoints to test, mock, and track HTTP requests and responses. It&apos;s a powerful tool for simulating different scenarios and behaviors in your API integrations, helping you ensure robustness and reliability.</p>
</li>
<li>
<p><strong><a href="https://www.statuspage.io/" target="_blank">Statuspage</a></strong>  <strong>Description:</strong> Statuspage is a comprehensive status and incident communication tool that enables you to keep your stakeholders informed about service availability and performance. It provides real-time status updates and incident notifications, helping you maintain transparency and trust with your users.</p>
</li>
<li>
<p><strong><a href="https://rapidapi.com/" target="_blank">Rapid API</a></strong><br>
<strong>Description:</strong> RapidAPI is the world&#x2019;s largest API marketplace, offering a vast selection of APIs for various use cases and industries. Explore and integrate APIs seamlessly into your applications, accelerating development and unlocking new capabilities.</p>
</li>
<li>
<p><strong><a href="https://apiembed.com/" target="_blank">APIembed</a></strong><br>
<strong>Description:</strong> APIembed provides embeddable API code snippets in multiple programming languages for your website, blog, or API documentation. Quickly generate code examples and documentation to simplify API integration for developers.</p>
</li>
<li>
<p><strong><a href="https://loader.io/" target="_blank">Loader</a></strong><br>
<strong>Description:</strong> Loader is a free load testing service that allows you to stress test your web apps and APIs with thousands of concurrent connections. Ensure your applications can handle peak loads and maintain performance under stress with Loader.</p>
</li>
<li>
<p><strong><a href="https://reqres.in/" target="_blank">REQ RES</a></strong><br>
<strong>Description:</strong> REQ RES is a hosted REST API ready to respond to your AJAX requests. It&apos;s a convenient tool for testing and prototyping API interactions, providing a reliable endpoint for your development and testing needs.</p>
</li>
</ul>
<h4 id="conclusion">Conclusion</h4>
<p>By incorporating API tools into your development workflow, you can streamline your API development process, enhance productivity, and ensure the reliability and performance of your applications and services.</p>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="dev-tools">Dev Tools</h2><h3 class="kg-header-card-subheader" id="stronguseful-software-tools-for-software-developmentstrong"><strong>Useful Software Tool&apos;s for Software Development</strong></h3></div><!--kg-card-begin: markdown--><h4 id="version-control">Version control</h4>
<ul>
<li>
<p><strong><a href="https://www.sourcetreeapp.com/" target="_blank">Sourcetree</a></strong><br>
Sourcetree is a user-friendly Git GUI client available for both Windows and Mac users. It simplifies the process of managing Git repositories through its intuitive interface.</p>
</li>
<li>
<p><strong><a href="https://desktop.github.com/" target="_blank">GitHub Desktop</a></strong><br>
GitHub Desktop provides a graphical interface for interacting with GitHub repositories, allowing users to perform common Git operations without using the command line.</p>
</li>
</ul>
<h4 id="virtualization">Virtualization</h4>
<ul>
<li>
<p><strong><a href="http://www.vmware.com/in/products/player" target="_blank">VMware Workstation</a></strong><br>
VMware Workstation is a powerful desktop virtualization application that enables users to run multiple operating systems simultaneously on a single computer without the need for rebooting.</p>
</li>
<li>
<p><strong><a href="https://virt-manager.org/" target="_blank">virt-manager</a></strong>  virt-manager is a desktop application for managing virtual machines using the libvirt API. It supports various virtualization technologies, including KVM, Xen, and LXC.</p>
</li>
</ul>
<h4 id="java">Java</h4>
<ul>
<li>
<p><strong><a href="http://jd.benow.ca/" target="_blank">Java Decompiler</a></strong><br>
Java Decompiler is a tool for decompiling and analyzing Java bytecode, allowing developers to understand and modify Java applications more effectively.</p>
</li>
<li>
<p><strong><a href="https://bytecodeviewer.com/" target="_blank">Bytecode Viewer</a></strong><br>
Bytecode Viewer is a comprehensive suite for reverse engineering Java 8 JAR files and Android APKs. It includes features such as decompilation, editing, debugging, and more.</p>
</li>
<li>
<p><strong><a href="https://github.com/skylot/jadx" target="_blank">jadx - Dex to Java decompiler</a></strong><br>
jadx is a command-line and GUI tool for converting Android DEX and APK files into human-readable Java source code, aiding in the analysis and modification of Android applications.</p>
</li>
</ul>
<h4 id="ides">IDE&apos;s</h4>
<ul>
<li>
<p><strong><a href="https://www.jetbrains.com/idea/" target="_blank">IntelliJ IDEA</a></strong><br>
IntelliJ IDEA is a feature-rich integrated development environment (IDE) for Java, Kotlin, Groovy, and other JVM-based languages. It offers advanced coding assistance, productivity tools, and seamless integration with version control systems.</p>
</li>
<li>
<p><strong><a href="https://www.jetbrains.com/pycharm/" target="_blank">PyCharm</a></strong><br>
PyCharm is a powerful Python IDE designed for professional developers. It provides intelligent code completion, debugging, testing, and other features to streamline Python development workflows.</p>
</li>
</ul>
<h4 id="project-management">Project Management</h4>
<ul>
<li><strong><a href="https://www.openproject.org/" target="_blank">Open Project</a></strong><br>
OpenProject is an open-source project management software solution that facilitates collaboration, task tracking, and project planning. It offers features such as Gantt charts, Agile boards, and time tracking.</li>
</ul>
<h4 id="various">Various</h4>
<ul>
<li>
<p><strong><a href="https://winscp.net/eng/index.php" target="_blank">WinSCP</a></strong><br>
WinSCP is a free and open-source SFTP client and FTP client for Windows, enabling secure file transfers between local and remote systems. It supports various transfer protocols and offers a user-friendly interface.</p>
</li>
<li>
<p><strong><a href="https://www.putty.org/" target="_blank">PuTTY</a></strong><br>
PuTTY is a widely-used SSH and telnet client for Windows, developed to provide secure remote access to Unix-based systems. It offers a simple yet powerful interface for managing SSH connections.</p>
</li>
<li>
<p><strong><a href="http://keystore-explorer.org/index.html" target="_blank">KeyStore Explorer</a></strong><br>
KeyStore Explorer is a GUI tool that serves as a replacement for the Java command-line utilities keytool and jarsigner. It simplifies the management of cryptographic keys, certificates, and keystores.</p>
</li>
<li>
<p><strong><a href="http://apns-gcm.bryantan.info/" target="_blank">APNS/GCM Online Tester</a></strong><br>
APNS/GCM Online Tester is a web-based tool for testing Apple Push Notification Service (APNS) and Google Cloud Messaging (GCM) integrations. It allows developers to send test notifications and verify their implementation.</p>
</li>
</ul>
<h4 id="datardbms">Data - RDBMS</h4>
<ul>
<li>
<p><strong><a href="https://dbeaver.jkiss.org/" target="_blank">DBeaver</a></strong><br>
DBeaver is a versatile multi-platform database tool designed for developers, SQL programmers, and database administrators. It supports various relational database management systems (RDBMS) and offers advanced SQL editing capabilities.</p>
</li>
<li>
<p><strong><a href="https://dev.mysql.com/downloads/windows/notifier/" target="_blank">MySQL Notifier</a></strong><br>
MySQL Notifier is a lightweight utility that monitors Windows and MySQL services, providing notifications about changes in their status. It helps administrators keep track of MySQL server operations and troubleshoot issues efficiently.</p>
</li>
</ul>
<h4 id="datanosql">Data - NoSQL</h4>
<ul>
<li><strong><a href="https://www.mongodb.com/products/compass" target="_blank">MongoDB Compass</a></strong><br>
MongoDB Compass is a graphical user interface (GUI) for MongoDB that simplifies database management and query building. It offers visual tools for exploring data, creating indexes, and analyzing performance metrics.</li>
</ul>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="others">Others</h2></div><!--kg-card-begin: markdown--><h4 id="others">Others</h4>
<ul>
<li>
<p><strong><a href="https://howsecureismypassword.net/" target="_blank">HOW SECURE IS MY PASSWORD?</a></strong><br>
<em>Password strength meter.</em></p>
</li>
<li>
<p><strong><a href="http://www.devopsbookmarks.com/" target="_blank">DevOps Bookmarks</a></strong><br>
<em>Discover tools and frameworks in the DevOps landscape.</em></p>
</li>
<li>
<p><strong><a href="https://www.lambdatest.com/" target="_blank">Lambda Test</a></strong><br>
<em>Cross Browser Testing Cloud</em></p>
</li>
<li>
<p><strong><a href="https://www.epochconverter.com/" target="_blank">Epoch Converter</a></strong><br>
<em>Epoch &amp; Unix Timestamp Conversion Tools</em></p>
</li>
<li>
<p><strong><a href="https://github.com/quicktype/quicktype" target="_blank">quicktype</a></strong><br>
<em>Generate types and converters from JSON, Schema, and GraphQL</em></p>
</li>
<li>
<p><strong><a href="http://www.useragentstring.com/" target="_blank">User Agent String</a></strong><br>
<em>Analyze User Agent of you browser or any other user agent string.</em></p>
</li>
</ul>
<h4 id="editor">Editor</h4>
<ul>
<li>
<p><strong><a href="https://www.gitbook.com/editor" target="_blank">GitBook Editor</a></strong><br>
<em>Expressive Markup (Markdown/Asciidoc), Advanced proofreading, Version Control</em></p>
</li>
<li>
<p><strong><a href="https://laverna.cc/index.html" target="_blank">Laverna</a></strong><br>
<em>Markdown note taking app focused on privacy. Consider it like open source alternative to Evernote.</em></p>
</li>
</ul>
<h4 id="design">Design</h4>
<ul>
<li><strong><a href="https://www.behance.net/" target="_blank">Behance</a></strong><br>
<em>Showcase and discover the latest work from top online portfolios by creative professionals across industries.</em></li>
</ul>
<h4 id="markdown">Markdown</h4>
<ul>
<li>
<p><strong><a href="https://marketplace.visualstudio.com/items?itemName=yzane.markdown-pdf" target="_blank">Markdown PDF</a></strong><br>
<em>This Visual Studio Code extension convert Markdown file to pdf, html, png or jpeg file.</em></p>
</li>
<li>
<p><strong><a href="https://github.com/DavidAnson/markdownlint" target="_blank">markdownlint</a></strong><br>
<em>markdown lint.</em></p>
</li>
<li>
<p><strong><a href="https://ecotrust-canada.github.io/markdown-toc/" target="_blank">markdown-toc</a></strong><br>
<em>Markdown Table of Contents generator.</em></p>
</li>
<li>
<p><strong><a href="https://markdowntohtml.com/" target="_blank">Markdown to HTML Converter</a></strong><br>
<em>Markdown to HTML Converter.</em></p>
</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Unlocking Data Visualization Brilliance with Apache Superset]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>In this guide, we&apos;ll embark on an exploration of <a href="https://superset.apache.org/">Apache Superset</a>, a cutting-edge platform for data exploration and visualization. By leveraging Apache Superset, we empower users to uncover insights and communicate data stories effectively through stunning visualizations.</p>
<p>&quot;Data and information visualization is an interdisciplinary field that</p>]]></description><link>http://localhost:2368/apache-superset/</link><guid isPermaLink="false">6370faedacb23e2fd22ca182</guid><category><![CDATA[Data]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 13 Nov 2022 14:16:20 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1593806967876-4ad6cd0c5759?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG1hbmRhbGF8ZW58MHx8fHwxNjY4MzQ4Nzk1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<img src="https://images.unsplash.com/photo-1593806967876-4ad6cd0c5759?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG1hbmRhbGF8ZW58MHx8fHwxNjY4MzQ4Nzk1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Unlocking Data Visualization Brilliance with Apache Superset"><p>In this guide, we&apos;ll embark on an exploration of <a href="https://superset.apache.org/">Apache Superset</a>, a cutting-edge platform for data exploration and visualization. By leveraging Apache Superset, we empower users to uncover insights and communicate data stories effectively through stunning visualizations.</p>
<p>&quot;Data and information visualization is an interdisciplinary field that deals with the graphic representation of data and information. It is a particularly efficient way of communicating when the data or information is numerous as for example a time series.&quot;</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following components installed to kickstart your Apache Superset journey:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a></li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a></li>
<li><a href="https://www.mysql.com/">MySQL</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="getting-started">Getting Started</h3>
<h4 id="setup">Setup</h4>
<p>First, ensure Docker and Docker Compose are installed on your system by checking their versions:</p>
<pre><code class="language-shell">docker version
docker-compose version
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-25-55.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1080" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-25-55.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-25-55.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-25-55.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-25-55.png 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="database-setup">Database Setup</h3>
<p>In this post we will use docker to deploy Apache Superset, in order to allow Apache Superset to connect to locally installed MySQL database we will have to perform the following operations.</p>
<ol>
<li>Enable MySQL to be able to listen for an external IP address where the server can be reached:</li>
</ol>
<pre><code class="language-shell">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf
</code></pre>
<p>update <strong>bind-address</strong> directive to a wildcard IP address, either *****, <strong>::</strong>, or <strong>0.0.0.0</strong> to reference an external IP address.</p>
<p>By default, <strong>bind-address</strong> is set to <strong>127.0.0.1</strong>, meaning that the server will only look for local connections.</p>
<p><em>Reference:</em> <a href="https://www.digitalocean.com/community/tutorials/how-to-allow-remote-access-to-mysql">https://www.digitalocean.com/community/tutorials/how-to-allow-remote-access-to-mysql</a></p>
<hr>
<ol start="2">
<li>Create MySQL users for Apache Superset:</li>
</ol>
<pre><code>CREATE USER &apos;myuser&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;mypass&apos;;
CREATE USER &apos;myuser&apos;@&apos;%&apos; IDENTIFIED BY &apos;mypass&apos;;
</code></pre>
<pre><code>GRANT ALL ON *.* TO &apos;myuser&apos;@&apos;localhost&apos;;
GRANT ALL ON *.* TO &apos;myuser&apos;@&apos;%&apos;;
</code></pre>
<pre><code>FLUSH PRIVILEGES;
</code></pre>
<p>Reference: <a href="https://stackoverflow.com/a/55742963">https://stackoverflow.com/a/55742963</a></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="bringing-up-apache-superset">Bringing up Apache Superset</h3>
<p>Updating <strong>docker-compose-non-dev.yml</strong> to connect to localhost.</p>
<script src="https://gist.github.com/AnanthaRajuC/ba602aada1311a007c228e1a1cbaafbc.js"></script>
<ol>
<li>Clone the Apache Superset repository and navigate to the directory:</li>
</ol>
<pre><code class="language-shell">git clone https://github.com/apache/superset.git
cd superset/
</code></pre>
<ol start="2">
<li>Pull and start Apache Superset using Docker Compose:</li>
</ol>
<pre><code class="language-shell">docker-compose -f docker-compose-non-dev.yml pull
docker-compose -f docker-compose-non-dev.yml up
</code></pre>
<p><strong>Reference:</strong> <a href="https://superset.apache.org/docs/installation/installing-superset-using-docker-compose">https://superset.apache.org/docs/installation/installing-superset-using-docker-compose</a></p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-30-26.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1080" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-30-26.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-30-26.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-30-26.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-30-26.png 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="accessing-superset-gui">Accessing Superset GUI</h3>
<p>Navigate to <a href="localhost:8088/login/">localhost:8088/login/</a> in your web browser to access the Apache Superset GUI.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-05.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1918" height="1006" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-31-05.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-31-05.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-31-05.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-05.png 1918w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-29.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1003" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-31-29.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-31-29.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-31-29.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-29.png 1920w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-03-25-11-28-43.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1003" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-03-25-11-28-43.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-03-25-11-28-43.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-03-25-11-28-43.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-03-25-11-28-43.png 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="connect-to-database">Connect to Database</h3>
<p>To connect Apache Superset to your MySQL database, use the following connection details:</p>
<p><strong>Host</strong>: <code>172.17.0.1</code><br>
<strong>Port</strong>: <code>3306</code><br>
<strong>Database Name</strong>: <code>mysqmpledb</code><br>
<strong>Username</strong>: <code>root</code><br>
<strong>Password</strong>: <code>root</code><br>
<strong>Display Name</strong>: MySQL - LocalHost</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/12/1234.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1347" height="651" srcset="http://localhost:2368/content/images/size/w600/2023/12/1234.png 600w, http://localhost:2368/content/images/size/w1000/2023/12/1234.png 1000w, http://localhost:2368/content/images/2023/12/1234.png 1347w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p>With Apache Superset, you unlock a world of possibilities for data exploration and visualization. Harness its capabilities to transform your data into actionable insights and captivating visualizations that drive informed decision-making and empower data-driven storytelling.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Empower Real-Time Stream Processing with ksqlDB and ksql-cli]]></title><description><![CDATA[<p>In this guide, we&apos;ll delve into setting up ksqlDB&#x2014;a powerful database purpose-built for stream processing applications&#x2014;alongside its command-line interface, ksql-cli. With these tools, you can seamlessly explore, query, and transform streaming data to derive valuable insights in real-time.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p><a href="https://ksqldb.io/">ksqlDB</a> stands as a cutting-edge</p>]]></description><link>http://localhost:2368/ksqldb/</link><guid isPermaLink="false">636517ee2ba729dae794e42f</guid><category><![CDATA[Apache Kafka]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Fri, 04 Nov 2022 14:10:01 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1528952686551-542043782ab9?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHZlbmRvcnxlbnwwfHx8fDE2Njc1Njk2Njk&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1528952686551-542043782ab9?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHZlbmRvcnxlbnwwfHx8fDE2Njc1Njk2Njk&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Empower Real-Time Stream Processing with ksqlDB and ksql-cli"><p>In this guide, we&apos;ll delve into setting up ksqlDB&#x2014;a powerful database purpose-built for stream processing applications&#x2014;alongside its command-line interface, ksql-cli. With these tools, you can seamlessly explore, query, and transform streaming data to derive valuable insights in real-time.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p><a href="https://ksqldb.io/">ksqlDB</a> stands as a cutting-edge database designed specifically for stream processing applications. It offers a streamlined platform for processing and analyzing streaming data, enabling users to unlock insights and respond to events as they occur. Coupled with <a href="https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/cli-config/">ksql-cli</a>, a command-line interface for interacting with ksqlDB, you gain unparalleled flexibility and control over your stream processing workflows.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following software components installed to kickstart your ksqlDB journey:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a> (for ksqldb-server, ksqldb-cli)</li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="getting-started">Getting Started</h3>
<h4 id="setup">Setup</h4>
<h4 id="running-the-application-via-docker-compose">Running the application via docker compose</h4>
<p>Leverage Docker Compose to orchestrate the deployment of ksqlDB and ksql-cli:</p>
<pre><code> ksqldb-server:
    image: confluentinc/ksqldb-server:latest
    container_name: ksqldb-server
    hostname: ksqldb-server
    depends_on: [kafka]
    ports:
      - 8088:8088
    networks:
      - webproxy
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: &quot;true&quot;
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: &quot;true&quot;
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-01:8084
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081

  ksqldb-cli:
    image: confluentinc/ksqldb-cli:latest
    container_name: ksqldb-cli
    networks:
      - webproxy
    depends_on: [kafka, ksqldb-server]
    entrypoint: /bin/sh
    tty: true
    environment:
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-01:8084
</code></pre>
<p>Pull all required docker images</p>
<pre><code class="language-shell">$ docker-compose pull
</code></pre>
<p>Start up the environment</p>
<p>Initiate the Docker containers to start up the ksqlDB environment.</p>
<p>The first time that you do this, the Docker images will be pulled down from the remote server. This may take a while!</p>
<pre><code class="language-shell">$ docker-compose up
</code></pre>
<pre><code class="language-shell">Creating ksqldb-server      ... done
Creating ksqldb-cli         ... done
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h3>
<p>Interact with ksqlDB using ksql-cli:</p>
<pre><code class="language-shell">$ docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
</code></pre>
<p><img src="http://localhost:2368/content/images/2022/11/10-ksql-db-initial.png" alt="Empower Real-Time Stream Processing with ksqlDB and ksql-cli" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="check-topics-streams-and-tables">Check topics, streams and tables</h3>
<pre><code class="language-sql">show topics;
</code></pre>
<pre><code class="language-sql">show streams;
</code></pre>
<pre><code class="language-sql">show tables;
</code></pre>
<h3 id="streams">Streams</h3>
<ul>
<li>Declare Streams</li>
</ul>
<pre><code class="language-sql">SET &apos;auto.offset.reset&apos; = &apos;earliest&apos;;
</code></pre>
<pre><code class="language-sql">CREATE STREAM PERSON_STREAM (id bigint,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,name VARCHAR,username VARCHAR,address_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">CREATE STREAM ADDRESS_STREAM (id bigint,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,city VARCHAR,street VARCHAR,suite VARCHAR,zipcode VARCHAR,geo_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Queries</li>
</ul>
<pre><code class="language-sql">DESCRIBE PERSON_STREAM;
</code></pre>
<pre><code class="language-sql">select * from PERSON_STREAM;
</code></pre>
<pre><code class="language-sql">SELECT * FROM PERSON_STREAM EMIT CHANGES LIMIT 1;
</code></pre>
<pre><code class="language-sql">+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|ID                       |UUID                     |CREATED_DATE_TIME        |LAST_MODIFIED_DATE_TIME  |NAME                     |USERNAME                 |ADDRESS_ID               |
+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|1                        |ce8d2120-1f93-11ed-8647-0|2022-08-19T13:22:00.000  |2022-08-19T13:22:00.000  |d14                      |dbz14                    |1                        |
|                         |c9a3cfadc50              |                         |                         |                         |                         |                         |
Limit Reached
Query terminated
</code></pre>
<ul>
<li>stream-stream join</li>
</ul>
<pre><code class="language-sql">CREATE STREAM PERSON_ADDRESS_ENRICHED_STREAM WITH (FORMAT=&apos;JSON&apos;, KAFKA_TOPIC=&apos;person_address_enriched&apos;, PARTITIONS=1, REPLICAS=1) AS 
SELECT
  P.ID P_ID,
  A.ID A_ID,
  P.NAME NAME,
  A.CITY CITY
FROM PERSON_STREAM P
LEFT OUTER JOIN ADDRESS_STREAM A WITHIN 1 HOURS GRACE PERIOD 30 MINUTES ON ((A.ID = P.ADDRESS_ID))
EMIT CHANGES;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="tables">Tables</h3>
<ul>
<li>Declare Tables</li>
</ul>
<pre><code class="language-sql">CREATE TABLE PERSON (id bigint PRIMARY KEY,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,name VARCHAR,username VARCHAR,address_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">CREATE TABLE ADDRESS (id bigint PRIMARY KEY,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,city VARCHAR,street VARCHAR,suite VARCHAR,zipcode VARCHAR,geo_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Query Tables</li>
</ul>
<pre><code class="language-sql">SELECT * FROM PERSON EMIT CHANGES LIMIT 1;
</code></pre>
<pre><code class="language-sql">SELECT * FROM ADDRESS EMIT CHANGES LIMIT 1;
</code></pre>
<ul>
<li>Table Joins</li>
</ul>
<pre><code class="language-sql">SELECT 
	P.NAME,
	A.CITY
FROM PERSON P
LEFT JOIN ADDRESS A on A.id = P.address_id
EMIT CHANGES 
LIMIT 1;
</code></pre>
<pre><code class="language-sql">SELECT 
  P.NAME, 
  A.CITY
FROM PERSON P
INNER JOIN ADDRESS A
ON A.id = P.address_id
EMIT CHANGES
LIMIT 1;
</code></pre>
<pre><code class="language-sql">CREATE TABLE PERSON_ADDRESS_ENRICHED (P_ID bigint,A_ID bigint,NAME VARCHAR,CITY VARCHAR) WITH (KAFKA_TOPIC=&apos;person_address_enriched&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Others</li>
</ul>
<pre><code class="language-sql">DROP TABLE IF EXISTS PERSON;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><hr>
<h3 id="tear-down-the-stack">Tear down the stack</h3>
<p>When done, tear down the ksqlDB stack:</p>
<pre><code class="language-shell">$ docker compose-down
</code></pre>
<pre><code class="language-shell">Stopping ksqldb-cli       ... done
Stopping ksqldb-server    ... done
Removing ksqldb-cli         ... done
Removing ksqldb-server      ... done
</code></pre>
<p><em>If you want to preserve the state of all containers, run <code>docker-compose stop</code> instead.</em></p>
<p>By following these steps, you can harness the power of ksqlDB and ksql-cli to perform real-time stream processing, enabling you to extract insights and derive value from streaming data with ease and efficiency.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Learning & Reference Resources for Developers]]></title><description><![CDATA[<p>Expand your knowledge and enhance your skills with these curated learning and reference resources. From versioning guidelines to design patterns and web tutorials, these resources provide valuable information for developers at all levels.</p><!--kg-card-begin: markdown--><h3 id="various-resources">Various Resources</h3>
<h4 id="semantic-versioning-200"><a href="https://semver.org/">Semantic Versioning 2.0.0</a></h4>
<p><strong>Description:</strong> Understand the guidelines for semantic versioning. This resource provides</p>]]></description><link>http://localhost:2368/learning-resources/</link><guid isPermaLink="false">635f3e8dd1f3be2eba75eb36</guid><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 31 Oct 2022 03:26:15 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1588769655209-422ea26a0398?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGphcnxlbnwwfHx8fDE2NjcxODYzMjU&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1588769655209-422ea26a0398?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGphcnxlbnwwfHx8fDE2NjcxODYzMjU&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Learning &amp; Reference Resources for Developers"><p>Expand your knowledge and enhance your skills with these curated learning and reference resources. From versioning guidelines to design patterns and web tutorials, these resources provide valuable information for developers at all levels.</p><!--kg-card-begin: markdown--><h3 id="various-resources">Various Resources</h3>
<h4 id="semantic-versioning-200"><a href="https://semver.org/">Semantic Versioning 2.0.0</a></h4>
<p><strong>Description:</strong> Understand the guidelines for semantic versioning. This resource provides a standardized versioning system that helps you manage project releases and dependencies effectively.</p>
<h4 id="java-design-patterns"><a href="http://java-design-patterns.com/">Java Design Patterns</a></h4>
<p><strong>Description:</strong> Explore a comprehensive collection of design patterns implemented in Java. This site offers clear examples and explanations, making it an invaluable resource for mastering design patterns in Java.</p>
<h4 id="common-words"><a href="https://anvaka.github.io/common-words/#?lang=java">Common Words</a></h4>
<p><strong>Description:</strong> Visualize the common words used in different programming languages. This interactive tool helps you understand language syntax and common terminology across various programming languages.</p>
<h3 id="web-resources">Web Resources</h3>
<h4 id="rest-api-tutorial"><a href="https://restfulapi.net/">REST API Tutorial</a></h4>
<p><strong>Description:</strong> Learn about RESTful APIs with this detailed tutorial. It covers the fundamentals of REST architecture, design principles, and best practices for building and consuming RESTful web services.</p>
<h4 id="http-status-codes"><a href="https://www.restapitutorial.com/httpstatuscodes.html">HTTP Status Codes</a></h4>
<p><strong>Description:</strong> Familiarize yourself with HTTP status codes using this comprehensive resource. It provides detailed descriptions and examples of various HTTP status codes, helping you understand their meanings and proper usage in web development.</p>
<h3 id="conclusion">Conclusion</h3>
<p>By leveraging these learning and reference resources, you can deepen your understanding of key concepts and best practices in software development. Whether you&apos;re looking to master versioning guidelines, design patterns, or RESTful APIs, these resources offer valuable insights to help you grow as a developer.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Software Development - Read & Publish]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Explore a variety of resources that can enhance your software development journey. Whether you&apos;re looking for entertainment, tools for creating presentations, community platforms, or documentation solutions, these resources have you covered.</p>
<h3 id="entertainment">Entertainment</h3>
<h4 id="commitstrip"><a href="http://www.commitstrip.com/en/">CommitStrip</a></h4>
<p><em>Description:</em> Enjoy a daily strip that humorously captures the life of a coder. CommitStrip combines</p>]]></description><link>http://localhost:2368/software-development-read-publish/</link><guid isPermaLink="false">635e68b68ea41c669967e077</guid><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 30 Oct 2022 12:14:04 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1546422904-90eab23c3d7e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fG5ld3N8ZW58MHx8fHwxNjY3MDczMDQ0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1546422904-90eab23c3d7e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fG5ld3N8ZW58MHx8fHwxNjY3MDczMDQ0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Software Development - Read &amp; Publish"><p>Explore a variety of resources that can enhance your software development journey. Whether you&apos;re looking for entertainment, tools for creating presentations, community platforms, or documentation solutions, these resources have you covered.</p>
<h3 id="entertainment">Entertainment</h3>
<h4 id="commitstrip"><a href="http://www.commitstrip.com/en/">CommitStrip</a></h4>
<p><em>Description:</em> Enjoy a daily strip that humorously captures the life of a coder. CommitStrip combines funny anecdotes with topical tech news, making it a delightful read for developers.</p>
<h4 id="dont-hit-save"><a href="http://donthitsave.com/">Don&apos;t Hit Save</a></h4>
<p><em>Description:</em> Dive into a comic that brings humor to tech, the workplace, and indie game development. Don&apos;t Hit Save offers a light-hearted take on the challenges and quirks of being a developer.</p>
<h3 id="presentation">Presentation</h3>
<h4 id="speaker-deck"><a href="https://speakerdeck.com/">Speaker Deck</a></h4>
<p><em>Description:</em> Share your presentations online effortlessly with Speaker Deck. This platform makes it easy to upload and share your slides with a global audience, ensuring your ideas reach far and wide.</p>
<h4 id="slideshare"><a href="http://www.slideshare.net/">SlideShare</a></h4>
<p><em>Description:</em> Discover, share, and present professional content with SlideShare. As the world&apos;s largest community for sharing presentations and infographics, SlideShare is an invaluable resource for professionals looking to disseminate knowledge and insights.</p>
<h4 id="marp"><a href="https://marp.app/">Marp</a></h4>
<p><em>Description:</em> Create beautiful presentations using Markdown with Marp. This tool transforms your Markdown files into stunning slide decks, making it perfect for developers who prefer writing in plain text.</p>
<h3 id="community">Community</h3>
<h4 id="dzone"><a href="https://dzone.com/">DZone</a></h4>
<p><em>Description:</em> Join a vibrant community of software professionals on DZone. This platform publishes technical content and provides a space for developers to share knowledge, stay updated on industry trends, and enhance their skills.</p>
<h4 id="codeproject"><a href="https://www.codeproject.com/">CodeProject</a></h4>
<p><em>Description:</em> Access free source code and tutorials on CodeProject. This resource offers extensive programming help, particularly for Windows developers working with Visual Basic .NET and other .NET languages.</p>
<h4 id="programmableweb"><a href="http://www.programmableweb.com/">ProgrammableWeb</a></h4>
<p><em>Description:</em> Stay informed about the API economy with ProgrammableWeb. This site is a leading source of news and information about APIs, offering a comprehensive API directory and chronicling the evolution of web services.</p>
<h4 id="stackshare"><a href="http://stackshare.io/">StackShare</a></h4>
<p><em>Description:</em> Discover and discuss the best software tools and services on StackShare. This platform allows developers to compare tools, share experiences, and make informed decisions about the technologies they use.</p>
<h3 id="documentation">Documentation</h3>
<h4 id="read-the-docs"><a href="https://readthedocs.org/">Read the Docs</a></h4>
<p><em>Description:</em> Host and browse documentation effortlessly with Read the Docs. This service makes your documentation fully searchable and easy to find, supporting imports from major version control systems.</p>
<h4 id="docusaurus"><a href="https://docusaurus.io/">Docusaurus</a></h4>
<p><em>Description:</em> Build and maintain open-source documentation websites with ease using Docusaurus. This tool helps you create beautiful documentation sites that are easy to update and manage.</p>
<h4 id="asciinema"><a href="https://asciinema.org/">asciinema</a></h4>
<p><em>Description:</em> Record and share terminal sessions with asciinema. This free and open-source solution captures terminal activity and allows you to share it on the web, making it an excellent tool for creating tutorials and demonstrating workflows.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Utilize these resources to enhance your software development process, stay entertained, and connect with the community. Whether you need tools for presentations, community support, or comprehensive documentation solutions, these platforms provide the necessary support to help you succeed in your projects.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Change Data Capture (CDC) - MySQL]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>Change Data Capture (CDC) is a powerful process that tracks and captures changes made to data in a database, delivering those changes in real-time to downstream processes or systems. This capability is crucial for maintaining data consistency across distributed systems, enabling real-time analytics, and more.</p>
<p>In this post, we</p>]]></description><link>http://localhost:2368/change-data-capture-cdc-mysql/</link><guid isPermaLink="false">63567eb675328d2010ba2658</guid><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 24 Oct 2022 12:08:33 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1440288736878-766bd5839edb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNhdGNofGVufDB8fHx8MTY2NjYxMjk1OA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<img src="https://images.unsplash.com/photo-1440288736878-766bd5839edb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNhdGNofGVufDB8fHx8MTY2NjYxMjk1OA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Change Data Capture (CDC) - MySQL"><p>Change Data Capture (CDC) is a powerful process that tracks and captures changes made to data in a database, delivering those changes in real-time to downstream processes or systems. This capability is crucial for maintaining data consistency across distributed systems, enabling real-time analytics, and more.</p>
<p>In this post, we will briefly document the process of setting up CDC with MySQL using various tools such as Docker, Kafka, and Debezium.</p>
<h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Before we get started, ensure you have the following software installed on your machine:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a>: Used to containerize applications (MySQL, Zookeeper, Kafka, Debezium, schema-registry, kafka-ui, ksqldb-server, ksqldb-cli)</li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a>: A tool for defining and running multi-container Docker applications.</li>
<li><a href="https://www.mysql.com/">MySQL</a> Database: : The relational database management system.</li>
<li><a href="https://www.mysql.com/products/workbench/">MySQL Workbench</a>: Or on any other MySQL database client/console.</li>
</ul>
<h3 id="mysql-configuration-mysqlcnf">MySQL Configuration (<code>mysql.cnf</code>)</h3>
<p>To enable CDC on your MySQL database, you need to configure the MySQL server appropriately. Here&#x2019;s an example of what your mysql.cnf file should look like:</p>
<pre><code class="language-txt">[mysqld]
server-id         = 223344
log_bin           = mysql-bin
expire_logs_days  = 1
binlog_format     = row
</code></pre>
<ul>
<li><code>server-id</code>: A unique identifier for the server.</li>
<li><code>log_bin</code>: Enables binary logging.</li>
<li><code>expire_logs_days</code>: Specifies the number of days to retain binary logs.</li>
<li><code>binlog_format</code>: Sets the binary log format to &apos;row&apos;, which is necessary for CDC.</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="sample-project-setup">Sample Project Setup</h3>
<p>Follow these steps to set up the CDC environment</p>
<p><strong>1. Clone the repository</strong></p>
<p>First, clone the repository containing the project setup:</p>
<pre><code class="language-shell">git clone https://github.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL.git
cd Streaming_ETL_pipeline_MySQL
</code></pre>
<h3 id="running-the-application-via-docker-compose">Running the application via docker compose</h3>
<p><strong>2. Pull all required docker images</strong></p>
<p>Pull all the necessary Docker images defined in the Docker Compose file:</p>
<pre><code class="language-shell">docker compose -f docker-compose.yaml pull
</code></pre>
<p><strong>3. Start up the environment</strong></p>
<p>Start the Docker containers. The first time you do this, the images will be downloaded from the remote server, which may take some time:</p>
<pre><code class="language-shell">docker compose -f docker-compose.yaml up
</code></pre>
<p>You should see output indicating that various services are being created and started:</p>
<pre><code class="language-shell">Creating network &quot;streaming_etl_pipeline_mysql_webproxy&quot; with driver &quot;bridge&quot;
Creating zookeeper ... done
Creating kafka     ... done
Creating debezium           ... done
Creating cp-schema-registry ... done
Creating kafka-connect-01   ... done
Creating ksqldb-server      ... done
Creating ksqldb-cli         ... done
</code></pre>
<p><strong>4. Accessing Kafka Topics via Kakfka-UI</strong></p>
<p>Optionally, start <a href="https://github.com/provectus/kafka-ui">Kafka UI</a>, an open-source web UI for Apache Kafka Management</p>
<pre><code class="language-shell">docker run --name=kafka-ui --network=streaming_etl_pipeline_mysql_webproxy -p 8080:8080 -e KAFKA_CLUSTERS_0_NAME=local -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092 -d provectuslabs/kafka-ui:latest
</code></pre>
<p>Access the Kafka-UI console at: <strong><a href="http://localhost:8080">http://localhost:8080</a></strong></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/04-kafka-topics-before-debezium.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p><strong>5. Verify Everything is Running</strong></p>
<p>Ensure all containers are up and running:</p>
<pre><code class="language-shell">docker ps
</code></pre>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/09-docker-ps.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p><strong>IMPORTANT:</strong> If any components do not show &quot;Up&quot; under the <code>Status</code> column (e.g., they say &quot;Exit&quot;) then you must rectify this before continuing. As a first solution, try re-issuing the <code>docker-compose up -d</code> command.</p>
<p><strong>6. Access ksqlDB via ksqlDB-CLI</strong></p>
<p>Launch the KSQL CLI in another terminal window.</p>
<pre><code class="language-shell">docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
</code></pre>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/10-ksql-db-initial.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<h4 id="tear-down-the-stack">Tear down the stack</h4>
<p>To stop and remove the containers, run:</p>
<pre><code class="language-shell">docker stop kafka-ui
</code></pre>
<pre><code class="language-shell">docker rm kafka-ui
</code></pre>
<p>Then, bring down the rest of the environment:</p>
<pre><code class="language-shell">docker compose -f docker-compose.yaml down
</code></pre>
<p>You should see output indicating that the containers are being stopped and removed:</p>
<pre><code class="language-shell">Stopping ksqldb-cli       ... done
Stopping ksqldb-server    ... done
Stopping kafka-connect-01 ... done
Stopping debezium         ... done
Stopping kafka            ... done
Stopping zookeeper        ... done
Removing ksqldb-cli         ... done
Removing ksqldb-server      ... done
Removing kafka-connect-01   ... done
Removing cp-schema-registry ... done
Removing debezium           ... done
Removing kafka              ... done
Removing zookeeper          ... done
Removing network streaming_etl_pipeline_mysql_webproxy
</code></pre>
<p><em>If you want to preserve the state of all containers, run <code>docker-compose stop</code> instead.</em></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="initial-mysql-preparation">Initial MySQL preparation</h4>
<p>To get started with our MySQL database, we&apos;ll set up the initial schema, tables, and populate them with sample data. This preparation is crucial for ensuring that our database is ready for further development and integration.</p>
<h4 id="mysql">MySQL</h4>
<p><strong>1. Declare schema, user and permissions.</strong></p>
<p>First, create a new schema and user, then grant the necessary permissions to the user.</p>
<pre><code class="language-sql">-- create schema
CREATE SCHEMA streaming_etl_db;

-- use schema
USE streaming_etl_db;

-- Create user 
CREATE USER &apos;debezium&apos; IDENTIFIED WITH mysql_native_password BY &apos;Debezium@123#&apos;;

-- Grant privileges to user
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &apos;debezium&apos;;

-- Reload the grant tables in the mysql database enabling the changes to take effect without reloading or restarting mysql service
FLUSH PRIVILEGES;
</code></pre>
<p><strong>2. Create Tables</strong></p>
<p>Next, create the tables for storing geographical data, addresses, and personal information. Each table includes fields and constraints tailored to the data it will store.</p>
<pre><code class="language-sql">-- Table for geographical data
CREATE TABLE `geo` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;Unique ID for each entry.&apos;,
  `uuid` VARCHAR(50) DEFAULT (uuid()),
  `created_date_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;Field representing the date the entity containing the field was created.&apos;,
  `last_modified_date_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ,
  `lat` varchar(255) DEFAULT NULL,
  `lng` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT=&apos;Application Log.&apos;;
</code></pre>
<pre><code class="language-sql">-- Table for addresses
CREATE TABLE `address` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;Unique ID for each entry.&apos;,
  `uuid` VARCHAR(50) DEFAULT (uuid()),
  `created_date_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;Field representing the date the entity containing the field was created.&apos;,
  `last_modified_date_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ,
  `city` varchar(255) DEFAULT NULL,
  `zipcode` varchar(255) DEFAULT NULL,
  `state` varchar(255) DEFAULT NULL,
  `geo_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `FK_geo_id` (`geo_id`),
  CONSTRAINT `FKC_geo_id` FOREIGN KEY (`geo_id`) REFERENCES `geo` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
</code></pre>
<pre><code class="language-sql">-- Table for personal information
CREATE TABLE `person` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;Unique ID for each entry.&apos;,
  `uuid` VARCHAR(50) DEFAULT (uuid()),
  `created_date_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;Field representing the date the entity containing the field was created.&apos;,
  `last_modified_date_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ,
  `first_name` varchar(255) NOT NULL,
  `last_name` varchar(255) DEFAULT NULL,
  `email` varchar(255) DEFAULT NULL,
  `gender` varchar(255) DEFAULT NULL,
  `registration` datetime DEFAULT NULL,
  `age` int DEFAULT NULL,
  `address_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `FK_address_id` (`address_id`),
  CONSTRAINT `FKC_address_id` FOREIGN KEY (`address_id`) REFERENCES `address` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
</code></pre>
<p><strong>3. Insert Sample Data</strong></p>
<p>Populate the tables with some sample data to verify the structure and relationships.</p>
<pre><code class="language-sql">-- Insert sample data into geo table
INSERT INTO `streaming_etl_db`.`geo`(`lat`,`lng`)VALUES(&apos;la14&apos;,&apos;lo14&apos;);

-- Insert sample data into address table
INSERT INTO `streaming_etl_db`.`address`(`city`,`zipcode`,`state`,`geo_id`)VALUES(&apos;c14&apos;,&apos;z14&apos;,&apos;s14&apos;,1);

-- Insert sample data into person table
INSERT INTO `streaming_etl_db`.`person`(`first_name`,`last_name`,`email`,`gender`,`registration`,`age`,`address_id`)VALUES(&apos;fn14&apos;,&apos;ln14&apos;,&apos;example@domain.com&apos;,&apos;M&apos;,now(),34,1);
</code></pre>
<p><strong>4. Select Statements</strong></p>
<p>Retrieve data from the tables to ensure everything is set up correctly.</p>
<pre><code class="language-sql">-- Join query to retrieve data from person, address, and geo tables
SELECT * 
FROM streaming_etl_db.person p
LEFT JOIN streaming_etl_db.address a on a.id = p.address_id
LEFT JOIN streaming_etl_db.geo g on g.id = a.geo_id;
</code></pre>
<pre><code class="language-sql">-- Select all data from person table
SELECT * FROM streaming_etl_db.person;

-- Select all data from address table
SELECT * FROM streaming_etl_db.address;
</code></pre>
<p>By following these steps, you will have a fully prepared MySQL database with the necessary schema, tables, and sample data. This setup will serve as a foundation for further development and data integration.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="debezium-registration">Debezium Registration</h3>
<p>In this section, we&apos;ll set up and register the Debezium connector to monitor changes in our MySQL database and stream them to Kafka. Debezium is an open-source distributed platform for change data capture (CDC).</p>
<p><strong>Registering the Debezium Connector</strong></p>
<p>To register the Debezium connector, use the following curl command.</p>
<pre><code class="language-shell">curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; 127.0.0.1:8083/connectors/ -d &apos;{
  &quot;name&quot;: &quot;streaming_ETL_pipeline_MySQL-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.hostname&quot;: &quot;172.17.0.1&quot;,
    &quot;database.port&quot;: &quot;3306&quot;,
    &quot;database.user&quot;: &quot;debezium&quot;,
    &quot;database.password&quot;: &quot;Debezium@123#&quot;,
    &quot;database.server.name&quot;: &quot;mysql&quot;,
	  &quot;database.server.id&quot;: &quot;223344&quot;,
    &quot;database.include.list&quot;: &quot;streaming_etl_db&quot;,
	  &quot;database.allowPublicKeyRetrieval&quot;: true,
	  &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
	  &quot;database.history.kafka.topic&quot;: &quot;mysql-streaming_etl_db-person&quot;,
	  &quot;time.precision.mode&quot;: &quot;connect&quot;,
    &quot;include.schema.changes&quot;: false,
    &quot;transforms&quot;: &quot;unwrap,dropTopicPrefix&quot;,
	  &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.ExtractNewRecordState&quot;,
	  &quot;transforms.dropTopicPrefix.type&quot;:&quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
	  &quot;transforms.dropTopicPrefix.regex&quot;:&quot;asgard.demo.(.*)&quot;,
	  &quot;transforms.dropTopicPrefix.replacement&quot;:&quot;$1&quot;,
	  &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
	  &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,
	  &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
	  &quot;value.converter.schemas.enable&quot;: &quot;false&quot;
  }
}&apos;
</code></pre>
<p>This command will:</p>
<ul>
<li>Create a new connector named streaming_ETL_pipeline_MySQL-connector.</li>
<li>Configure it to monitor the streaming_etl_db schema on the MySQL instance.</li>
<li>Stream the change data to a Kafka topic named mysql-streaming_etl_db-person.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/01-debezium-registration.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p>After registering the Debezium connector, you can check the status and information of the connectors.</p>
<p><strong>Checking Connector Status</strong><br>
To verify the status of the connectors, visit:</p>
<p><a href="http://localhost:8083/connectors?expand=info&amp;expand=status">http://localhost:8083/connectors?expand=info&amp;expand=status</a></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/02-debezium-connectors.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p>To check the status of the specific Debezium connector, visit:</p>
<p><a href="http://localhost:8083/connectors/streaming_ETL_pipeline_MySQL-connector/status">http://localhost:8083/connectors/streaming_ETL_pipeline_MySQL-connector/status</a></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/03-debezium-connector-status.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p><strong>Kafka UI</strong></p>
<p>You can also use the Kafka UI to inspect the topics and messages being streamed. Visit the Kafka UI at:</p>
<p><a href="http://localhost:8080">http://localhost:8080</a></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/05-kafka-topics-after-registration.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p><strong>Viewing the Person Topic</strong></p>
<p>Finally, check the messages in the person topic to ensure data is being streamed correctly:</p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/06-kafka-topic-message.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h3>
<p>In this section, we will explore how to access and use ksqlDB via the ksqlDB-CLI to interact with Kafka topics, create streams, perform stream-stream joins, and finally, sink the enriched stream back to MySQL.</p>
<p><strong>Checking Topics, Streams, and Tables</strong><br>
Start by checking the available topics, streams, and tables in ksqlDB.</p>
<pre><code class="language-sql">-- Display available topics
SHOW TOPICS;

-- Display available streams
SHOW STREAMS;

-- Display available tables
SHOW TABLES;
</code></pre>
<p><strong>Declaring Streams</strong><br>
To begin processing data, declare the necessary streams. Set the offset to the earliest to ensure you capture all existing messages.</p>
<pre><code class="language-sql">-- Set offset to the earliest
SET &apos;auto.offset.reset&apos; = &apos;earliest&apos;;
</code></pre>
<p>Create streams to capture data from the Kafka topics corresponding to the MySQL tables.</p>
<pre><code class="language-sql">-- Create stream for the person topic
CREATE STREAM PERSON_STREAM (
  id BIGINT,
  uuid VARCHAR,
  created_date_time TIMESTAMP,
  last_modified_date_time TIMESTAMP,
  name VARCHAR,
  username VARCHAR,
  address_id BIGINT
) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;, VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">-- Create stream for the address topic
CREATE STREAM ADDRESS_STREAM (
  id BIGINT,
  uuid VARCHAR,
  created_date_time TIMESTAMP,
  last_modified_date_time TIMESTAMP,
  city VARCHAR,
  street VARCHAR,
  suite VARCHAR,
  zipcode VARCHAR,
  geo_id BIGINT
) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;, VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<p><strong>Querying Streams</strong><br>
Retrieve data from the streams to ensure they are correctly set up.</p>
<pre><code class="language-sql">-- Select a single record from the PERSON_STREAM
SELECT * FROM PERSON_STREAM EMIT CHANGES LIMIT 1;
</code></pre>
<pre><code class="language-sql">+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|ID                       |UUID                     |CREATED_DATE_TIME        |LAST_MODIFIED_DATE_TIME  |NAME                     |USERNAME                 |ADDRESS_ID               |
+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|1                        |ce8d2120-1f93-11ed-8647-0|2022-08-19T13:22:00.000  |2022-08-19T13:22:00.000  |d14                      |dbz14                    |1                        |
|                         |c9a3cfadc50              |                         |                         |                         |                         |                         |
Limit Reached
Query terminated
</code></pre>
<p>Describe the stream to get details about its schema.</p>
<pre><code class="language-sql">-- Describe the PERSON_STREAM
DESCRIBE PERSON_STREAM;

-- Select all records from PERSON_STREAM
SELECT * FROM PERSON_STREAM;
</code></pre>
<p><strong>Stream-Stream Join</strong><br>
Perform a join between the <code>PERSON_STREAM</code> and <code>ADDRESS_STREAM</code> to create an enriched stream combining data from both.</p>
<pre><code class="language-sql">-- Create an enriched stream by joining PERSON_STREAM and ADDRESS_STREAM
CREATE STREAM PERSON_ADDRESS_ENRICHED_STREAM WITH (
FORMAT=&apos;JSON&apos;, 
KAFKA_TOPIC=&apos;person_address_enriched&apos;, 
PARTITIONS=1, 
REPLICAS=1
) AS 
SELECT
  P.ID P_ID,
  A.ID A_ID,
  P.NAME NAME,
  A.CITY CITY
FROM PERSON_STREAM P
LEFT OUTER JOIN ADDRESS_STREAM A WITHIN 1 HOURS GRACE PERIOD 30 MINUTES 
ON ((A.ID = P.ADDRESS_ID))
EMIT CHANGES;
</code></pre>
<h4 id="kafka-sink-mysql-db">Kafka Sink MySQL DB</h4>
<p>Finally, create a sink connector to write the enriched stream back to the MySQL database.</p>
<pre><code class="language-sql">-- Create a sink connector for the enriched stream
CREATE SINK CONNECTOR SINK_PERSON_ADDRESS_ENRICHED_STREAM WITH (
  &apos;connector.class&apos;            = &apos;io.confluent.connect.jdbc.JdbcSinkConnector&apos;,
  &apos;connection.url&apos;             = &apos;jdbc:mysql://172.17.0.1:3306/&apos;,
  &apos;connection.user&apos;            = &apos;debezium&apos;,
  &apos;connection.password&apos;        = &apos;Debezium@123#&apos;,
  &apos;topics&apos;                     = &apos;PERSON_ADDRESS_ENRICHED_STREAM&apos;,
  &apos;key.converter&apos;              = &apos;org.apache.kafka.connect.json.JsonConverter&apos;,
  &apos;key.converter.schemas.enable&apos; = &apos;false&apos;,
  &apos;value.converter&apos;            = &apos;org.apache.kafka.connect.json.JsonConverter&apos;,
  &apos;value.converter.schemas.enable&apos; = &apos;false&apos;
);
</code></pre>
<p>By following these steps, you can access ksqlDB via the ksqlDB-CLI, create and query streams, perform joins, and sink the enriched data back to MySQL.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h3>
<p>In this section, we&apos;ll delve into how to access ksqlDB using the ksqlDB-CLI, focusing on creating and querying tables, and performing joins.</p>
<p><strong>Checking Topics, Streams, and Tables</strong><br>
Start by listing the available topics, streams, and tables in your ksqlDB environment to ensure you have everything set up correctly.</p>
<pre><code class="language-sql">-- Display available topics
SHOW TOPICS;

-- Display available streams
SHOW STREAMS;

-- Display available tables
SHOW TABLES;
</code></pre>
<p><strong>Declaring Tables</strong><br>
Next, declare tables to represent your Kafka topics in ksqlDB. These tables will enable you to perform SQL-like queries on streaming data.</p>
<pre><code class="language-sql">-- Create table for the person topic
CREATE TABLE PERSON (
  id BIGINT PRIMARY KEY,
  uuid VARCHAR,
  created_date_time TIMESTAMP,
  last_modified_date_time TIMESTAMP,
  name VARCHAR,
  username VARCHAR,
  address_id BIGINT
) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;, VALUE_FORMAT=&apos;JSON&apos;);

-- Create table for the address topic
CREATE TABLE ADDRESS (
  id BIGINT PRIMARY KEY,
  uuid VARCHAR,
  created_date_time TIMESTAMP,
  last_modified_date_time TIMESTAMP,
  city VARCHAR,
  street VARCHAR,
  suite VARCHAR,
  zipcode VARCHAR,
  geo_id BIGINT
) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;, VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<p><strong>Querying Tables</strong><br>
Retrieve data from the tables to verify that they are correctly set up and ingesting data from the corresponding Kafka topics.</p>
<pre><code class="language-sql">-- Select a single record from the PERSON table
SELECT * FROM PERSON EMIT CHANGES LIMIT 1;

-- Select a single record from the ADDRESS table
SELECT * FROM ADDRESS EMIT CHANGES LIMIT 1;
</code></pre>
<p><strong>Performing Joins</strong><br>
Join the <strong>PERSON</strong> and <strong>ADDRESS</strong> tables to enrich the data, combining fields from both tables based on a common key.</p>
<pre><code class="language-sql">-- Perform a left join between PERSON and ADDRESS tables
SELECT 
  P.NAME,
  A.CITY
FROM PERSON P
LEFT JOIN ADDRESS A 
ON A.id = P.address_id
EMIT CHANGES 
LIMIT 1;
</code></pre>
<pre><code class="language-sql">-- Perform an inner join between PERSON and ADDRESS tables
SELECT 
  P.NAME, 
  A.CITY
FROM PERSON P
INNER JOIN ADDRESS A
ON A.id = P.address_id
EMIT CHANGES
LIMIT 1;
</code></pre>
<p><strong>Creating Enriched Table</strong><br>
Create a new table to store the results of the join, providing a persistent view of the enriched data.</p>
<pre><code class="language-sql">-- Create a table for the enriched person and address data
CREATE TABLE PERSON_ADDRESS_ENRICHED (
  P_ID BIGINT,
  A_ID BIGINT,
  NAME VARCHAR,
  CITY VARCHAR
) WITH (KAFKA_TOPIC=&apos;person_address_enriched&apos;, VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<p><strong>Managing Tables</strong><br>
You can also manage your tables by dropping them when they are no longer needed.</p>
<pre><code class="language-sql">-- Drop the PERSON table if it exists
DROP TABLE IF EXISTS PERSON;
</code></pre>
<p>By following these steps, you can effectively use ksqlDB to create and manage tables, perform joins, and enrich your streaming data, enabling powerful real-time data processing capabilities in your ETL pipeline.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="conclusion">Conclusion</h3>
<p>In this tutorial, we walked through the process of setting up a streaming ETL pipeline using MySQL, Debezium, Kafka, and ksqlDB. We began by preparing our MySQL database with the necessary schema, tables, and sample data. Then, we registered the Debezium connector to monitor changes and stream them to Kafka. Using ksqlDB, we created and queried streams and tables, performed joins to enrich our data, and finally, sank the enriched data back into MySQL.</p>
<p>By leveraging these powerful tools, you can build robust and scalable real-time data processing pipelines that enable you to respond to changes in your data as they happen. This setup provides a foundation for more complex stream processing tasks and opens up possibilities for real-time analytics, monitoring, and more.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Useful Websites/Services for Software Development]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Discover a curated list of websites and services that can enhance your software development workflow. From platforms offering cloud services to tools for data analysis and design inspiration, these resources cover various aspects of the development lifecycle.</p>
<h2 id="platform-as-a-service-paasfree-tier">Platform as a Service (PaaS - Free Tier)</h2>
<ul>
<li>
<p><strong><a href="https://www.heroku.com/">Heroku</a></strong><br>
Heroku is a cloud</p></li></ul>]]></description><link>http://localhost:2368/useful-websites-for-development/</link><guid isPermaLink="false">63538c8e3be8822ef99ed643</guid><category><![CDATA[Tools]]></category><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sat, 22 Oct 2022 06:27:32 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1648652678596-d3873bd0c157?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxoZWxwZXJ8ZW58MHx8fHwxNjY2NDE5ODg0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1648652678596-d3873bd0c157?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxoZWxwZXJ8ZW58MHx8fHwxNjY2NDE5ODg0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Useful Websites/Services for Software Development"><p>Discover a curated list of websites and services that can enhance your software development workflow. From platforms offering cloud services to tools for data analysis and design inspiration, these resources cover various aspects of the development lifecycle.</p>
<h2 id="platform-as-a-service-paasfree-tier">Platform as a Service (PaaS - Free Tier)</h2>
<ul>
<li>
<p><strong><a href="https://www.heroku.com/">Heroku</a></strong><br>
Heroku is a cloud platform that lets you build, deliver, monitor, and scale apps effortlessly. With its easy-to-use interface and extensive documentation, Heroku is a popular choice among developers for deploying applications.</p>
</li>
<li>
<p><strong><a href="https://aws.amazon.com/">Amazon Web Services</a></strong><br>
Amazon Web Services (AWS) provides a secure cloud services platform, offering a wide range of services including compute power, database storage, content delivery, and more. With its pay-as-you-go pricing model and global infrastructure, AWS is trusted by businesses of all sizes.</p>
</li>
<li>
<p><strong><a href="https://mlab.com/">mLab</a></strong><br>
mLab is a Database-as-a-Service for MongoDB, offering a simple and scalable solution for managing MongoDB databases in the cloud. With features like automated backups and 24/7 support, mLab is a reliable choice for hosting MongoDB databases.</p>
</li>
<li>
<p><strong><a href="https://www.netlify.com/">Netlify</a></strong><br>
Build, deploy, and manage modern web projects with Netlify. Featuring continuous deployment, serverless functions, and form handling, Netlify simplifies the process of building and deploying websites.</p>
</li>
</ul>
<h2 id="data">Data</h2>
<ul>
<li>
<p><strong><a href="https://www.kaggle.com/datasets">Kaggle Datasets</a></strong><br>
Kaggle Datasets is the premier destination for discovering and analyzing publicly-available data. With thousands of datasets covering a wide range of topics, Kaggle Datasets is a valuable resource for data scientists and analysts.</p>
</li>
<li>
<p><strong><a href="http://academictorrents.com/">Academic Torrents</a></strong><br>
Academic Torrents is a distributed system for sharing enormous datasets for researchers, by researchers. With its peer-to-peer architecture and comprehensive dataset collection, Academic Torrents is a valuable resource for academic research.</p>
</li>
<li>
<p><strong><a href="https://www.mockaroo.com/">Mockaroo</a></strong><br>
Mockaroo is a powerful tool for generating realistic test data in various formats, including CSV, JSON, SQL, and Excel. With features like custom data generation and batch processing, Mockaroo is an essential tool for testing and development.</p>
</li>
<li>
<p><strong><a href="https://github.com/kapilratnani/JSON-Viewer">JSON viewer</a></strong><br>
JSON viewer is a plugin for Notepad++ that allows you to view JSON strings in a tree view format. With its intuitive interface and powerful features, JSON viewer makes it easy to navigate and analyze JSON data.</p>
</li>
<li>
<p><strong><a href="http://konklone.io/json/">JSON to CSV Converter</a></strong><br>
JSON to CSV Converter is a free, in-browser tool for converting JSON data to CSV format. With its simple and intuitive interface, JSON to CSV Converter makes it easy to convert and download JSON data in CSV format.</p>
</li>
<li>
<p><strong><a href="http://json2table.com/">JSON2table</a></strong><br>
JSON2table is a tool for visualizing JSON data in table and tree view format. With its intuitive interface and powerful features, JSON2table makes it easy to analyze and validate JSON data.</p>
</li>
</ul>
<h2 id="design">Design</h2>
<ul>
<li>
<p><strong><a href="https://dribbble.com/">Dribbble</a></strong><br>
Dribbble is a community of designers sharing their work and providing inspiration to others. With its vast collection of design samples and active community, Dribbble is a valuable resource for designers looking for inspiration and feedback.</p>
</li>
<li>
<p><strong><a href="http://www.uplabs.com/">UpLabs</a></strong><br>
UpLabs curates the best of design and development inspiration, resources, and freebies. With its comprehensive collection of design assets and active community, UpLabs is a valuable resource for designers and developers alike.</p>
</li>
<li>
<p><strong><a href="http://www.materialup.com/">MaterialUp</a></strong><br>
MaterialUp curates the best of Material Design, Google&apos;s design language for creating intuitive and beautiful user interfaces. With its extensive collection of Material Design examples and resources, MaterialUp is a valuable resource for designers and developers working with Material Design.</p>
</li>
<li>
<p><strong><a href="http://www.ios.uplabs.com/">iOSUp</a></strong><br>
iOSUp curates the best of iOS and macOS design, providing inspiration and resources for designers and developers working on Apple platforms. With its comprehensive collection of iOS and macOS design examples and resources, iOSUp is a valuable resource for Apple developers.</p>
</li>
<li>
<p><strong><a href="http://www.site.uplabs.com/">SiteUp</a></strong><br>
SiteUp curates the best of web design, providing inspiration and resources for designers and developers working on web projects. With its extensive collection of web design examples and resources, SiteUp is a valuable resource for web developers.</p>
</li>
</ul>
<h2 id="java">Java</h2>
<ul>
<li>
<p><strong><a href="https://www.codota.com/">Codota</a></strong><br>
Codota is an AI Pair Programmer that provides contextual code suggestions based on your code. With its machine learning algorithms and extensive code database, Codota helps developers write code faster and more efficiently.</p>
</li>
<li>
<p><strong><a href="http://www.java2s.com/">java2s</a></strong><br>
java2s is a repository of code examples and tutorials organized by topic. With its extensive collection of Java code examples and tutorials, java2s is a valuable resource for Java developers.</p>
</li>
</ul>
<h2 id="various">Various</h2>
<ul>
<li>
<p><strong><a href="http://codebeautify.org/">CodeBeautify</a></strong><br>
CodeBeautify offers a suite of online tools for developers, including beautifiers, validators, minifiers, and converters for various programming languages and formats. With its comprehensive collection of tools, CodeBeautify simplifies common development tasks and improves productivity.</p>
</li>
<li>
<p><strong><a href="https://www.freeformatter.com/">Free Formatter</a></strong><br>
Free Formatter offers a variety of tools for developers, including formatters, validators, code minifiers, string escapers, encoders and decoders, and more. With its extensive collection of tools, Free Formatter is a valuable resource for developers working with various data formats and programming languages.</p>
</li>
<li>
<p><strong><a href="http://jsbin.com/">JS Bin</a></strong><br>
JS Bin is an open-source collaborative web development debugging tool. With its live editing and real-time collaboration features, JS Bin makes it easy to debug and experiment with web code.</p>
</li>
<li>
<p><strong><a href="https://fonts.google.com/">Google Fonts</a></strong><br>
Google Fonts offers a vast collection of free, open-source fonts optimized for the web. With its easy-to-use interface and extensive collection of fonts, Google Fonts is a valuable resource for designers and developers looking to enhance their web projects with beautiful typography.</p>
</li>
<li>
<p><strong><a href="https://coggle.it/">Coggle</a></strong><br>
Coggle is a collaborative mind-mapping tool that helps you make sense of complex ideas and processes. With its intuitive interface and real-time collaboration features, Coggle makes it easy to brainstorm, organize, and visualize your ideas.</p>
</li>
<li>
<p><strong><a href="https://bestpractices.coreinfrastructure.org/">CII Best Practices Badge Program</a></strong><br>
The CII Best Practices Badge Program is a way for FLOSS projects to show that they follow best practices. With its comprehensive set of guidelines and criteria, the CII Best Practices Badge Program helps projects improve their security, reliability, and sustainability.</p>
</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="text-to-ascii-art">Text to ASCII Art</h2>
<ul>
<li><strong><a href="http://patorjk.com/software/taag/">TAAG</a></strong><br>
TAAG is a text-to-ASCII art generator that allows you to convert text into visually appealing ASCII art. With its customizable options and easy-to-use interface, TAAG is a fun tool for creating unique text designs.</li>
</ul>
<h2 id="graphic-resources">Graphic Resources</h2>
<ul>
<li><strong><a href="https://www.freepik.com/">Freepik</a></strong><br>
Freepik offers a wide range of graphic resources for everyone, including free vectors, stock photos, PSD files, and icons. With its extensive collection of high-quality graphics, Freepik is a valuable resource for designers and developers alike.</li>
</ul>
<h2 id="productivity-tracking">Productivity Tracking</h2>
<ul>
<li><strong><a href="https://wakatime.com/">WakaTime</a></strong><br>
WakaTime helps you quantify your coding by providing metrics, insights, and time tracking automatically generated from your programming activity. With its detailed reports and integrations with popular IDEs, WakaTime helps you understand and improve your coding habits.</li>
</ul>
<h2 id="development-utilities">Development Utilities</h2>
<ul>
<li>
<p><strong><a href="https://devops.datenkollektiv.de/banner.txt/index.html">Online Spring Boot Banner Generator</a></strong><br>
The Online Spring Boot Banner Generator allows you to create custom banners for your Spring Boot applications. With its easy-to-use interface and customizable options, the generator makes it simple to add a personalized touch to your projects.</p>
</li>
<li>
<p><strong><a href="http://htmlpreview.github.io/">GitHub and BitBucket HTML Preview</a></strong><br>
GitHub and BitBucket HTML Preview is a handy tool that allows you to preview HTML files hosted on GitHub or BitBucket. With its simple interface and seamless integration with version control platforms, HTML Preview makes it easy to view and share HTML content online.</p>
</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Change Data Capture (CDC) with PostgreSQL and Debezium]]></title><description><![CDATA[<p>Change Data Capture (CDC) has revolutionized the way we interact with data, enabling real-time tracking and propagation of changes within databases. In this blog post, we&apos;ll delve into harnessing the power of CDC using two formidable tools: PostgreSQL and Debezium.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>Change Data Capture (CDC) is the backbone</p>]]></description><link>http://localhost:2368/change-data-capture-cdc-with-p/</link><guid isPermaLink="false">62f8cb7e618689890e6a486e</guid><category><![CDATA[Change Data Capture]]></category><category><![CDATA[Docker]]></category><category><![CDATA[PostgreSQL]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 14 Aug 2022 11:09:21 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1508423134147-addf71308178?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDMwfHxsZW5zfGVufDB8fHx8MTY2MDQ3MjIxMA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1508423134147-addf71308178?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDMwfHxsZW5zfGVufDB8fHx8MTY2MDQ3MjIxMA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Change Data Capture (CDC) with PostgreSQL and Debezium"><p>Change Data Capture (CDC) has revolutionized the way we interact with data, enabling real-time tracking and propagation of changes within databases. In this blog post, we&apos;ll delve into harnessing the power of CDC using two formidable tools: PostgreSQL and Debezium.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>Change Data Capture (CDC) is the backbone of modern data architectures, allowing seamless tracking and delivery of database changes to downstream systems. PostgreSQL, revered for its extensibility and SQL compliance, serves as our foundation for data storage. Complementing it is Debezium, a versatile open-source platform dedicated to change data capture.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>To embark on this journey, ensure you have <a href="https://www.docker.com/">Docker</a> installed, facilitating the effortless setup of PostgreSQL, Zookeeper, Kafka, Debezium, and schema-registry components. <a href="https://docs.docker.com/compose/">Docker Compose</a> streamlines the orchestration of these services, ensuring a hassle-free setup process.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="getting-started">Getting Started</h2>
<h3 id="setup">Setup</h3>
<p>To kickstart our CDC adventure, we utilize a Docker Compose configuration, encapsulating the required services. This configuration, detailed in the provided docker-compose.yaml file, lays the groundwork for our PostgreSQL and Debezium setup.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: html--><script src="https://gist.github.com/AnanthaRajuC/134394876375cc861dfe7d7d223918ab.js"></script><!--kg-card-end: html--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/08/12.png" class="kg-image" alt="Change Data Capture (CDC) with PostgreSQL and Debezium" loading="lazy" width="1905" height="331" srcset="http://localhost:2368/content/images/size/w600/2022/08/12.png 600w, http://localhost:2368/content/images/size/w1000/2022/08/12.png 1000w, http://localhost:2368/content/images/size/w1600/2022/08/12.png 1600w, http://localhost:2368/content/images/2022/08/12.png 1905w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="db-configuration">DB Configuration</h3>
<p>In PostgreSQL, we define a <strong>STUDENT</strong> table with essential columns, setting the stage for our CDC operations. Leveraging PostgreSQL&apos;s robust functionality, we configure the <strong>STUDENT</strong> table with full REPLICA IDENTITY, ensuring comprehensive change tracking.</p>
<pre><code class="language-sql">CREATE TABLE STUDENT(id INTEGER PRIMARY KEY, name VARCHAR);

ALTER TABLE public.student REPLICA IDENTITY FULL;

SELECT * FROM STUDENT;
</code></pre>
<p><img src="http://localhost:2368/content/images/2022/08/14.png" alt="Change Data Capture (CDC) with PostgreSQL and Debezium" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="setup-debezium-connector">Setup debezium Connector</h3>
<p>Debezium seamlessly integrates with PostgreSQL via a dedicated connector. With the provided script, <a href="https://gist.github.com/AnanthaRajuC/49ec813281221879e412a4e51d1a3136#file-debezium-postgresql-connector-sh">debezium-postgresql-connector.sh</a>, we initiate and configure the connector, enabling seamless communication between our database and Debezium.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: html--><script src="https://gist.github.com/AnanthaRajuC/49ec813281221879e412a4e51d1a3136.js"></script><!--kg-card-end: html--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/08/15.png" class="kg-image" alt="Change Data Capture (CDC) with PostgreSQL and Debezium" loading="lazy" width="1904" height="508" srcset="http://localhost:2368/content/images/size/w600/2022/08/15.png 600w, http://localhost:2368/content/images/size/w1000/2022/08/15.png 1000w, http://localhost:2368/content/images/size/w1600/2022/08/15.png 1600w, http://localhost:2368/content/images/2022/08/15.png 1904w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="tail-kafka-cdc-topic">Tail kafka CDC topic</h3>
<p>Monitoring the CDC topic in Kafka provides real-time insights into database changes. By tailing the Kafka CDC topic, as illustrated in <a href="https://gist.github.com/AnanthaRajuC/2dba6d7e3bde7e1def6e7bf902c08b0e#file-tail_cdc_kafka_topic-sh">tail_cdc_kafka_topic.sh</a>, we gain immediate visibility into data modifications, empowering us to track changes effectively.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: html--><script src="https://gist.github.com/AnanthaRajuC/2dba6d7e3bde7e1def6e7bf902c08b0e.js"></script><!--kg-card-end: html--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/08/16.png" class="kg-image" alt="Change Data Capture (CDC) with PostgreSQL and Debezium" loading="lazy" width="1905" height="168" srcset="http://localhost:2368/content/images/size/w600/2022/08/16.png 600w, http://localhost:2368/content/images/size/w1000/2022/08/16.png 1000w, http://localhost:2368/content/images/size/w1600/2022/08/16.png 1600w, http://localhost:2368/content/images/2022/08/16.png 1905w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="add-update-delete-records">Add, Update, Delete Records</h3>
<p>To witness CDC in action, we simulate data modifications within our PostgreSQL database. By executing SQL statements to add, update, and delete records in the STUDENT table, we trigger CDC events that are seamlessly propagated to the Kafka CDC topic.</p>
<pre><code class="language-sql">-- Add records
INSERT INTO STUDENT(id,name) VALUES (1,&apos;JOHN&apos;);
INSERT INTO STUDENT(id,name) VALUES (2,&apos;JANE&apos;);

-- Update record
UPDATE STUDENT SET name=&apos;JOHNNY&apos; WHERE ID=1;
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/08/18.png" class="kg-image" alt="Change Data Capture (CDC) with PostgreSQL and Debezium" loading="lazy" width="1907" height="340" srcset="http://localhost:2368/content/images/size/w600/2022/08/18.png 600w, http://localhost:2368/content/images/size/w1000/2022/08/18.png 1000w, http://localhost:2368/content/images/size/w1600/2022/08/18.png 1600w, http://localhost:2368/content/images/2022/08/18.png 1907w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p>In conclusion, the synergy between PostgreSQL and Debezium unlocks unparalleled capabilities in Change Data Capture, revolutionizing how we perceive and interact with real-time data changes. Through meticulous setup and intuitive configuration, harnessing CDC becomes not just a possibility but a powerful reality, propelling data-driven decision-making to new heights.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>