<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Anantha Raju C]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Anantha Raju C</title><link>http://localhost:2368/</link></image><generator>Ghost 5.8</generator><lastBuildDate>Sat, 15 Jun 2024 13:38:08 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Building a Real-Time Data Pipeline Using Python, MySQL, Kafka, and ClickHouse]]></title><description><![CDATA[This post explores a data pipeline architecture for real-time data streaming, processing, and visualization using popular open-source tools like Python, MySQL, Kafka, and ClickHouse.]]></description><link>http://localhost:2368/building-a-real-time-data-pipeline-using-python-mysql-kafka-and-clickhouse/</link><guid isPermaLink="false">666522ff85dc8624db48eecf</guid><category><![CDATA[Apache Kafka]]></category><category><![CDATA[Change Data Capture]]></category><category><![CDATA[ClickHouse]]></category><category><![CDATA[Data]]></category><category><![CDATA[dbt]]></category><category><![CDATA[MySQL]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 09 Jun 2024 03:48:53 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1593583845845-7d67ede93328?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQ4fHxwaXBlbGluZXxlbnwwfHx8fDE3MTc5MDQxNDd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1593583845845-7d67ede93328?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQ4fHxwaXBlbGluZXxlbnwwfHx8fDE3MTc5MDQxNDd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Building a Real-Time Data Pipeline Using Python, MySQL, Kafka, and ClickHouse"><p>In the modern data-driven world, real-time data processing and analytics are critical for making timely and informed decisions. This post explores a data pipeline architecture for real-time data streaming, processing, and visualization using popular open-source tools like Python, MySQL, Kafka, and ClickHouse.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><img src="http://localhost:2368/content/images/2024/06/streaming_etl.png" alt="Building a Real-Time Data Pipeline Using Python, MySQL, Kafka, and ClickHouse" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="code-and-documentation">Code and Documentation</h3>
<p>The complete code for this data pipeline, along with detailed documentation of all steps, is available on <a href="https://github.com/AnanthaRajuC/Streaming-ETL-Pipeline-for-Realtime-Analytics">GitHub</a>.</p>
<h4 id="key-features-of-the-github-project">Key Features of the GitHub Project</h4>
<ul>
<li><strong>Step-by-Step Documentation</strong>: The GitHub repository includes detailed instructions for setting up and running the entire pipeline. This makes it easy for users to replicate the setup and understand each component&apos;s role.</li>
<li><strong>Sample Data Generator</strong>: Python scripts to simulate real-time data generation are provided, helping users to test and experiment with the pipeline.</li>
<li><strong>Configuration Files</strong>: Pre-configured settings for MySQL, Debezium, Kafka, and ClickHouse are included to streamline the setup process.</li>
<li><strong>Stream Processing with ksql</strong>: Examples of how to use ksql for real-time data enrichment and transformation are provided, showcasing the power of stream processing.</li>
<li><strong>Integration with BI Tools</strong>: Guidance on how to connect Metabase and Apache Superset to the data pipeline for visualization and reporting is available.</li>
</ul>
<p>By following the instructions and leveraging the provided code, users can quickly set up a robust and scalable real-time data pipeline tailored to their specific needs.</p>
<h3 id="data-generation">Data Generation</h3>
<p>The journey begins with the <strong>Data Generator</strong>. In our example, Python is used to simulate and generate event data. This could be any real-time data source, such as IoT devices, transaction logs, or user interactions on a website.</p>
<h3 id="oltp-online-transaction-processing-data-source">OLTP (Online Transaction Processing) Data Source</h3>
<p>The generated data is initially stored in a <strong>MySQL</strong> database. MySQL serves as the OLTP data source, managing high transaction volumes and maintaining data integrity. It logs changes in real-time using a binlog (binary log), which captures all updates made to the database.</p>
<h3 id="change-data-capture-cdc-with-debezium">Change Data Capture (CDC) with Debezium</h3>
<p>To stream changes from the MySQL database in real-time, <strong>Debezium</strong> is employed. Debezium is a distributed platform that captures row-level changes in databases. It acts as a MySQL Source Connector, monitoring the MySQL binlog for data changes and converting these changes into events.</p>
<h3 id="stream-processing-platform-with-kafka">Stream Processing Platform with Kafka</h3>
<p>These change events are then sent to <strong>Kafka</strong>, a distributed streaming platform capable of handling large volumes of data with low latency. Kafka organizes these events into topics. Each topic is a log of messages, which can be processed in real-time.</p>
<p>To enrich and process these messages further, a stream processing tool like <strong>ksql</strong> (Kafka Stream Processing) can be used. ksql enables real-time data transformations and filtering directly on Kafka topics.</p>
<h3 id="analytical-database-with-clickhouse">Analytical Database with ClickHouse</h3>
<p>For analytical processing, the data is transferred from Kafka to <strong>ClickHouse</strong>, an OLAP (Online Analytical Processing) data warehouse known for its high performance and efficiency in handling analytical queries. ClickHouse consumes Kafka topics through its Kafka Table Engine. The data is then transformed into Materialized Views and stored in MergeTree Tables, optimized for fast query performance.</p>
<h3 id="business-intelligence-and-data-visualization">Business Intelligence and Data Visualization</h3>
<p>Finally, the processed and aggregated data is ready for visualization and analysis. Tools like <strong>Metabase</strong> and <strong>Apache Superset</strong> can connect to ClickHouse, providing interactive dashboards and reports. Additionally, tools like <strong>dbt</strong> (data build tool) can be used for data transformation and modeling, enabling more advanced analytics and insights.</p>
<p>Other applications of this data pipeline include feeding into <strong>ML models</strong> for predictive analytics, performing <strong>operational analytics</strong>, and various other data-driven applications.</p>
<h3 id="conclusion">Conclusion</h3>
<p>This data pipeline demonstrates a architecture for real-time data processing. By leveraging tools like Python, MySQL, Debezium, Kafka, ClickHouse, and various BI tools, organizations can ensure that they have timely and accurate data for decision-making and analytics. The flexibility of this setup make it suitable for a wide range of use cases, from operational monitoring to analytics etc.,</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster]]></title><description><![CDATA[This post briefly captures the usage of dbt-core and it's integration with Dagster.]]></description><link>http://localhost:2368/dbt-core/</link><guid isPermaLink="false">658933857bbe782450364b75</guid><category><![CDATA[Data]]></category><category><![CDATA[ClickHouse]]></category><category><![CDATA[dbt]]></category><category><![CDATA[Dagster]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 25 Dec 2023 12:22:54 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1591164811435-2b8a547039de?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDEyfHx0cmFuc2Zvcm1lcnN8ZW58MHx8fHwxNzAzNDkwNDYxfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1591164811435-2b8a547039de?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDEyfHx0cmFuc2Zvcm1lcnN8ZW58MHx8fHwxNzAzNDkwNDYxfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster"><p>In this blog post, we embark on a journey to explore the seamless integration of dbt-core with Dagster, complemented by the powerful analytics capabilities of ClickHouse. Let&apos;s dive in and witness the synergy of these tools in action.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>At the heart of our data transformation endeavors lies <a href="https://github.com/dbt-labs/dbt-core"><strong>dbt Core</strong></a>, a versatile tool that empowers data teams to wield analytics engineering best practices with ease. Coupled with <a href="https://github.com/dagster-io/dagster"><strong>dagster</strong></a>, an orchestration platform tailored for data asset development and observation, and <a href="https://clickhouse.com/"><strong>ClickHouse</strong></a>, an open-source column-oriented database management system.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following components installed to get started.</p>
<ul>
<li><a href="https://github.com/dagster-io/dagster">Dagster</a></li>
<li><a href="https://github.com/dbt-labs/dbt-core">dbt-core</a></li>
<li><a href="https://clickhouse.com/">ClickHouse</a></li>
<li><a href="https://www.python.org/">Python</a></li>
<li><a href="https://github.com/ClickHouse/dbt-clickhouse">dbt ClickHouse plugin</a></li>
<li><a href="https://docs.dagster.io/integrations/dbt/using-dbt-with-dagster">dagster-dbt</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="installations">Installations</h3>
<p>Install the dbt ClickHouse plugin.</p>
<pre><code class="language-shell">pip install dbt-clickhouse
</code></pre>
<p>Install the <strong>dagster-dbt</strong> library.</p>
<pre><code class="language-shell">pip install dagster-dbt dagster-webserver
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="getting-started">Getting Started</h3>
<p>These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.</p>
<p>Initialize the dbt project and configure it to interface with ClickHouse.</p>
<pre><code class="language-shell">dbt init dbt_data_practitioner 
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt1-1.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>Navigate to Your dbt Project Directory</p>
<pre><code class="language-shell">cd dbt_data_practitioner
</code></pre>
<pre><code class="language-shell">touch profiles.yml
</code></pre>
<p>Add ClickHouse configuration details to <code>profiles.yml</code>. Make sure all dependencies are correctly set up in your <code>profiles.yml</code> file for your database connections.</p>
<pre><code class="language-yml">dbt_data_practitioner:
  target: dev
  outputs:
    dev:
      type: clickhouse
      schema: sakila_db
      host: localhost
      port: 8123
      user: default
      password: root
      secure: False
</code></pre>
<p>Running <code>dbt debug</code> provides valuable insights into your dbt project&apos;s configuration and environment, ensuring everything is set up correctly before diving into data transformations. When you run dbt debug, it performs a series of checks and validations, including: Configuration Validation, Connection Testing, Adapter Information, Environment Information.</p>
<p>By running <code>dbt debug</code> as part of your setup process, you can catch any configuration errors or connectivity issues early on, ensuring a smoother experience when running dbt commands and executing data transformations.</p>
<pre><code class="language-shell">dbt debug
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt2.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>After executing this command, carefully review the output to ensure that everything is configured correctly and that dbt can successfully connect to your ClickHouse instance.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="dbt-docs">dbt docs</h3>
<p>Generate documentation for the dbt project:</p>
<pre><code class="language-shell">dbt docs generate
</code></pre>
<p>This command generates the documentation artifacts for your project. It creates a <code>manifest.json</code> and <code>catalog.json</code> file in the target directory.</p>
<p><img src="http://localhost:2368/content/images/2023/12/dbt3.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>To view the documentation, you need to serve it locally using the following command:</p>
<pre><code class="language-shell">dbt docs serve
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt4.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>By default, this will start a local server at <a href="http://localhost:8080">http://localhost:8080</a> where you can view the documentation.</p>
<p><img src="http://localhost:2368/content/images/2023/12/2525.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="models">models</h3>
<p>Create models and define transformations:</p>
<pre><code class="language-shell">cd models  
mkdir sakila_db  
cd sakila_db  
</code></pre>
<pre><code class="language-shell">touch actor_film_actor_join.sql
touch point_of_interest_1.sql 
</code></pre>
<script src="https://gist.github.com/AnanthaRajuC/4346dd1c6df08418b43778056bfb5e10.js"></script>
<p>Delete the <strong>examples</strong> folder present inside the models folder.</p>
<pre><code class="language-shell">cd ..
cd .. 
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="build-the-project">Build the project:</h3>
<p>The following command runs a series of tasks to build your models, including running models, tests, and snapshots. This command essentially combines the functionality of <code>dbt run</code>, <code>dbt test</code>, and <code>dbt snapshot</code>.</p>
<p>The dbt build command will perform the following tasks:</p>
<ol>
<li><strong>Run Models</strong>: Executes SQL scripts to create views or tables based on your model definitions.</li>
<li><strong>Run Tests</strong>: Executes tests defined in your project to validate data quality and integrity.</li>
<li><strong>Run Snapshots</strong>: Executes snapshot scripts to capture and store historical data.</li>
</ol>
<pre><code class="language-shell">dbt build
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dbt5.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>The tables and views defined are now generated in ClickHouse DB.</p>
<pre><code class="language-shell">dbt docs generate
</code></pre>
<pre><code class="language-shell">dbt docs serve
</code></pre>
<p>Lineage Graph and other details.</p>
<p><img src="http://localhost:2368/content/images/2023/12/25252.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="dagster-integration">Dagster Integration</h3>
<p>Now, let&apos;s integrate Dagster into our data transformation pipeline:</p>
<pre><code class="language-shell">cd dbt_data_practitioner
</code></pre>
<pre><code class="language-shell">dagster-dbt project scaffold --project-name dagster_data_practitioner
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dag1.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<pre><code class="language-shell">cd dagster_data_practitioner
</code></pre>
<pre><code class="language-shell">DAGSTER_DBT_PARSE_PROJECT_ON_LOAD=1 dagster dev
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/dag2-1.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>To access the Dagster UI from your browser, navigate to: <a href="http://127.0.0.1:3000">http://127.0.0.1:3000</a></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="dagster-ui">Dagster UI</h4>
<p><img src="http://localhost:2368/content/images/2023/12/dag3.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p>Click on the black &quot;<strong>Materialize all</strong>&quot; button to materialize the transformations.</p>
<p><img src="http://localhost:2368/content/images/2023/12/dag4.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<p><img src="http://localhost:2368/content/images/2023/12/dag5.png" alt="Unlocking Data Transformation Magic with dbt-core, ClickHouse, and Dagster" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="sample-project">Sample Project</h4>
<p><a href="https://github.com/AnanthaRajuC/DataPractitioner">DataPractitioner</a> is the sample project i&apos;ve used to illustrate the usage of the aforementioned tools.</p>
<p><em>Noticed an issue with this Sample Project? Open an <a href="https://github.com/AnanthaRajuC/DataPractitioner/issues">issue</a> or a <a href="https://github.com/AnanthaRajuC/DataPractitioner/pulls">PR</a> on GitHub!</em></p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine]]></title><description><![CDATA[<p>In this guide, we&apos;ll walk through the seamless process of replicating a MySQL database into ClickHouse, leveraging the experimental MaterializedMySQL Database Engine. By replicating MySQL data into ClickHouse, we harness the real-time analytical capabilities of ClickHouse while preserving data integrity and consistency.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>MySQL stands as a stalwart</p>]]></description><link>http://localhost:2368/replicate-mysql-database-in-clickhouse-using/</link><guid isPermaLink="false">65891ce27bbe782450364a54</guid><category><![CDATA[Data]]></category><category><![CDATA[MySQL]]></category><category><![CDATA[ClickHouse]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 25 Dec 2023 07:34:10 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1472017053394-b29fded587cd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI4fHx0d2lufGVufDB8fHx8MTcwMzQ4OTU2N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1472017053394-b29fded587cd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI4fHx0d2lufGVufDB8fHx8MTcwMzQ4OTU2N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine"><p>In this guide, we&apos;ll walk through the seamless process of replicating a MySQL database into ClickHouse, leveraging the experimental MaterializedMySQL Database Engine. By replicating MySQL data into ClickHouse, we harness the real-time analytical capabilities of ClickHouse while preserving data integrity and consistency.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>MySQL stands as a stalwart in the realm of relational database management systems, renowned for its open-source nature and robust features. On the other hand, ClickHouse shines as a lightning-fast, column-oriented database management system, empowering users to generate analytical data reports in real-time through SQL queries.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following components installed and configured:</p>
<ul>
<li><a href="https://www.mysql.com/">MySQL</a> database.</li>
<li><a href="https://clickhouse.com/">ClickHouse</a> database.</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="mysqlcnf-mysql-server-configuration-for-replication">mysql.cnf MySQL server configuration for Replication</h3>
<p>The following are mandatory MySQL server configuration which must be set.</p>
<ol>
<li>Stop MySQL service.</li>
</ol>
<pre><code class="language-shell">systemctl status mysql
systemctl stop mysql.service
</code></pre>
<ol start="2">
<li>Update the configuration.</li>
</ol>
<p>Edit the MySQL configuration file mysqld.cnf:</p>
<pre><code class="language-shell">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf
</code></pre>
<p>Add/Update the following parameters:</p>
<pre><code class="language-txt">[mysqld]
server-id                     = 223344
log_bin                       = mysql-bin
expire_logs_days              = 1
binlog_format                 = row
binlog_row_image              = FULL
default_authentication_plugin = mysql_native_password
gtid_mode                     = on
enforce_gtid_consistency      = on
</code></pre>
<p>ClickHouse reads binlog and performs DDL and DML queries.</p>
<ol start="3">
<li>Start MySQL service.</li>
</ol>
<pre><code class="language-shell">systemctl start mysql.service
</code></pre>
<p><em>Reference:</em> <a href="https://clickhouse.com/docs/en/engines/database-engines/materialized-mysql">https://clickhouse.com/docs/en/engines/database-engines/materialized-mysql</a></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="mysql-user-creation-for-replication">MySQL user creation for replication</h3>
<p>Create a MySQL user specifically for replication:</p>
<pre><code class="language-shell">create user clickhouse_replication@&apos;localhost&apos; identified with mysql_native_password by &apos;ChRep$316&apos;;
</code></pre>
<pre><code class="language-shell">grant replication slave, replication client, reload, select on *.* to clickhouse_replication@&apos;localhost&apos;;
</code></pre>
<pre><code class="language-shell">flush privileges;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="sample-mysql-data-setup">Sample MySQL Data Setup</h3>
<p>Setup a sample table and insert data into MySQL:</p>
<ol>
<li>Create a table.</li>
</ol>
<pre><code class="language-shell">CREATE TABLE `user` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `created_date` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `last_modified_date` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `username` varchar(255) DEFAULT NULL,
  `email` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `UK_sb8bbouer5wak8vyiiy4pf2bx` (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
</code></pre>
<ol start="2">
<li>Insert sample data.</li>
</ol>
<pre><code class="language-shell">INSERT INTO `user`(`username`,`email`)VALUES(&apos;John Doe&apos;,&apos;johndoe@example.com&apos;);

INSERT INTO `user`(`username`,`email`)VALUES(&apos;Jane Doe&apos;,&apos;janedoe@example.com&apos;);
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="clickhouse-server-configuration">CLickHouse server configuration</h3>
<pre><code class="language-shell">sudo systemctl status clickhouse-server
sudo systemctl stop clickhouse-server
</code></pre>
<p>Update ClickHouse server configuration to allow experimental MaterializedMySQL engine:</p>
<pre><code class="language-shell">sudo nano /etc/clickhouse-server/users.xml
</code></pre>
<p>Add/Update the following value within <code>&lt;profiles&gt;</code>:</p>
<pre><code class="language-xml">    &lt;profiles&gt;
        &lt;default&gt;                           &lt;allow_experimental_database_materialized_mysql&gt;1&lt;/allow_experimental_database_materialized_mysql&gt;
        &lt;/default&gt;
    &lt;/profiles&gt;
</code></pre>
<pre><code class="language-shell">sudo systemctl start clickhouse-server
</code></pre>
<p>Login to ClickHouse server using terminal or any GUI tool and verify that the changes have been saved.</p>
<pre><code class="language-shell">clickhouse-client --password   
</code></pre>
<pre><code class="language-sql">SELECT
    name,
    value,
    changed,
    description
FROM system.settings
WHERE name = &apos;allow_experimental_database_materialized_mysql&apos;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="activating-the-replication-in-clickhouse">Activating the replication in ClickHouse</h3>
<p>Create a ClickHouse database with MaterializedMySQL engine for replication:</p>
<pre><code class="language-shell">CREATE DATABASE mysqlCH
ENGINE = MaterializeMySQL(&apos;127.0.0.1&apos;, &apos;ch&apos;, &apos;clickhouse_replication&apos;, &apos;ChRep$316&apos;)
SETTINGS allows_query_when_mysql_lost = 1, max_wait_time_when_mysql_unavailable = 10000, materialized_mysql_tables_list = &apos;user&apos;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--1-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>Verify Replication</strong></p>
<p>Check if the database and the table with data got replicated into ClickHouse:</p>
<pre><code class="language-sql">select * from mysqlCH.user;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--2-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>Additional Operations</strong></p>
<p>Perform additional operations in MySQL and verify replication status in ClickHouse:</p>
<p><strong>INSERT</strong> additional rows in MySQL table and check the replication status.</p>
<pre><code class="language-sql">INSERT INTO `user`(`username`,`email`)VALUES(&apos;Alice&apos;,&apos;alice@example.com&apos;);
INSERT INTO `user`(`username`,`email`)VALUES(&apos;Bob&apos;,&apos;alice@example.com&apos;);
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--3-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>UPDATE</strong> Alice&apos;s email in MySQL user table.</p>
<pre><code class="language-sql">UPDATE `ch`.`user` SET `email` = &apos;alice@domain.com&apos; WHERE `username` = &apos;Alice&apos;;
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--4-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p><strong>DELETE</strong> Bob in MySQL user table.</p>
<pre><code class="language-sql">DELETE FROM `ch`.`user` WHERE (`id` = &apos;4&apos;);
</code></pre>
<p><img src="http://localhost:2368/content/images/2023/12/carbon--5-.png" alt="Replicate MySQL database in ClickHouse with [experimental] MaterializedMySQL Database Engine" loading="lazy"></p>
<p>With these steps, you&apos;ve successfully set up MySQL replication into ClickHouse using the MaterializedMySQL engine. Harness the power of ClickHouse&apos;s real-time analytical capabilities while ensuring data consistency and integrity. Explore the vast possibilities of real-time analytics with ClickHouse and unlock insights from your replicated MySQL data effortlessly.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[ClickHouse]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>This post briefly documents the process of using ClickHouse.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="introduction">Introduction</h4>
<p>ClickHouse is a fast open-source column-oriented database management system that allows generating analytical data reports in real-time using SQL queries.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="minimum-software-requirements">Minimum Software Requirements</h4>
<ul>
<li><a href="https://clickhouse.com/">ClickHouse</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="download-the-software">Download the software</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--12-.png" class="kg-image" alt loading="lazy" width="858" height="577" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--12-.png 600w, http://localhost:2368/content/images/2023/11/carbon--12-.png 858w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="running-the-application">Running the application</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--14--1.png" class="kg-image" alt loading="lazy" width="1256" height="465" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--14--1.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--14--1.png 1000w, http://localhost:2368/content/images/2023/11/carbon--14--1.png 1256w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="accessing-the-client">Accessing the client</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--15-.png" class="kg-image" alt loading="lazy" width="1145" height="1507" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--15-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--15-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--15-.png 1145w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--16-.png" class="kg-image" alt loading="lazy" width="1145" height="1343" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--16-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--16-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--16-.png 1145w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="resources">Resources</h4>
<p><a href="https://clickhouse.com/docs/en/getting-started/example-datasets/menus">New York Public Library &quot;What&</a></p>]]></description><link>http://localhost:2368/clickhouse/</link><guid isPermaLink="false">655223945650dc3b96a57209</guid><category><![CDATA[Data]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 13 Nov 2023 13:59:16 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1542274368-443d694d79aa?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHZlcnRpY2FsJTIwcGlwZXN8ZW58MHx8fHwxNjk5ODgxODgzfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1542274368-443d694d79aa?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHZlcnRpY2FsJTIwcGlwZXN8ZW58MHx8fHwxNjk5ODgxODgzfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="ClickHouse"><p>This post briefly documents the process of using ClickHouse.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="introduction">Introduction</h4>
<p>ClickHouse is a fast open-source column-oriented database management system that allows generating analytical data reports in real-time using SQL queries.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="minimum-software-requirements">Minimum Software Requirements</h4>
<ul>
<li><a href="https://clickhouse.com/">ClickHouse</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="download-the-software">Download the software</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--12-.png" class="kg-image" alt="ClickHouse" loading="lazy" width="858" height="577" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--12-.png 600w, http://localhost:2368/content/images/2023/11/carbon--12-.png 858w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="running-the-application">Running the application</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--14--1.png" class="kg-image" alt="ClickHouse" loading="lazy" width="1256" height="465" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--14--1.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--14--1.png 1000w, http://localhost:2368/content/images/2023/11/carbon--14--1.png 1256w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="accessing-the-client">Accessing the client</h4>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--15-.png" class="kg-image" alt="ClickHouse" loading="lazy" width="1145" height="1507" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--15-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--15-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--15-.png 1145w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--16-.png" class="kg-image" alt="ClickHouse" loading="lazy" width="1145" height="1343" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--16-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--16-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--16-.png 1145w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="resources">Resources</h4>
<p><a href="https://clickhouse.com/docs/en/getting-started/example-datasets/menus">New York Public Library &quot;What&apos;s on the Menu?&quot; Dataset</a> an example of denormalizing data.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Airbyte]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>Airbyte is a powerful, open-source data integration engine designed to help you seamlessly consolidate your data into data warehouses, lakes, and databases. Whether you&apos;re managing data from various sources or need to ensure your data is easily accessible for analysis, Airbyte simplifies the process.</p>
<p>In this blog</p>]]></description><link>http://localhost:2368/airbyte/</link><guid isPermaLink="false">65521b155650dc3b96a571c7</guid><category><![CDATA[Data]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 13 Nov 2023 13:23:17 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1619249722898-492c571615fe?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDl8fGNvbm5lY3RvcnN8ZW58MHx8fHwxNjk5ODgwMDc0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<img src="https://images.unsplash.com/photo-1619249722898-492c571615fe?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDl8fGNvbm5lY3RvcnN8ZW58MHx8fHwxNjk5ODgwMDc0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Airbyte"><p>Airbyte is a powerful, open-source data integration engine designed to help you seamlessly consolidate your data into data warehouses, lakes, and databases. Whether you&apos;re managing data from various sources or need to ensure your data is easily accessible for analysis, Airbyte simplifies the process.</p>
<p>In this blog post, I will demonstrate how to use Airbyte with MySQL installed locally on Ubuntu as the source and a local JSON file as the output.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>To get started with Airbyte, ensure you have the following software installed:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a></li>
<li><a href="https://www.mysql.com/">MySQL</a> (installed locally on Ubuntu)</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="running-the-application">Running the application</h3>
<p>First, ensure Docker is running on your machine. If Docker isn&apos;t installed, download and install it from the official Docker website.</p>
<p>Open a terminal and run the following command&apos;s to download and start Airbyte:</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--10-.png" class="kg-image" alt="Airbyte" loading="lazy" width="800" height="291" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--10-.png 600w, http://localhost:2368/content/images/2023/11/carbon--10-.png 800w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/carbon--6-.png" class="kg-image" alt="Airbyte" loading="lazy" width="1024" height="1340" srcset="http://localhost:2368/content/images/size/w600/2023/11/carbon--6-.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/carbon--6-.png 1000w, http://localhost:2368/content/images/2023/11/carbon--6-.png 1024w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="accessing-the-ui">Accessing the UI</h3>
<p>Once Airbyte is up and running, you can access the user interface by navigating to the following URL in your web browser:</p>
<p><a href="http://localhost:8000/">http://localhost:8000/</a></p>
<p>Use the following credentials to log in:</p>
<p><strong>username</strong> <em>airbyte</em><br>
<strong>password</strong> <em>password</em></p>
<p><em>note</em>: For security reasons, it&apos;s recommended to change these credentials. You can do this by modifying the .env file located in your Airbyte directory.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/123.png" class="kg-image" alt="Airbyte" loading="lazy" width="938" height="299" srcset="http://localhost:2368/content/images/size/w600/2023/11/123.png 600w, http://localhost:2368/content/images/2023/11/123.png 938w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/ab1.png" class="kg-image" alt="Airbyte" loading="lazy" width="1366" height="680" srcset="http://localhost:2368/content/images/size/w600/2023/11/ab1.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/ab1.png 1000w, http://localhost:2368/content/images/2023/11/ab1.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="source-configuration">Source Configuration</h3>
<p>After logging in, you&apos;ll need to configure your data sources. Airbyte supports a wide variety of data sources, allowing you to integrate data from multiple platforms effortlessly.</p>
<h4 id="1-adding-a-source">1. Adding a Source</h4>
<ul>
<li>Click on the &quot;Sources&quot; tab in the UI.</li>
<li>Select &quot;MySQL&quot; as the type of source you want to add.</li>
<li>Fill in the required details, such as connection name, host, port, database name, username, and password.</li>
<li>Test the connection to ensure it&apos;s working correctly.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/ab3.png" class="kg-image" alt="Airbyte" loading="lazy" width="1195" height="1557" srcset="http://localhost:2368/content/images/size/w600/2023/11/ab3.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/ab3.png 1000w, http://localhost:2368/content/images/2023/11/ab3.png 1195w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="2-setting-up-a-destination">2. Setting Up a Destination</h4>
<p>For this example, we&apos;ll use a local JSON file as the destination.</p>
<ul>
<li>Click on the &quot;Destinations&quot; tab.</li>
<li>Select &quot;Local JSON&quot; as your desired destination.</li>
<li>Enter the necessary details, such as the file path where the JSON file will be stored.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2024/06/localjson.png" class="kg-image" alt="Airbyte" loading="lazy" width="1195" height="722" srcset="http://localhost:2368/content/images/size/w600/2024/06/localjson.png 600w, http://localhost:2368/content/images/size/w1000/2024/06/localjson.png 1000w, http://localhost:2368/content/images/2024/06/localjson.png 1195w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="3-creating-a-connection">3. Creating a Connection</h4>
<ul>
<li>Once your source and destination are configured, navigate to the &quot;Connections&quot; tab.</li>
<li>Click &quot;New Connection&quot; and select your source and destination.</li>
<li>Configure the sync frequency and any other options as needed.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2024/06/conn.png" class="kg-image" alt="Airbyte" loading="lazy" width="1366" height="659" srcset="http://localhost:2368/content/images/size/w600/2024/06/conn.png 600w, http://localhost:2368/content/images/size/w1000/2024/06/conn.png 1000w, http://localhost:2368/content/images/2024/06/conn.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h4 id="4-running-your-first-sync">4. Running Your First Sync</h4>
<ul>
<li>After setting up your connection, you can run your first sync manually by clicking &quot;Sync Now&quot; on your connection page.</li>
<li>Monitor the progress and check for any errors that may need to be addressed.</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2024/06/syn.png" class="kg-image" alt="Airbyte" loading="lazy" width="1366" height="659" srcset="http://localhost:2368/content/images/size/w600/2024/06/syn.png 600w, http://localhost:2368/content/images/size/w1000/2024/06/syn.png 1000w, http://localhost:2368/content/images/2024/06/syn.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="conclusion">Conclusion</h3>
<p>Airbyte is an excellent tool for integrating and consolidating your data from various sources. By following this quick start guide, you can set up and run your first data sync, ensuring your data is centralized and ready for analysis. For more advanced configurations and troubleshooting, refer to the official Airbyte documentation.</p>
<p>By leveraging Airbyte&apos;s capabilities, you can streamline your data workflows and focus on deriving insights from your data, rather than managing the complexities of data integration.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Exploratory data analysis with sweetviz]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>&quot;Exploratory Data Analysis (EDA) is an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis.&quot; <a href="https://www.epa.gov/caddis-vol4/exploratory-data-analysis#:~:text=Exploratory%20Data%20Analysis%20(EDA)%20is,step%20in%20any%20data%20analysis.">1</a></p>
<p>&quot;<a href="https://github.com/fbdesignpro/sweetviz">Sweetviz</a> is an open-source Python library that generates</p>]]></description><link>http://localhost:2368/exploratory-data-analysis-with/</link><guid isPermaLink="false">65450e67b9d645b14615fe98</guid><category><![CDATA[Data]]></category><category><![CDATA[Python]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Fri, 03 Nov 2023 15:38:01 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1682687220509-61b8a906ca19?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wxfDF8YWxsfDF8fHx8fHwyfHwxNjk5MDI1Njg5fA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1682687220509-61b8a906ca19?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wxfDF8YWxsfDF8fHx8fHwyfHwxNjk5MDI1Njg5fA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Exploratory data analysis with sweetviz"><p>&quot;Exploratory Data Analysis (EDA) is an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis.&quot; <a href="https://www.epa.gov/caddis-vol4/exploratory-data-analysis#:~:text=Exploratory%20Data%20Analysis%20(EDA)%20is,step%20in%20any%20data%20analysis.">1</a></p>
<p>&quot;<a href="https://github.com/fbdesignpro/sweetviz">Sweetviz</a> is an open-source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with just two lines of code. Output is a fully self-contained HTML application.&quot;</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><strong>Installation</strong></p>
<p>Before installing Sweetviz, you need to ensure that Python is installed on your system. You can check this by opening your command line or terminal and running the following command:</p>
<pre><code class="language-sh">python --version
</code></pre>
<p>If Python is installed, this command will display the version number. For example, you might see something like Python <strong>3.8.10</strong>. If Python is not installed, you can download and install it from the official Python website.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/python_version_check.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1360" height="484" srcset="http://localhost:2368/content/images/size/w600/2023/11/python_version_check.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/python_version_check.png 1000w, http://localhost:2368/content/images/2023/11/python_version_check.png 1360w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p>Once Python is installed, you can proceed to install Sweetviz using pip. Run the following command in your command line or terminal:</p>
<pre><code class="language-sh">pip install sweetviz
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/pip_install_sweetviz.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1024" height="1321" srcset="http://localhost:2368/content/images/size/w600/2023/11/pip_install_sweetviz.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/pip_install_sweetviz.png 1000w, http://localhost:2368/content/images/2023/11/pip_install_sweetviz.png 1024w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong>Basic Program</strong></p>
<p>Once Sweetviz is installed, you can create a simple program to generate a data report. We&apos;ll use the Titanic dataset for this example.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class="language-python">import pandas as pd
import sweetviz as sv

# Load the dataset
data = pd.read_csv(&apos;titanic.csv&apos;)

# Generate the Sweetviz report
report = sv.analyze(data)

# Display the report in the browser
report.show_html()
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><strong>Explanation of the Code</strong></p>
<ol>
<li>
<p><strong>Import Libraries:</strong> Import the necessary libraries, pandas for data manipulation and sweetviz for generating the report.</p>
</li>
<li>
<p><strong>Load Dataset:</strong> Read the Titanic dataset into a Pandas DataFrame.</p>
</li>
<li>
<p><strong>Generate Report:</strong> Use Sweetviz&apos;s analyze function to create a report based on the DataFrame.</p>
</li>
<li>
<p><strong>Show Report:</strong> Generate an HTML report that is automatically opened in your web browser.</p>
</li>
</ol>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/sweetviz_python_file_execution.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1024" height="298" srcset="http://localhost:2368/content/images/size/w600/2023/11/sweetviz_python_file_execution.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/sweetviz_python_file_execution.png 1000w, http://localhost:2368/content/images/2023/11/sweetviz_python_file_execution.png 1024w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong>Output</strong></p>
<p>After running the script, Sweetviz will generate a self-contained HTML report and open it in your default web browser. The report provides visualizations and analysis of your dataset.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/11/sweetvizoutput.png" class="kg-image" alt="Exploratory data analysis with sweetviz" loading="lazy" width="1366" height="1509" srcset="http://localhost:2368/content/images/size/w600/2023/11/sweetvizoutput.png 600w, http://localhost:2368/content/images/size/w1000/2023/11/sweetvizoutput.png 1000w, http://localhost:2368/content/images/2023/11/sweetvizoutput.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong>Resources</strong></p>
<p>To further enhance your understanding and usage of Sweetviz and Python scripts, here are some additional resources:</p>
<ul>
<li>
<p>Download the dataset used in this example: <a href="https://github.com/datasciencedojo/datasets/blob/master/titanic.csv">https://github.com/datasciencedojo/datasets/blob/master/titanic.csv</a></p>
</li>
<li>
<p>A comprehensive guide on running Python scripts, useful for beginners: <a href="https://www.knowledgehut.com/blog/programming/run-python-scripts#how-to-run-python-script-by-the-interpreter">https://www.knowledgehut.com/blog/programming/run-python-scripts#how-to-run-python-script-by-the-interpreter</a></p>
</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><strong>Conclusion</strong></p>
<p>Sweetviz is a powerful tool that simplifies the process of exploratory data analysis by generating interactive reports. By following the steps outlined in this article, you can quickly start analyzing your datasets and gain valuable insights. Feel free to explore more features and customize your reports to suit your needs.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Automating Your Ubuntu Setup: A Post-Installation Shell Script]]></title><description><![CDATA[<p>Streamline your Ubuntu setup process with a post-installation shell script designed to automate the installation of essential software and configurations. This script simplifies the setup of a new Ubuntu system, saving you time and effort by handling common post-install tasks automatically.</p><!--kg-card-begin: markdown--><h3 id="understanding-shell-scripts">Understanding Shell Scripts</h3>
<p>A shell script is a computer</p>]]></description><link>http://localhost:2368/ubuntu-post-install-script/</link><guid isPermaLink="false">638af8c4bbbc8447f5ca93ed</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sat, 03 Dec 2022 07:30:42 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1605791767308-46f38113f418?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGFmdGVyfGVufDB8fHx8MTY3MDA1MjA0Mw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1605791767308-46f38113f418?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGFmdGVyfGVufDB8fHx8MTY3MDA1MjA0Mw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Automating Your Ubuntu Setup: A Post-Installation Shell Script"><p>Streamline your Ubuntu setup process with a post-installation shell script designed to automate the installation of essential software and configurations. This script simplifies the setup of a new Ubuntu system, saving you time and effort by handling common post-install tasks automatically.</p><!--kg-card-begin: markdown--><h3 id="understanding-shell-scripts">Understanding Shell Scripts</h3>
<p>A shell script is a computer program designed to be run by a Unix shell, serving as a command-line interpreter. By leveraging shell scripting, you can automate repetitive tasks and streamline your workflow on Ubuntu.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>To utilize this post-installation shell script, you need a system running <a href="https://ubuntu.com/">Ubuntu</a>, a popular Linux distribution known for its user-friendly interface and robustness.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="sample-project">Sample Project</h3>
<p>Explore the <a href="https://github.com/AnanthaRajuC/ubuntu-post-install">Ubuntu Post Install Scripts repository</a> on GitHub to access the sample script and customize it according to your preferences.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="basic-usage">Basic Usage</h3>
<ol>
<li>
<p><strong>Open a Terminal Window:</strong> Launch a terminal window on Ubuntu by pressing either of the following key combinations on your keyboard:</p>
<ul>
<li><kbd>Ctrl</kbd> + <kbd>Alt</kbd> + <kbd>T</kbd></li>
<li><kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd></li>
</ul>
</li>
<li>
<p><strong>Set Script Permissions:</strong> Update the permissions of the script to make it executable using the <code>chmod</code> command.</p>
<ul>
<li><code>sudo chmod +x ubuntu-post-install.sh</code></li>
</ul>
</li>
<li>
<p>Run the script with the <code>bash</code> command.</p>
<ul>
<li><code>sudo bash ubuntu-post-install.sh</code></li>
</ul>
</li>
</ol>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="customization">Customization</h3>
<p>While the provided script includes a set of preferential packages and configurations, you can customize it to suit your specific requirements. Feel free to update the script with your preferred software packages and system configurations by modifying the script available on the <a href="https://github.com/AnanthaRajuC/ubuntu-post-install/blob/main/ubuntu-post-install.sh">GitHub repository</a>.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p><img src="http://localhost:2368/content/images/2023/11/ubuntu_post_unstall_script.png" alt="Automating Your Ubuntu Setup: A Post-Installation Shell Script" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="conclusion">Conclusion</h3>
<p>By leveraging the Ubuntu Post Install Script, you can expedite the setup process of your Ubuntu system, ensuring that it is configured according to your preferences and equipped with essential software packages. Embrace automation to enhance your productivity and make the most out of your Ubuntu experience.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[List: Gnome Shell Extensions, IntelliJ IDEA Plugins]]></title><description><![CDATA[<div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="gnome-shell-extensions">Gnome Shell Extensions</h2></div><!--kg-card-begin: markdown--><p><a href="https://extensions.gnome.org/">GNOME Shell extensions</a> provide a powerful way to customize your GNOME desktop environment, tailoring it to your preferences and enhancing its functionality. Here, i&#x2019;ve compiled a list of some of useful GNOME Shell extensions that can improve your user experience.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="extensions">Extensions</h4>
<p><strong><a href="https://gitlab.com/arcmenu/ArcMenu" target="_blank">ArcMenu</a></strong></p>
<p><strong>Description:</strong> ArcMenu is</p>]]></description><link>http://localhost:2368/gnome-shell-extensions/</link><guid isPermaLink="false">6382ec72f99fa23f8440d39f</guid><category><![CDATA[Tools]]></category><category><![CDATA[Linux]]></category><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 27 Nov 2022 05:25:15 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1584974414562-cc750a809485?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxFeHRlbnNpb258ZW58MHx8fHwxNjY5NTI0NjAx&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="gnome-shell-extensions">Gnome Shell Extensions</h2></div><!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1584974414562-cc750a809485?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxFeHRlbnNpb258ZW58MHx8fHwxNjY5NTI0NjAx&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins"><p><a href="https://extensions.gnome.org/">GNOME Shell extensions</a> provide a powerful way to customize your GNOME desktop environment, tailoring it to your preferences and enhancing its functionality. Here, i&#x2019;ve compiled a list of some of useful GNOME Shell extensions that can improve your user experience.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="extensions">Extensions</h4>
<p><strong><a href="https://gitlab.com/arcmenu/ArcMenu" target="_blank">ArcMenu</a></strong></p>
<p><strong>Description:</strong> ArcMenu is a customizable application menu for GNOME Shell, offering a modern and intuitive way to access your applications and system settings. It can be configured to resemble traditional application menus found in other desktop environments, making it ideal for users transitioning to GNOME.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Highly customizable menu layout</li>
<li>Search functionality for quick access to applications</li>
<li>Support for themes and icon packs</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/ArcMenu_Banner.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="1602" height="629" srcset="http://localhost:2368/content/images/size/w600/2022/11/ArcMenu_Banner.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/ArcMenu_Banner.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/ArcMenu_Banner.png 1600w, http://localhost:2368/content/images/2022/11/ArcMenu_Banner.png 1602w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/PRATAP-KUMAR/control-blur-effect-on-lock-screen" target="_blank">Control Blur Effect On Lock Screen</a></strong><br>
<strong>Description:</strong> This extension allows you to control the blur effect on the lock screen, providing a customizable aesthetic touch to your GNOME desktop. You can adjust the blur intensity to your liking, ensuring a perfect balance between style and readability.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Adjustable blur intensity</li>
<li>Simple and straightforward configuration</li>
<li>Enhances visual appeal of the lock screen</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot_2935_QPQQttf.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="1366" height="768" srcset="http://localhost:2368/content/images/size/w600/2022/11/screenshot_2935_QPQQttf.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/screenshot_2935_QPQQttf.png 1000w, http://localhost:2368/content/images/2022/11/screenshot_2935_QPQQttf.png 1366w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/home-sweet-gnome/dash-to-panel" target="_blank">Dash to Panel</a></strong><br>
<strong>Description:</strong> Dash to Panel transforms your GNOME Shell dash into a unified taskbar, combining application launchers and the system tray into a single panel. This extension is perfect for users who prefer a more traditional desktop layout similar to Windows or KDE Plasma.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Combines dash and system tray into one panel</li>
<li>Highly customizable appearance and behavior</li>
<li>Supports multi-monitor setups</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/dtp-main-p2.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="1294" height="397" srcset="http://localhost:2368/content/images/size/w600/2022/11/dtp-main-p2.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/dtp-main-p2.png 1000w, http://localhost:2368/content/images/2022/11/dtp-main-p2.png 1294w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/neffo/earth-view-wallpaper-gnome-extension" target="_blank">Google Earth Wallpaper</a></strong><br>
<strong>Description:</strong> Bring the beauty of the world to your desktop with the Google Earth Wallpaper extension. This extension sets your wallpaper to a random, high-quality photo from the curated Google Earth collection, ensuring your background is always stunning and unique.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Automatically updates wallpaper with random Google Earth photos</li>
<li>High-resolution images</li>
<li>Option to manually refresh the wallpaper</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot_1295.jpg" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="1920" height="1080" srcset="http://localhost:2368/content/images/size/w600/2022/11/screenshot_1295.jpg 600w, http://localhost:2368/content/images/size/w1000/2022/11/screenshot_1295.jpg 1000w, http://localhost:2368/content/images/size/w1600/2022/11/screenshot_1295.jpg 1600w, http://localhost:2368/content/images/2022/11/screenshot_1295.jpg 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://gitlab.com/skrewball/openweather" target="_blank">OpenWeather</a></strong><br>
<strong>Description:</strong> Stay updated with real-time weather information directly on your GNOME Shell with the OpenWeather extension. It displays weather data for any location worldwide, offering detailed forecasts and current conditions.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Displays current weather and forecasts</li>
<li>Supports multiple locations</li>
<li>Detailed weather information including temperature, humidity, and wind speed</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/openweather-screenshot.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="1674" height="1202" srcset="http://localhost:2368/content/images/size/w600/2022/11/openweather-screenshot.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/openweather-screenshot.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/openweather-screenshot.png 1600w, http://localhost:2368/content/images/2022/11/openweather-screenshot.png 1674w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://github.com/kazysmaster/gnome-shell-extension-lockkeys" target="_blank">Lock Keys</a></strong><br>
<strong>Description:</strong> The Lock Keys extension displays the status of Numlock and Capslock on the GNOME panel, providing a convenient way to check if these keys are active. This is especially useful for users who frequently switch between text and numeric input.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Visual indicator for Numlock and Capslock status</li>
<li>Simple and unobtrusive design</li>
<li>Essential for users with keyboards lacking LED indicators</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="240" height="170"></figure><!--kg-card-begin: markdown--><p><strong><a href="https://gitlab.gnome.org/World/ShellExtensions/desktop-icons" target="_blank">Desktop Icons</a></strong><br>
<strong>Description:</strong> The Desktop Icons extension brings back the ability to add and manage icons on your desktop, a feature that is missing in the default GNOME Shell. This extension is perfect for users who prefer to have quick access to files, folders, and applications directly from their desktop.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Add and manage desktop icons</li>
<li>Support for dragging and dropping files</li>
<li>Customizable icon placement</li>
</ul>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/screenshot_1465_2g841mr.png" class="kg-image" alt="List: Gnome Shell Extensions, IntelliJ IDEA Plugins" loading="lazy" width="224" height="227"></figure><!--kg-card-begin: markdown--><p>With these GNOME Shell extensions, you can significantly enhance your GNOME desktop environment, making it more functional and personalized to suit your needs. Whether you&apos;re looking for aesthetic improvements, productivity boosts, or additional features, these extensions have got you covered.</p>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header" id="intellij-idea-plugins">IntelliJ IDEA Plugins</h2></div><!--kg-card-begin: markdown--><p><a href="https://www.jetbrains.com/idea/">IntelliJ IDEA</a>: stands as a premier integrated development environment (IDE) crafted by JetBrains, designed to facilitate the creation of software using Java, Kotlin, Groovy, and other JVM-based languages. It comes in two editions: a community edition licensed under Apache 2 and a proprietary commercial edition. One of its most compelling features is its extensibility through plugins, which enrich its capabilities and tailor it to individual developer needs.</p>
<h3 id="understanding-plugins">Understanding Plugins</h3>
<p>Plugins are software extensions that augment the functionality of a program. In the realm of IntelliJ IDEA, they serve as indispensable tools, providing developers with additional features, productivity enhancements, and integration with external services.</p>
<h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<ul>
<li>To take advantage of these plugins, ensure you have <a href="https://www.jetbrains.com/idea/download/#section=linux">IntelliJ IDEA</a> installed on your system.</li>
</ul>
<h3 id="plugins">Plugins</h3>
<p><strong><a href="https://github.com/andrey4623/intellij-rainbow-csv" target="_blank">Rainbow CSV</a></strong><br>
<strong>Description:</strong> Rainbow CSV is a versatile plugin that enhances the readability of CSV files by highlighting them in different colors. This visual distinction makes it easier to interpret and manipulate large datasets, thereby improving productivity during data analysis and manipulation tasks.</p>
<p><strong><a href="https://plugins.jetbrains.com/plugin/7973-sonarlint" target="_blank">SonarLint</a></strong><br>
<strong>Description:</strong> SonarLint is a free IDE extension that acts as a guardian for your codebase, continuously analyzing it to detect and rectify bugs, vulnerabilities, and code smells in real-time. Similar to a spell checker, SonarLint identifies issues as you write code, offering quick fixes and actionable insights to ensure clean and robust code.</p>
<p><strong><a href="https://github.com/robohorse/RoboPOJOGenerator" target="_blank">RoboPOJOGenerator</a></strong><br>
<strong>Description:</strong> This indispensable plugin streamlines the process of transforming JSON data structures into Plain Old Java Objects (POJOs). Ideal for IntelliJ IDEA and Android Studio users, RoboPOJOGenerator automates the tedious task of manual POJO generation, saving developers valuable time and effort.</p>
<p><strong><a href="https://github.com/gejun123456/intellij-generateAllSetMethod" target="_blank">intellij-generateAllSetMethod</a></strong><br>
<strong>Description:</strong> Simplify the task of generating setter method calls for class properties with this intuitive IntelliJ IDEA plugin. By automating the creation of setter method invocations, it accelerates the development process and reduces the likelihood of manual errors.</p>
<p><strong><a href="https://plugins.jetbrains.com/plugin/9792-key-promoter-x" target="_blank">Key Promoter X</a></strong><br>
<strong>Description:</strong> Mastering keyboard shortcuts is key to efficient coding, and Key Promoter X is here to help. This plugin facilitates the learning process by displaying keyboard shortcuts whenever you perform an action using the mouse within the IDE. By encouraging the use of keyboard shortcuts, it promotes a faster, mouse-free development workflow.</p>
<p><strong><a href="https://plugins.jetbrains.com/plugin/7179-maven-helper" target="_blank">Maven Helper</a></strong><br>
<strong>Description:</strong> Maven Helper is an indispensable tool for managing dependencies within your IntelliJ IDEA projects. It provides valuable insights into dependency conflicts, allowing you to analyze and exclude conflicting dependencies effortlessly. Additionally, it offers advanced features for optimizing Maven builds, ensuring smooth and efficient project development.</p>
<p>IntelliJ IDEA plugins play a crucial role in enhancing productivity, streamlining development workflows, and empowering developers to write clean, efficient code. By incorporating IntelliJ IDEA plugins into your IntelliJ IDEA setup, you can unlock new capabilities, automate repetitive tasks.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Unlocking Data Visualization Brilliance with Apache Superset]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p>In this guide, we&apos;ll embark on an exploration of <a href="https://superset.apache.org/">Apache Superset</a>, a cutting-edge platform for data exploration and visualization. By leveraging Apache Superset, we empower users to uncover insights and communicate data stories effectively through stunning visualizations.</p>
<p>&quot;Data and information visualization is an interdisciplinary field that</p>]]></description><link>http://localhost:2368/apache-superset/</link><guid isPermaLink="false">6370faedacb23e2fd22ca182</guid><category><![CDATA[Data]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 13 Nov 2022 14:16:20 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1593806967876-4ad6cd0c5759?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG1hbmRhbGF8ZW58MHx8fHwxNjY4MzQ4Nzk1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<img src="https://images.unsplash.com/photo-1593806967876-4ad6cd0c5759?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG1hbmRhbGF8ZW58MHx8fHwxNjY4MzQ4Nzk1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Unlocking Data Visualization Brilliance with Apache Superset"><p>In this guide, we&apos;ll embark on an exploration of <a href="https://superset.apache.org/">Apache Superset</a>, a cutting-edge platform for data exploration and visualization. By leveraging Apache Superset, we empower users to uncover insights and communicate data stories effectively through stunning visualizations.</p>
<p>&quot;Data and information visualization is an interdisciplinary field that deals with the graphic representation of data and information. It is a particularly efficient way of communicating when the data or information is numerous as for example a time series.&quot;</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following components installed to kickstart your Apache Superset journey:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a></li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a></li>
<li><a href="https://www.mysql.com/">MySQL</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="getting-started">Getting Started</h3>
<h4 id="setup">Setup</h4>
<p>First, ensure Docker and Docker Compose are installed on your system by checking their versions:</p>
<pre><code class="language-shell">docker version
docker-compose version
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-25-55.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1080" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-25-55.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-25-55.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-25-55.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-25-55.png 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="database-setup">Database Setup</h3>
<p>In this post we will use docker to deploy Apache Superset, in order to allow Apache Superset to connect to locally installed MySQL database we will have to perform the following operations.</p>
<ol>
<li>Enable MySQL to be able to listen for an external IP address where the server can be reached:</li>
</ol>
<pre><code class="language-shell">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf
</code></pre>
<p>update <strong>bind-address</strong> directive to a wildcard IP address, either *****, <strong>::</strong>, or <strong>0.0.0.0</strong> to reference an external IP address.</p>
<p>By default, <strong>bind-address</strong> is set to <strong>127.0.0.1</strong>, meaning that the server will only look for local connections.</p>
<p><em>Reference:</em> <a href="https://www.digitalocean.com/community/tutorials/how-to-allow-remote-access-to-mysql">https://www.digitalocean.com/community/tutorials/how-to-allow-remote-access-to-mysql</a></p>
<hr>
<ol start="2">
<li>Create MySQL users for Apache Superset:</li>
</ol>
<pre><code>CREATE USER &apos;myuser&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;mypass&apos;;
CREATE USER &apos;myuser&apos;@&apos;%&apos; IDENTIFIED BY &apos;mypass&apos;;
</code></pre>
<pre><code>GRANT ALL ON *.* TO &apos;myuser&apos;@&apos;localhost&apos;;
GRANT ALL ON *.* TO &apos;myuser&apos;@&apos;%&apos;;
</code></pre>
<pre><code>FLUSH PRIVILEGES;
</code></pre>
<p>Reference: <a href="https://stackoverflow.com/a/55742963">https://stackoverflow.com/a/55742963</a></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="bringing-up-apache-superset">Bringing up Apache Superset</h3>
<p>Updating <strong>docker-compose-non-dev.yml</strong> to connect to localhost.</p>
<script src="https://gist.github.com/AnanthaRajuC/ba602aada1311a007c228e1a1cbaafbc.js"></script>
<ol>
<li>Clone the Apache Superset repository and navigate to the directory:</li>
</ol>
<pre><code class="language-shell">git clone https://github.com/apache/superset.git
cd superset/
</code></pre>
<ol start="2">
<li>Pull and start Apache Superset using Docker Compose:</li>
</ol>
<pre><code class="language-shell">docker-compose -f docker-compose-non-dev.yml pull
docker-compose -f docker-compose-non-dev.yml up
</code></pre>
<p><strong>Reference:</strong> <a href="https://superset.apache.org/docs/installation/installing-superset-using-docker-compose">https://superset.apache.org/docs/installation/installing-superset-using-docker-compose</a></p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-30-26.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1080" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-30-26.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-30-26.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-30-26.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-30-26.png 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="accessing-superset-gui">Accessing Superset GUI</h3>
<p>Navigate to <a href="localhost:8088/login/">localhost:8088/login/</a> in your web browser to access the Apache Superset GUI.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-05.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1918" height="1006" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-31-05.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-31-05.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-31-05.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-05.png 1918w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-29.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1003" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-02-10-13-31-29.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-02-10-13-31-29.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-02-10-13-31-29.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-02-10-13-31-29.png 1920w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2022/11/Screenshot-from-2022-03-25-11-28-43.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1920" height="1003" srcset="http://localhost:2368/content/images/size/w600/2022/11/Screenshot-from-2022-03-25-11-28-43.png 600w, http://localhost:2368/content/images/size/w1000/2022/11/Screenshot-from-2022-03-25-11-28-43.png 1000w, http://localhost:2368/content/images/size/w1600/2022/11/Screenshot-from-2022-03-25-11-28-43.png 1600w, http://localhost:2368/content/images/2022/11/Screenshot-from-2022-03-25-11-28-43.png 1920w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h3 id="connect-to-database">Connect to Database</h3>
<p>To connect Apache Superset to your MySQL database, use the following connection details:</p>
<p><strong>Host</strong>: <code>172.17.0.1</code><br>
<strong>Port</strong>: <code>3306</code><br>
<strong>Database Name</strong>: <code>mysqmpledb</code><br>
<strong>Username</strong>: <code>root</code><br>
<strong>Password</strong>: <code>root</code><br>
<strong>Display Name</strong>: MySQL - LocalHost</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2023/12/1234.png" class="kg-image" alt="Unlocking Data Visualization Brilliance with Apache Superset" loading="lazy" width="1347" height="651" srcset="http://localhost:2368/content/images/size/w600/2023/12/1234.png 600w, http://localhost:2368/content/images/size/w1000/2023/12/1234.png 1000w, http://localhost:2368/content/images/2023/12/1234.png 1347w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><p>With Apache Superset, you unlock a world of possibilities for data exploration and visualization. Harness its capabilities to transform your data into actionable insights and captivating visualizations that drive informed decision-making and empower data-driven storytelling.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Empower Real-Time Stream Processing with ksqlDB and ksql-cli]]></title><description><![CDATA[<p>In this guide, we&apos;ll delve into setting up ksqlDB&#x2014;a powerful database purpose-built for stream processing applications&#x2014;alongside its command-line interface, ksql-cli. With these tools, you can seamlessly explore, query, and transform streaming data to derive valuable insights in real-time.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p><a href="https://ksqldb.io/">ksqlDB</a> stands as a cutting-edge</p>]]></description><link>http://localhost:2368/ksqldb/</link><guid isPermaLink="false">636517ee2ba729dae794e42f</guid><category><![CDATA[Apache Kafka]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Fri, 04 Nov 2022 14:10:01 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1528952686551-542043782ab9?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHZlbmRvcnxlbnwwfHx8fDE2Njc1Njk2Njk&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1528952686551-542043782ab9?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHZlbmRvcnxlbnwwfHx8fDE2Njc1Njk2Njk&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Empower Real-Time Stream Processing with ksqlDB and ksql-cli"><p>In this guide, we&apos;ll delve into setting up ksqlDB&#x2014;a powerful database purpose-built for stream processing applications&#x2014;alongside its command-line interface, ksql-cli. With these tools, you can seamlessly explore, query, and transform streaming data to derive valuable insights in real-time.</p><!--kg-card-begin: markdown--><h3 id="introduction">Introduction</h3>
<p><a href="https://ksqldb.io/">ksqlDB</a> stands as a cutting-edge database designed specifically for stream processing applications. It offers a streamlined platform for processing and analyzing streaming data, enabling users to unlock insights and respond to events as they occur. Coupled with <a href="https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/cli-config/">ksql-cli</a>, a command-line interface for interacting with ksqlDB, you gain unparalleled flexibility and control over your stream processing workflows.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="minimum-software-requirements">Minimum Software Requirements</h3>
<p>Ensure you have the following software components installed to kickstart your ksqlDB journey:</p>
<ul>
<li><a href="https://www.docker.com/">Docker</a> (for ksqldb-server, ksqldb-cli)</li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a></li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="getting-started">Getting Started</h3>
<h4 id="setup">Setup</h4>
<h4 id="running-the-application-via-docker-compose">Running the application via docker compose</h4>
<p>Leverage Docker Compose to orchestrate the deployment of ksqlDB and ksql-cli:</p>
<pre><code> ksqldb-server:
    image: confluentinc/ksqldb-server:latest
    container_name: ksqldb-server
    hostname: ksqldb-server
    depends_on: [kafka]
    ports:
      - 8088:8088
    networks:
      - webproxy
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: &quot;true&quot;
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: &quot;true&quot;
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-01:8084
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081

  ksqldb-cli:
    image: confluentinc/ksqldb-cli:latest
    container_name: ksqldb-cli
    networks:
      - webproxy
    depends_on: [kafka, ksqldb-server]
    entrypoint: /bin/sh
    tty: true
    environment:
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-01:8084
</code></pre>
<p>Pull all required docker images</p>
<pre><code class="language-shell">$ docker-compose pull
</code></pre>
<p>Start up the environment</p>
<p>Initiate the Docker containers to start up the ksqlDB environment.</p>
<p>The first time that you do this, the Docker images will be pulled down from the remote server. This may take a while!</p>
<pre><code class="language-shell">$ docker-compose up
</code></pre>
<pre><code class="language-shell">Creating ksqldb-server      ... done
Creating ksqldb-cli         ... done
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h3>
<p>Interact with ksqlDB using ksql-cli:</p>
<pre><code class="language-shell">$ docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
</code></pre>
<p><img src="http://localhost:2368/content/images/2022/11/10-ksql-db-initial.png" alt="Empower Real-Time Stream Processing with ksqlDB and ksql-cli" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="check-topics-streams-and-tables">Check topics, streams and tables</h3>
<pre><code class="language-sql">show topics;
</code></pre>
<pre><code class="language-sql">show streams;
</code></pre>
<pre><code class="language-sql">show tables;
</code></pre>
<h3 id="streams">Streams</h3>
<ul>
<li>Declare Streams</li>
</ul>
<pre><code class="language-sql">SET &apos;auto.offset.reset&apos; = &apos;earliest&apos;;
</code></pre>
<pre><code class="language-sql">CREATE STREAM PERSON_STREAM (id bigint,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,name VARCHAR,username VARCHAR,address_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">CREATE STREAM ADDRESS_STREAM (id bigint,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,city VARCHAR,street VARCHAR,suite VARCHAR,zipcode VARCHAR,geo_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Queries</li>
</ul>
<pre><code class="language-sql">DESCRIBE PERSON_STREAM;
</code></pre>
<pre><code class="language-sql">select * from PERSON_STREAM;
</code></pre>
<pre><code class="language-sql">SELECT * FROM PERSON_STREAM EMIT CHANGES LIMIT 1;
</code></pre>
<pre><code class="language-sql">+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|ID                       |UUID                     |CREATED_DATE_TIME        |LAST_MODIFIED_DATE_TIME  |NAME                     |USERNAME                 |ADDRESS_ID               |
+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|1                        |ce8d2120-1f93-11ed-8647-0|2022-08-19T13:22:00.000  |2022-08-19T13:22:00.000  |d14                      |dbz14                    |1                        |
|                         |c9a3cfadc50              |                         |                         |                         |                         |                         |
Limit Reached
Query terminated
</code></pre>
<ul>
<li>stream-stream join</li>
</ul>
<pre><code class="language-sql">CREATE STREAM PERSON_ADDRESS_ENRICHED_STREAM WITH (FORMAT=&apos;JSON&apos;, KAFKA_TOPIC=&apos;person_address_enriched&apos;, PARTITIONS=1, REPLICAS=1) AS 
SELECT
  P.ID P_ID,
  A.ID A_ID,
  P.NAME NAME,
  A.CITY CITY
FROM PERSON_STREAM P
LEFT OUTER JOIN ADDRESS_STREAM A WITHIN 1 HOURS GRACE PERIOD 30 MINUTES ON ((A.ID = P.ADDRESS_ID))
EMIT CHANGES;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="tables">Tables</h3>
<ul>
<li>Declare Tables</li>
</ul>
<pre><code class="language-sql">CREATE TABLE PERSON (id bigint PRIMARY KEY,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,name VARCHAR,username VARCHAR,address_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">CREATE TABLE ADDRESS (id bigint PRIMARY KEY,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,city VARCHAR,street VARCHAR,suite VARCHAR,zipcode VARCHAR,geo_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Query Tables</li>
</ul>
<pre><code class="language-sql">SELECT * FROM PERSON EMIT CHANGES LIMIT 1;
</code></pre>
<pre><code class="language-sql">SELECT * FROM ADDRESS EMIT CHANGES LIMIT 1;
</code></pre>
<ul>
<li>Table Joins</li>
</ul>
<pre><code class="language-sql">SELECT 
	P.NAME,
	A.CITY
FROM PERSON P
LEFT JOIN ADDRESS A on A.id = P.address_id
EMIT CHANGES 
LIMIT 1;
</code></pre>
<pre><code class="language-sql">SELECT 
  P.NAME, 
  A.CITY
FROM PERSON P
INNER JOIN ADDRESS A
ON A.id = P.address_id
EMIT CHANGES
LIMIT 1;
</code></pre>
<pre><code class="language-sql">CREATE TABLE PERSON_ADDRESS_ENRICHED (P_ID bigint,A_ID bigint,NAME VARCHAR,CITY VARCHAR) WITH (KAFKA_TOPIC=&apos;person_address_enriched&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Others</li>
</ul>
<pre><code class="language-sql">DROP TABLE IF EXISTS PERSON;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><hr>
<h3 id="tear-down-the-stack">Tear down the stack</h3>
<p>When done, tear down the ksqlDB stack:</p>
<pre><code class="language-shell">$ docker compose-down
</code></pre>
<pre><code class="language-shell">Stopping ksqldb-cli       ... done
Stopping ksqldb-server    ... done
Removing ksqldb-cli         ... done
Removing ksqldb-server      ... done
</code></pre>
<p><em>If you want to preserve the state of all containers, run <code>docker-compose stop</code> instead.</em></p>
<p>By following these steps, you can harness the power of ksqlDB and ksql-cli to perform real-time stream processing, enabling you to extract insights and derive value from streaming data with ease and efficiency.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Learning & Reference Resources for Developers]]></title><description><![CDATA[<p>Expand your knowledge and enhance your skills with these curated learning and reference resources. From versioning guidelines to design patterns and web tutorials, these resources provide valuable information for developers at all levels.</p><!--kg-card-begin: markdown--><h3 id="various-resources">Various Resources</h3>
<h4 id="semantic-versioning-200"><a href="https://semver.org/">Semantic Versioning 2.0.0</a></h4>
<p><strong>Description:</strong> Understand the guidelines for semantic versioning. This resource provides</p>]]></description><link>http://localhost:2368/learning-resources/</link><guid isPermaLink="false">635f3e8dd1f3be2eba75eb36</guid><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 31 Oct 2022 03:26:15 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1588769655209-422ea26a0398?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGphcnxlbnwwfHx8fDE2NjcxODYzMjU&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1588769655209-422ea26a0398?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGphcnxlbnwwfHx8fDE2NjcxODYzMjU&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Learning &amp; Reference Resources for Developers"><p>Expand your knowledge and enhance your skills with these curated learning and reference resources. From versioning guidelines to design patterns and web tutorials, these resources provide valuable information for developers at all levels.</p><!--kg-card-begin: markdown--><h3 id="various-resources">Various Resources</h3>
<h4 id="semantic-versioning-200"><a href="https://semver.org/">Semantic Versioning 2.0.0</a></h4>
<p><strong>Description:</strong> Understand the guidelines for semantic versioning. This resource provides a standardized versioning system that helps you manage project releases and dependencies effectively.</p>
<h4 id="java-design-patterns"><a href="http://java-design-patterns.com/">Java Design Patterns</a></h4>
<p><strong>Description:</strong> Explore a comprehensive collection of design patterns implemented in Java. This site offers clear examples and explanations, making it an invaluable resource for mastering design patterns in Java.</p>
<h4 id="common-words"><a href="https://anvaka.github.io/common-words/#?lang=java">Common Words</a></h4>
<p><strong>Description:</strong> Visualize the common words used in different programming languages. This interactive tool helps you understand language syntax and common terminology across various programming languages.</p>
<h3 id="web-resources">Web Resources</h3>
<h4 id="rest-api-tutorial"><a href="https://restfulapi.net/">REST API Tutorial</a></h4>
<p><strong>Description:</strong> Learn about RESTful APIs with this detailed tutorial. It covers the fundamentals of REST architecture, design principles, and best practices for building and consuming RESTful web services.</p>
<h4 id="http-status-codes"><a href="https://www.restapitutorial.com/httpstatuscodes.html">HTTP Status Codes</a></h4>
<p><strong>Description:</strong> Familiarize yourself with HTTP status codes using this comprehensive resource. It provides detailed descriptions and examples of various HTTP status codes, helping you understand their meanings and proper usage in web development.</p>
<h3 id="conclusion">Conclusion</h3>
<p>By leveraging these learning and reference resources, you can deepen your understanding of key concepts and best practices in software development. Whether you&apos;re looking to master versioning guidelines, design patterns, or RESTful APIs, these resources offer valuable insights to help you grow as a developer.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Online Coding Environments]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="ides-online">IDE&apos;s (Online)</h2>
<h3 id="try-it-online">Try It Online</h3>
<ul>
<li><strong><a href="https://tio.run/#">Try It Online</a></strong><br>
Try It Online (TIO) is an online compiler that supports a wide range of practical and recreational programming languages. It provides a convenient platform for quickly testing and running code snippets without the need for local installations.</li>
</ul>
<h3 id="ideone">Ideone</h3>
<ul>
<li><strong><a href="https://ideone.com/">Ideone</a></strong><br>
Ideone</li></ul>]]></description><link>http://localhost:2368/online-coding-environments/</link><guid isPermaLink="false">635e898d7a27371ee7afc411</guid><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 30 Oct 2022 14:39:39 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxvbmxpbmV8ZW58MHx8fHwxNjY3MTM5OTg2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="ides-online">IDE&apos;s (Online)</h2>
<h3 id="try-it-online">Try It Online</h3>
<ul>
<li><strong><a href="https://tio.run/#">Try It Online</a></strong><br>
Try It Online (TIO) is an online compiler that supports a wide range of practical and recreational programming languages. It provides a convenient platform for quickly testing and running code snippets without the need for local installations.</li>
</ul>
<h3 id="ideone">Ideone</h3>
<ul>
<li><strong><a href="https://ideone.com/">Ideone</a></strong><br>
Ideone is a versatile online compiler and debugging tool that supports over 60 programming languages. It allows developers to compile and execute code directly in the browser, making it an excellent choice for rapid prototyping and sharing code snippets.</li>
</ul>
<h3 id="codenvy">Codenvy</h3>
<ul>
<li><strong><a href="https://codenvy.io">Codenvy</a></strong><br>
Codenvy offers self-service Eclipse Che workspaces in the cloud. It provides a collaborative environment for teams to develop, build, and deploy applications using the power of container-based development.</li>
</ul>
<h3 id="coding-groundtutorialspoint">Coding Ground - TutorialsPoint</h3>
<ul>
<li><strong><a href="http://www.tutorialspoint.com/codingground.htm">Coding Ground - TutorialsPoint</a></strong><br>
TutorialsPoint&apos;s Coding Ground is a comprehensive online platform that supports coding in multiple popular programming languages. It offers a seamless editing, compiling, executing, and sharing experience, all within a browser-based interface.</li>
</ul>
<h2 id="code-fiddles">Code Fiddles</h2>
<h3 id="phpfiddle">PhpFiddle</h3>
<ul>
<li><strong><a href="http://phpfiddle.org/">PhpFiddle</a></strong><br>
PhpFiddle is a versatile online PHP IDE that provides a range of tools and resources for PHP, MySQL, SQLite, HTML, CSS, and JavaScript development. It enables users to test, debug, and share PHP code snippets and web applications with ease.</li>
</ul>
<h3 id="net-fiddle">.NET Fiddle</h3>
<ul>
<li><strong><a href="https://dotnetfiddle.net/">.NET Fiddle</a></strong><br>
.NET Fiddle is a convenient online sandbox for experimenting with .NET code snippets. It offers a lightweight development environment for quickly trying out C#, F#, and Visual Basic code without the need for local installations.</li>
</ul>
<h3 id="jsfiddle">JSFiddle</h3>
<ul>
<li><strong><a href="https://jsfiddle.net/">JSFiddle</a></strong><br>
JSFiddle is a popular online playground for testing and sharing JavaScript, CSS, HTML, and CoffeeScript code snippets. It provides a simple yet powerful code editor and live preview functionality, making it ideal for frontend development and prototyping.</li>
</ul>
<h3 id="sql-fiddle">SQL Fiddle</h3>
<ul>
<li><strong><a href="http://sqlfiddle.com/">SQL Fiddle</a></strong><br>
SQL Fiddle is a handy online tool for testing and sharing SQL database queries and schema designs. It supports various database systems, including MySQL, PostgreSQL, Oracle, and SQLite, allowing users to experiment with SQL code in a collaborative environment.</li>
</ul>
<h3 id="python-fiddle">Python Fiddle</h3>
<ul>
<li><strong><a href="http://pythonfiddle.com/">Python Fiddle</a></strong><br>
Python Fiddle offers a user-friendly web-based Python IDE for experimenting with Python code snippets. It allows users to run, edit, and share Python scripts in real-time, making it an excellent choice for learning Python and testing small programs.</li>
</ul>
<h3 id="r-fiddle">R-Fiddle</h3>
<ul>
<li><strong><a href="http://www.r-fiddle.org/#/">R-Fiddle</a></strong><br>
R-Fiddle provides a convenient online platform for coding, testing, and sharing R code snippets and data analysis scripts. It offers an interactive R environment with support for plotting, statistical analysis, and package management, making it ideal for statistical computing tasks.</li>
</ul>
<h3 id="go-playground">Go Playground</h3>
<ul>
<li><strong><a href="https://go.dev/play/">Go Playground</a></strong><br>
Go Playground is an official online tool provided by the Go programming language community. It offers a clean and minimalist environment for writing and executing Go code, making it easy to experiment with Go features and syntax without setting up a local development environment.</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Enhancing Your API Workflow: Essential Tools for Developers]]></title><description><![CDATA[<p>APIs (Application Programming Interfaces) and web services play a crucial role in modern software development, enabling seamless communication and integration between different applications and systems. To streamline your API development process and enhance productivity, consider incorporating the following essential tools into your toolkit:</p><!--kg-card-begin: markdown--><h3 id="apis-and-web-services">API&apos;s and Web Services</h3>
<p><strong><a href="http://www.downforeveryoneorjustme.com/" target="_blank">down</a></strong></p>]]></description><link>http://localhost:2368/api-tools/</link><guid isPermaLink="false">635e6d0f8ea41c669967e0a5</guid><category><![CDATA[Lists]]></category><category><![CDATA[Tools]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 30 Oct 2022 12:25:54 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1659153196652-7bc7de1ff602?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE2fHxBUEl8ZW58MHx8fHwxNjY3MTMyNjky&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1659153196652-7bc7de1ff602?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE2fHxBUEl8ZW58MHx8fHwxNjY3MTMyNjky&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Enhancing Your API Workflow: Essential Tools for Developers"><p>APIs (Application Programming Interfaces) and web services play a crucial role in modern software development, enabling seamless communication and integration between different applications and systems. To streamline your API development process and enhance productivity, consider incorporating the following essential tools into your toolkit:</p><!--kg-card-begin: markdown--><h3 id="apis-and-web-services">API&apos;s and Web Services</h3>
<p><strong><a href="http://www.downforeveryoneorjustme.com/" target="_blank">down for everyone or just me</a></strong><br>
<strong>Description:</strong> This simple tool allows you to quickly check if a website is down for everyone or just for you. It provides valuable insights into website availability, helping you troubleshoot connectivity issues more efficiently.</p>
<p><strong><a href="https://uptime.is/" target="_blank">Uptime</a></strong><br>
<strong>Description:</strong> Ensure your service-level agreements (SLAs) are met with this uptime calculator tool. It helps you calculate and track uptime metrics, enabling you to maintain high availability and reliability for your applications and services.</p>
<p><strong><a href="https://apigee.com/providers" target="_blank">Apigee API Console</a></strong><br>
<strong>Description:</strong> Discover, learn, test, and debug any API with interactive developer tools and documentation provided by Apigee API Console. It offers a comprehensive set of features for API exploration and testing, empowering developers to build and integrate APIs seamlessly.</p>
<p><strong><a href="https://postb.in/" target="_blank">PostBin</a></strong><br>
<strong>Description:</strong> Programatically test your API clients or webhooks with PostBin. It provides a convenient platform for sending and receiving HTTP requests, allowing you to validate and debug your API integrations effectively.</p>
<p><strong><a href="http://httpbin.org/" target="_blank">httpbin</a></strong><br>
<strong>Description:</strong> httpbin is a versatile HTTP request and response service designed for client testing. It offers a wide range of endpoints for testing various HTTP methods and parameters, making it an invaluable tool for API development and debugging.</p>
<p><strong><a href="http://requestbin.com/" target="_blank">RequestBin</a></strong><br>
<strong>Description:</strong> RequestBin provides a URL that collects requests made to it, allowing you to inspect and debug them in a human-friendly way. Use RequestBin to monitor and analyze HTTP requests sent by your clients or webhooks, facilitating effective troubleshooting and debugging.</p>
<p><strong><a href="http://mockbin.com/" target="_blank">Mockbin</a></strong><br>
<strong>Description:</strong> Mockbin enables you to generate custom endpoints to test, mock, and track HTTP requests and responses. It&apos;s a powerful tool for simulating different scenarios and behaviors in your API integrations, helping you ensure robustness and reliability.</p>
<p><strong><a href="https://www.statuspage.io/" target="_blank">Statuspage</a></strong><br>
<strong>Description:</strong> Statuspage is a comprehensive status and incident communication tool that enables you to keep your stakeholders informed about service availability and performance. It provides real-time status updates and incident notifications, helping you maintain transparency and trust with your users.</p>
<p><strong><a href="https://rapidapi.com/" target="_blank">Rapid API</a></strong><br>
<strong>Description:</strong> RapidAPI is the world&#x2019;s largest API marketplace, offering a vast selection of APIs for various use cases and industries. Explore and integrate APIs seamlessly into your applications, accelerating development and unlocking new capabilities.</p>
<p><strong><a href="https://apiembed.com/" target="_blank">APIembed</a></strong><br>
<strong>Description:</strong> APIembed provides embeddable API code snippets in multiple programming languages for your website, blog, or API documentation. Quickly generate code examples and documentation to simplify API integration for developers.</p>
<p><strong><a href="https://loader.io/" target="_blank">Loader</a></strong><br>
<strong>Description:</strong> Loader is a free load testing service that allows you to stress test your web apps and APIs with thousands of concurrent connections. Ensure your applications can handle peak loads and maintain performance under stress with Loader.</p>
<p><strong><a href="https://reqres.in/" target="_blank">REQ RES</a></strong><br>
<strong>Description:</strong> REQ RES is a hosted REST API ready to respond to your AJAX requests. It&apos;s a convenient tool for testing and prototyping API interactions, providing a reliable endpoint for your development and testing needs.</p>
<h3 id="conclusion">Conclusion</h3>
<p>By incorporating API tools into your development workflow, you can streamline your API development process, enhance productivity, and ensure the reliability and performance of your applications and services.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Software Development - Read & Publish]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Explore a variety of resources that can enhance your software development journey. Whether you&apos;re looking for entertainment, tools for creating presentations, community platforms, or documentation solutions, these resources have you covered.</p>
<h3 id="entertainment">Entertainment</h3>
<h4 id="commitstrip"><a href="http://www.commitstrip.com/en/">CommitStrip</a></h4>
<p><em>Description:</em> Enjoy a daily strip that humorously captures the life of a coder. CommitStrip combines</p>]]></description><link>http://localhost:2368/software-development-read-publish/</link><guid isPermaLink="false">635e68b68ea41c669967e077</guid><category><![CDATA[Lists]]></category><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Sun, 30 Oct 2022 12:14:04 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1546422904-90eab23c3d7e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fG5ld3N8ZW58MHx8fHwxNjY3MDczMDQ0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1546422904-90eab23c3d7e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fG5ld3N8ZW58MHx8fHwxNjY3MDczMDQ0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Software Development - Read &amp; Publish"><p>Explore a variety of resources that can enhance your software development journey. Whether you&apos;re looking for entertainment, tools for creating presentations, community platforms, or documentation solutions, these resources have you covered.</p>
<h3 id="entertainment">Entertainment</h3>
<h4 id="commitstrip"><a href="http://www.commitstrip.com/en/">CommitStrip</a></h4>
<p><em>Description:</em> Enjoy a daily strip that humorously captures the life of a coder. CommitStrip combines funny anecdotes with topical tech news, making it a delightful read for developers.</p>
<h4 id="dont-hit-save"><a href="http://donthitsave.com/">Don&apos;t Hit Save</a></h4>
<p><em>Description:</em> Dive into a comic that brings humor to tech, the workplace, and indie game development. Don&apos;t Hit Save offers a light-hearted take on the challenges and quirks of being a developer.</p>
<h3 id="presentation">Presentation</h3>
<h4 id="speaker-deck"><a href="https://speakerdeck.com/">Speaker Deck</a></h4>
<p><em>Description:</em> Share your presentations online effortlessly with Speaker Deck. This platform makes it easy to upload and share your slides with a global audience, ensuring your ideas reach far and wide.</p>
<h4 id="slideshare"><a href="http://www.slideshare.net/">SlideShare</a></h4>
<p><em>Description:</em> Discover, share, and present professional content with SlideShare. As the world&apos;s largest community for sharing presentations and infographics, SlideShare is an invaluable resource for professionals looking to disseminate knowledge and insights.</p>
<h4 id="marp"><a href="https://marp.app/">Marp</a></h4>
<p><em>Description:</em> Create beautiful presentations using Markdown with Marp. This tool transforms your Markdown files into stunning slide decks, making it perfect for developers who prefer writing in plain text.</p>
<h3 id="community">Community</h3>
<h4 id="dzone"><a href="https://dzone.com/">DZone</a></h4>
<p><em>Description:</em> Join a vibrant community of software professionals on DZone. This platform publishes technical content and provides a space for developers to share knowledge, stay updated on industry trends, and enhance their skills.</p>
<h4 id="codeproject"><a href="https://www.codeproject.com/">CodeProject</a></h4>
<p><em>Description:</em> Access free source code and tutorials on CodeProject. This resource offers extensive programming help, particularly for Windows developers working with Visual Basic .NET and other .NET languages.</p>
<h4 id="programmableweb"><a href="http://www.programmableweb.com/">ProgrammableWeb</a></h4>
<p><em>Description:</em> Stay informed about the API economy with ProgrammableWeb. This site is a leading source of news and information about APIs, offering a comprehensive API directory and chronicling the evolution of web services.</p>
<h4 id="stackshare"><a href="http://stackshare.io/">StackShare</a></h4>
<p><em>Description:</em> Discover and discuss the best software tools and services on StackShare. This platform allows developers to compare tools, share experiences, and make informed decisions about the technologies they use.</p>
<h3 id="documentation">Documentation</h3>
<h4 id="read-the-docs"><a href="https://readthedocs.org/">Read the Docs</a></h4>
<p><em>Description:</em> Host and browse documentation effortlessly with Read the Docs. This service makes your documentation fully searchable and easy to find, supporting imports from major version control systems.</p>
<h4 id="docusaurus"><a href="https://docusaurus.io/">Docusaurus</a></h4>
<p><em>Description:</em> Build and maintain open-source documentation websites with ease using Docusaurus. This tool helps you create beautiful documentation sites that are easy to update and manage.</p>
<h4 id="asciinema"><a href="https://asciinema.org/">asciinema</a></h4>
<p><em>Description:</em> Record and share terminal sessions with asciinema. This free and open-source solution captures terminal activity and allows you to share it on the web, making it an excellent tool for creating tutorials and demonstrating workflows.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Utilize these resources to enhance your software development process, stay entertained, and connect with the community. Whether you need tools for presentations, community support, or comprehensive documentation solutions, these platforms provide the necessary support to help you succeed in your projects.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Change Data Capture (CDC) - MySQL]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>This post briefly documents the process of Change Data Capture (CDC) with MySQL.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="introduction">Introduction</h4>
<p>Change data capture (CDC) refers to the process of identifying and capturing changes made to data in a database and then delivering those changes in real-time to a downstream process or system.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="minimum-software-requirements">Minimum Software Requirements</h4>
<ul>
<li><a href="https://www.docker.com/">Docker</a></li></ul>]]></description><link>http://localhost:2368/change-data-capture-cdc-mysql/</link><guid isPermaLink="false">63567eb675328d2010ba2658</guid><dc:creator><![CDATA[Anantha Raju C]]></dc:creator><pubDate>Mon, 24 Oct 2022 12:08:33 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1440288736878-766bd5839edb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNhdGNofGVufDB8fHx8MTY2NjYxMjk1OA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1440288736878-766bd5839edb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNhdGNofGVufDB8fHx8MTY2NjYxMjk1OA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Change Data Capture (CDC) - MySQL"><p>This post briefly documents the process of Change Data Capture (CDC) with MySQL.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="introduction">Introduction</h4>
<p>Change data capture (CDC) refers to the process of identifying and capturing changes made to data in a database and then delivering those changes in real-time to a downstream process or system.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="minimum-software-requirements">Minimum Software Requirements</h4>
<ul>
<li><a href="https://www.docker.com/">Docker</a> (MySQL, Zookeeper, Kafka, Debezium, schema-registry, kafka-ui, ksqldb-server, ksqldb-cli)</li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a></li>
<li><a href="https://www.mysql.com/">MySQL</a> database.</li>
<li><a href="https://www.mysql.com/products/workbench/">MySQL Workbench</a> or on any other MySQL database client/console.</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="mysqlcnf-mysql-database-configuration">mysql.cnf MySQL database configuration</h3>
<pre><code class="language-txt">[mysqld]
server-id         = 223344
log_bin           = mysql-bin
expire_logs_days  = 1
binlog_format     = row
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h4 id="sample-project-setup">Sample Project Setup</h4>
<ol>
<li>Clone the repository</li>
</ol>
<pre><code class="language-shell">$ git clone https://github.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL.git
$ cd Streaming_ETL_pipeline_MySQL
</code></pre>
<h4 id="running-the-application-via-docker-compose">Running the application via docker compose</h4>
<ol start="2">
<li>Pull all required docker images</li>
</ol>
<pre><code class="language-shell">$ docker compose -f docker-compose.yaml pull
</code></pre>
<ol start="3">
<li>Start up the environment</li>
</ol>
<p>The first time that you do this, the Docker images will be pulled down from the remote server. This may take a while!</p>
<pre><code class="language-shell">$ docker compose -f docker-compose.yaml up
</code></pre>
<pre><code class="language-shell">Creating network &quot;streaming_etl_pipeline_mysql_webproxy&quot; with driver &quot;bridge&quot;
Creating zookeeper ... done
Creating kafka     ... done
Creating debezium           ... done
Creating cp-schema-registry ... done
Creating kafka-connect-01   ... done
Creating ksqldb-server      ... done
Creating ksqldb-cli         ... done
</code></pre>
<h4 id="accessing-kafka-topics-via-kakfka-ui">Accessing Kafka Topics via Kakfka-UI</h4>
<ol start="4">
<li>Optionally, start <a href="https://github.com/provectus/kafka-ui">Kafka UI</a>, an open-source web UI for Apache Kafka Management</li>
</ol>
<pre><code class="language-shell">$ docker run --name=kafka-ui --network=streaming_etl_pipeline_mysql_webproxy -p 8080:8080 -e KAFKA_CLUSTERS_0_NAME=local -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092 -d provectuslabs/kafka-ui:latest
</code></pre>
<p>URL to access Kakfka-UI console: <strong><a href="http://localhost:8080">http://localhost:8080</a></strong></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/04-kafka-topics-before-debezium.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<ol start="5">
<li>Make sure everything is up and running</li>
</ol>
<pre><code class="language-shell">$ docker ps
</code></pre>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/09-docker-ps.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p>IMPORTANT: If any components do not show &quot;Up&quot; under the <code>Status</code> column (e.g., they say &quot;Exit&quot;) then you must rectify this before continuing.<br>
As a first solution, try re-issuing the <code>docker-compose up -d</code> command.</p>
<h4 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h4>
<ol start="6">
<li>Launch the KSQL CLI in another terminal window.</li>
</ol>
<pre><code class="language-shell">$ docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
</code></pre>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/10-ksql-db-initial.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<h2 id="tear-down-the-stack">Tear down the stack</h2>
<pre><code class="language-shell">$ docker stop kafka-ui

$ docker rm kafka-ui
</code></pre>
<pre><code class="language-shell">$ docker compose -f docker-compose.yaml down
</code></pre>
<pre><code class="language-shell">Stopping ksqldb-cli       ... done
Stopping ksqldb-server    ... done
Stopping kafka-connect-01 ... done
Stopping debezium         ... done
Stopping kafka            ... done
Stopping zookeeper        ... done
Removing ksqldb-cli         ... done
Removing ksqldb-server      ... done
Removing kafka-connect-01   ... done
Removing cp-schema-registry ... done
Removing debezium           ... done
Removing kafka              ... done
Removing zookeeper          ... done
Removing network streaming_etl_pipeline_mysql_webproxy
</code></pre>
<p><em>If you want to preserve the state of all containers, run <code>docker-compose stop</code> instead.</em></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="initial-mysql-preparation">Initial MySQL preparation</h2>
<p>Initial Data setup.</p>
<details open="open">
	<ul>
		<li><a href="#mysql">MySQL</a></li>
	</ul>
</details>
<h3 id="mysql">MySQL</h3>
<ul>
<li>Declare schema, user and permissions.</li>
</ul>
<pre><code class="language-sql">-- create schema
CREATE SCHEMA streaming_etl_db;

-- use schema
USE streaming_etl_db;

-- Create user 
CREATE USER &apos;debezium&apos; IDENTIFIED WITH mysql_native_password BY &apos;Debezium@123#&apos;;

-- Grant privileges to user
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &apos;debezium&apos;;

-- Reload the grant tables in the mysql database enabling the changes to take effect without reloading or restarting mysql service
FLUSH PRIVILEGES;
</code></pre>
<ul>
<li>Declare Tables</li>
</ul>
<pre><code class="language-sql">CREATE TABLE `geo` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;Unique ID for each entry.&apos;,
  `uuid` VARCHAR(50) DEFAULT (uuid()),
  `created_date_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;Field representing the date the entity containing the field was created.&apos;,
  `last_modified_date_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ,
  `lat` varchar(255) DEFAULT NULL,
  `lng` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT=&apos;Application Log.&apos;;
</code></pre>
<pre><code class="language-sql">CREATE TABLE `address` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;Unique ID for each entry.&apos;,
  `uuid` VARCHAR(50) DEFAULT (uuid()),
  `created_date_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;Field representing the date the entity containing the field was created.&apos;,
  `last_modified_date_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ,
  `city` varchar(255) DEFAULT NULL,
  `zipcode` varchar(255) DEFAULT NULL,
  `state` varchar(255) DEFAULT NULL,
  `geo_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `FK_geo_id` (`geo_id`),
  CONSTRAINT `FKC_geo_id` FOREIGN KEY (`geo_id`) REFERENCES `geo` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
</code></pre>
<pre><code class="language-sql">CREATE TABLE `person` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;Unique ID for each entry.&apos;,
  `uuid` VARCHAR(50) DEFAULT (uuid()),
  `created_date_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;Field representing the date the entity containing the field was created.&apos;,
  `last_modified_date_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ,
  `first_name` varchar(255) NOT NULL,
  `last_name` varchar(255) DEFAULT NULL,
  `email` varchar(255) DEFAULT NULL,
  `gender` varchar(255) DEFAULT NULL,
  `registration` datetime DEFAULT NULL,
  `age` int DEFAULT NULL,
  `address_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `FK_address_id` (`address_id`),
  CONSTRAINT `FKC_address_id` FOREIGN KEY (`address_id`) REFERENCES `address` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
</code></pre>
<ul>
<li>Sample Data</li>
</ul>
<pre><code class="language-sql">INSERT INTO `streaming_etl_db`.`geo`(`lat`,`lng`)VALUES(&apos;la14&apos;,&apos;lo14&apos;);
INSERT INTO `streaming_etl_db`.`address`(`city`,`zipcode`,`state`,`geo_id`)VALUES(&apos;c14&apos;,&apos;z14&apos;,&apos;s14&apos;,1);
INSERT INTO `streaming_etl_db`.`person`(`first_name`,`last_name`,`email`,`gender`,`registration`,`age`,`address_id`)VALUES(&apos;fn14&apos;,&apos;ln14&apos;,&apos;example@domain.com&apos;,&apos;M&apos;,now(),34,1);
</code></pre>
<ul>
<li>Select Statement</li>
</ul>
<pre><code class="language-sql">SELECT * 
FROM streaming_etl_db.person p
LEFT JOIN streaming_etl_db.address a on a.id = p.address_id
LEFT JOIN streaming_etl_db.geo g on g.id = a.geo_id;
</code></pre>
<pre><code class="language-sql">SELECT * FROM streaming_etl_db.person;
SELECT * FROM streaming_etl_db.address;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="debezium-registration">Debezium Registration</h3>
<pre><code class="language-shell">curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; 127.0.0.1:8083/connectors/ -d &apos;{
  &quot;name&quot;: &quot;streaming_ETL_pipeline_MySQL-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.hostname&quot;: &quot;172.17.0.1&quot;,
    &quot;database.port&quot;: &quot;3306&quot;,
    &quot;database.user&quot;: &quot;debezium&quot;,
    &quot;database.password&quot;: &quot;Debezium@123#&quot;,
    &quot;database.server.name&quot;: &quot;mysql&quot;,
	  &quot;database.server.id&quot;: &quot;223344&quot;,
    &quot;database.include.list&quot;: &quot;streaming_etl_db&quot;,
	  &quot;database.allowPublicKeyRetrieval&quot;: true,
	  &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
	  &quot;database.history.kafka.topic&quot;: &quot;mysql-streaming_etl_db-person&quot;,
	  &quot;time.precision.mode&quot;: &quot;connect&quot;,
    &quot;include.schema.changes&quot;: false,
    &quot;transforms&quot;: &quot;unwrap,dropTopicPrefix&quot;,
	  &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.ExtractNewRecordState&quot;,
	  &quot;transforms.dropTopicPrefix.type&quot;:&quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
	  &quot;transforms.dropTopicPrefix.regex&quot;:&quot;asgard.demo.(.*)&quot;,
	  &quot;transforms.dropTopicPrefix.replacement&quot;:&quot;$1&quot;,
	  &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
	  &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,
	  &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
	  &quot;value.converter.schemas.enable&quot;: &quot;false&quot;
  }
}&apos;
</code></pre>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/01-debezium-registration.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p>After Debezium registration.</p>
<p><a href="http://localhost:8083/connectors?expand=info&amp;expand=status">http://localhost:8083/connectors?expand=info&amp;expand=status</a></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/02-debezium-connectors.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p><a href="http://localhost:8083/connectors/streaming_ETL_pipeline_MySQL-connector/status">http://localhost:8083/connectors/streaming_ETL_pipeline_MySQL-connector/status</a></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/03-debezium-connector-status.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p>Kafka UI</p>
<p><a href="http://localhost:8080">http://localhost:8080</a></p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/05-kafka-topics-after-registration.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<p>Person Topic</p>
<p><img src="https://raw.githubusercontent.com/AnanthaRajuC/Streaming_ETL_pipeline_MySQL/main/documentation/images/06-kafka-topic-message.png" alt="Change Data Capture (CDC) - MySQL" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h3>
<h3 id="ksqldb">ksqlDB</h3>
<ul>
<li>Check topics, streams and tables</li>
</ul>
<pre><code class="language-sql">show topics;
show streams;
show tables;
</code></pre>
<h3 id="streams">Streams</h3>
<ul>
<li>Declare Streams</li>
</ul>
<pre><code class="language-sql">SET &apos;auto.offset.reset&apos; = &apos;earliest&apos;;
</code></pre>
<pre><code class="language-sql">CREATE STREAM PERSON_STREAM (id bigint,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,name VARCHAR,username VARCHAR,address_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;,VALUE_FORMAT=&apos;JSON&apos;);

CREATE STREAM ADDRESS_STREAM (id bigint,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,city VARCHAR,street VARCHAR,suite VARCHAR,zipcode VARCHAR,geo_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">SELECT * FROM PERSON_STREAM EMIT CHANGES LIMIT 1;

+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|ID                       |UUID                     |CREATED_DATE_TIME        |LAST_MODIFIED_DATE_TIME  |NAME                     |USERNAME                 |ADDRESS_ID               |
+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+
|1                        |ce8d2120-1f93-11ed-8647-0|2022-08-19T13:22:00.000  |2022-08-19T13:22:00.000  |d14                      |dbz14                    |1                        |
|                         |c9a3cfadc50              |                         |                         |                         |                         |                         |
Limit Reached
Query terminated
</code></pre>
<pre><code class="language-sql">DESCRIBE PERSON_STREAM;
select * from PERSON_STREAM;
</code></pre>
<ul>
<li>stream-stream join</li>
</ul>
<pre><code class="language-sql">CREATE STREAM PERSON_ADDRESS_ENRICHED_STREAM WITH (FORMAT=&apos;JSON&apos;, KAFKA_TOPIC=&apos;person_address_enriched&apos;, PARTITIONS=1, REPLICAS=1) AS 
SELECT
  P.ID P_ID,
  A.ID A_ID,
  P.NAME NAME,
  A.CITY CITY
FROM PERSON_STREAM P
LEFT OUTER JOIN ADDRESS_STREAM A WITHIN 1 HOURS GRACE PERIOD 30 MINUTES ON ((A.ID = P.ADDRESS_ID))
EMIT CHANGES;
</code></pre>
<h3 id="kafka-sink-mysql-db">Kafka Sink MySQL DB</h3>
<pre><code class="language-sql">CREATE SINK CONNECTOR SINK_PERSON_ADDRESS_ENRICHED_STREAM WITH (
    &apos;connector.class&apos;                     = &apos;io.confluent.connect.jdbc.JdbcSinkConnector&apos;,
    &apos;connection.url&apos;                      = &apos;jdbc:mysql://172.17.0.1:3306/&apos;,
    &apos;connection.user&apos;                     = &apos;debezium&apos;,
    &apos;connection.password&apos;                 = &apos;Debezium@123#&apos;,
    &apos;topics&apos;                              = &apos;PERSON_ADDRESS_ENRICHED_STREAM&apos;,
    &apos;key.converter&apos;						            = &apos;org.apache.kafka.connect.json.JsonConverter&apos;,	
	  &apos;key.converter.schemas.enable&apos;		    = &apos;false&apos;,
	  &apos;value.converter&apos;					            = &apos;org.apache.kafka.connect.json.JsonConverter&apos;,
	  &apos;value.converter.schemas.enable&apos;      = &apos;false&apos;
);
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id="accessing-ksqldb-via-ksqldb-cli">Accessing ksqlDb via ksqldb-cli</h3>
<h3 id="ksqldb">ksqlDB</h3>
<ul>
<li>Check topics, streams and tables</li>
</ul>
<pre><code class="language-sql">show topics;
show streams;
show tables;
</code></pre>
<h3 id="tables">Tables</h3>
<ul>
<li>Declare Tables</li>
</ul>
<pre><code class="language-sql">CREATE TABLE PERSON (id bigint PRIMARY KEY,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,name VARCHAR,username VARCHAR,address_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.person&apos;,VALUE_FORMAT=&apos;JSON&apos;);

CREATE TABLE ADDRESS (id bigint PRIMARY KEY,uuid VARCHAR,created_date_time TIMESTAMP,last_modified_date_time TIMESTAMP,city VARCHAR,street VARCHAR,suite VARCHAR,zipcode VARCHAR,geo_id bigint) WITH (KAFKA_TOPIC=&apos;mysql.streaming_etl_db.address&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<pre><code class="language-sql">SELECT * FROM PERSON EMIT CHANGES LIMIT 1;

SELECT * FROM ADDRESS EMIT CHANGES LIMIT 1;
</code></pre>
<p>Joins</p>
<pre><code class="language-sql">SELECT 
	P.NAME,
	A.CITY
FROM PERSON P
LEFT JOIN ADDRESS A on A.id = P.address_id
EMIT CHANGES 
LIMIT 1;
</code></pre>
<pre><code class="language-sql">SELECT 
  P.NAME, 
  A.CITY
FROM PERSON P
INNER JOIN ADDRESS A
ON A.id = P.address_id
EMIT CHANGES
LIMIT 1;
</code></pre>
<pre><code class="language-sql">CREATE TABLE PERSON_ADDRESS_ENRICHED (P_ID bigint,A_ID bigint,NAME VARCHAR,CITY VARCHAR) WITH (KAFKA_TOPIC=&apos;person_address_enriched&apos;,VALUE_FORMAT=&apos;JSON&apos;);
</code></pre>
<ul>
<li>Others</li>
</ul>
<pre><code class="language-sql">DROP TABLE IF EXISTS PERSON;
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>